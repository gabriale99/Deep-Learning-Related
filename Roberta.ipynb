{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Demo"
      ],
      "metadata": {
        "id": "Rwt6p18wevzs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Go to cell 20 to see submission"
      ],
      "metadata": {
        "id": "_1NdYDyWUaMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Auto Encoding (for NLU purpose)"
      ],
      "metadata": {
        "id": "iFZOmjxYeyGT"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4B9iMQk6xwtR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hugging Face"
      ],
      "metadata": {
        "id": "sBm7ySUte1_R"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ApXz6zwb4rF",
        "outputId": "d62698e2-a2d1-41dd-c0eb-e8bc2864deda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.10/dist-packages (0.1.73)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.10/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (2.0.0)\n"
          ]
        }
      ],
      "source": [
        " # installing required libraries\n",
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install contractions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing required libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "from torch.nn import BCEWithLogitsLoss, BCELoss\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix, f1_score, accuracy_score\n",
        "import pickle\n",
        "from tqdm import tqdm, trange\n",
        "from ast import literal_eval\n",
        "import json\n",
        "import nltk\n",
        "from nltk.tokenize import LineTokenizer, sent_tokenize\n",
        "nltk.download('punkt')\n",
        "import re\n",
        "import contractions\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "WoasF6wXcRnh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c5e2ce2-f92a-4a98-c46b-1fcf40f3ec5a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mounting google drive\n",
        "drive.mount('/content/drive/', force_remount = True)"
      ],
      "metadata": {
        "id": "llKpkwIQcc92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d038ea40-965c-4ef8-9c6d-6957aa0854fc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This function processes the text so that it can be used to produce meaningful embeddings\n",
        "def processText(text):\n",
        "\n",
        "    # Using the line tokenizer to split using new line character (\\n) and discarding the blank lines\n",
        "    if text is not None:\n",
        "\n",
        "        lines = LineTokenizer(blanklines = 'discard').tokenize(text)\n",
        "\n",
        "        sentences = []\n",
        "\n",
        "        # For each line found above we are doing sentence tokenization\n",
        "        for line in lines:\n",
        "            currentSentences = sent_tokenize(line)\n",
        "            # sentences.extend([sentence for sentence in currentSentences])\n",
        "            for sentence in currentSentences:\n",
        "                currentText = str(sentence.encode(\"ascii\", \"ignore\").decode().strip())\n",
        "                sentences.extend([currentText])\n",
        "                text = '. '.join([processSentence(sentence) for sentence in sentences])\n",
        "    return text\n",
        "\n",
        "\n",
        "# This function processes the sentence so that it can be used to produce meaningful embeddings\n",
        "def processSentence(text):\n",
        "    # Removing HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    # Removing new line characters\n",
        "    text = text.replace('\\\\n', '')\n",
        "    # Removing urls\n",
        "    # text = re.sub('[^ ]+\\.[^ ]+', ' ', text)\n",
        "    text = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]|\\(([^\\s()<>]|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''',\" \",text)\n",
        "    # Removing email addresses\n",
        "    text = re.sub(r'''\\S*@\\S*\\s?''', \" \", text)\n",
        "    # Expanding the contractions\n",
        "    text = contractions.fix(text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "EJ5wbeCrcf2u"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/My Drive/training.1600000.processed.noemoticon.csv\", encoding='mac_roman', header = None)\n",
        "df = df.sample(1600)\n",
        "df[0] = df[0].apply(lambda x: 0 if x == 0 else 1)\n",
        "df"
      ],
      "metadata": {
        "id": "6EyI8mzAc4ex",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "6d061737-f0dc-4e1c-b8c4-74691b995ef8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       0           1                             2         3                4  \\\n",
              "7854   0  1469974463  Tue Apr 07 07:58:39 PDT 2009  NO_QUERY    sassafrasstic   \n",
              "12930  1  2191843587  Tue Jun 16 06:04:11 PDT 2009  NO_QUERY         dcynaira   \n",
              "18501  1  2193188165  Tue Jun 16 08:06:57 PDT 2009  NO_QUERY  SallytheShizzle   \n",
              "1465   0  1468160042  Tue Apr 07 00:02:35 PDT 2009  NO_QUERY     jooyoung_yeu   \n",
              "5864   0  1469218042  Tue Apr 07 05:38:52 PDT 2009  NO_QUERY        uniquest1   \n",
              "...   ..         ...                           ...       ...              ...   \n",
              "17174  1  2192860072  Tue Jun 16 07:39:21 PDT 2009  NO_QUERY     ambersturgis   \n",
              "8840   0  1548496797  Fri Apr 17 21:06:59 PDT 2009  NO_QUERY     ScarlettDane   \n",
              "8314   0  1470150659  Tue Apr 07 08:29:57 PDT 2009  NO_QUERY           AlanLB   \n",
              "3840   0  1468686594  Tue Apr 07 03:09:58 PDT 2009  NO_QUERY    DivasMistress   \n",
              "15609  1  2192421256  Tue Jun 16 07:00:48 PDT 2009  NO_QUERY      danagillbro   \n",
              "\n",
              "                                                       5  \n",
              "7854   my twitter is so much more boring without @joh...  \n",
              "12930  Dreams are nothing but nightmares if you don't...  \n",
              "18501  @wimjimjam hahaha! Well all I can say is... YO...  \n",
              "1465   oh god one of the teachers here gave me a rott...  \n",
              "5864   why do i only think of good april fools jokes ...  \n",
              "...                                                  ...  \n",
              "17174  Finished that paper with an hour and a half to...  \n",
              "8840   @Chels_V I am good, just about to leave for Fo...  \n",
              "8314   @tgadget Thanks for your tweet, but I am   Hop...  \n",
              "3840   @nikkiwoods Exactamundo!!! For some reason I t...  \n",
              "15609                         @IndigoBlue68  So right!!   \n",
              "\n",
              "[1600 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-55f9a502-f321-44e1-b0b0-1f62b3ee77af\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7854</th>\n",
              "      <td>0</td>\n",
              "      <td>1469974463</td>\n",
              "      <td>Tue Apr 07 07:58:39 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>sassafrasstic</td>\n",
              "      <td>my twitter is so much more boring without @joh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12930</th>\n",
              "      <td>1</td>\n",
              "      <td>2191843587</td>\n",
              "      <td>Tue Jun 16 06:04:11 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>dcynaira</td>\n",
              "      <td>Dreams are nothing but nightmares if you don't...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18501</th>\n",
              "      <td>1</td>\n",
              "      <td>2193188165</td>\n",
              "      <td>Tue Jun 16 08:06:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>SallytheShizzle</td>\n",
              "      <td>@wimjimjam hahaha! Well all I can say is... YO...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1465</th>\n",
              "      <td>0</td>\n",
              "      <td>1468160042</td>\n",
              "      <td>Tue Apr 07 00:02:35 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>jooyoung_yeu</td>\n",
              "      <td>oh god one of the teachers here gave me a rott...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5864</th>\n",
              "      <td>0</td>\n",
              "      <td>1469218042</td>\n",
              "      <td>Tue Apr 07 05:38:52 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>uniquest1</td>\n",
              "      <td>why do i only think of good april fools jokes ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17174</th>\n",
              "      <td>1</td>\n",
              "      <td>2192860072</td>\n",
              "      <td>Tue Jun 16 07:39:21 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>ambersturgis</td>\n",
              "      <td>Finished that paper with an hour and a half to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8840</th>\n",
              "      <td>0</td>\n",
              "      <td>1548496797</td>\n",
              "      <td>Fri Apr 17 21:06:59 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>ScarlettDane</td>\n",
              "      <td>@Chels_V I am good, just about to leave for Fo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8314</th>\n",
              "      <td>0</td>\n",
              "      <td>1470150659</td>\n",
              "      <td>Tue Apr 07 08:29:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>AlanLB</td>\n",
              "      <td>@tgadget Thanks for your tweet, but I am   Hop...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3840</th>\n",
              "      <td>0</td>\n",
              "      <td>1468686594</td>\n",
              "      <td>Tue Apr 07 03:09:58 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>DivasMistress</td>\n",
              "      <td>@nikkiwoods Exactamundo!!! For some reason I t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15609</th>\n",
              "      <td>1</td>\n",
              "      <td>2192421256</td>\n",
              "      <td>Tue Jun 16 07:00:48 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>danagillbro</td>\n",
              "      <td>@IndigoBlue68  So right!!</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1600 rows × 6 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-55f9a502-f321-44e1-b0b0-1f62b3ee77af')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-55f9a502-f321-44e1-b0b0-1f62b3ee77af button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-55f9a502-f321-44e1-b0b0-1f62b3ee77af');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-809074d7-08ee-4bf9-afed-21604dfb6084\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-809074d7-08ee-4bf9-afed-21604dfb6084')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-809074d7-08ee-4bf9-afed-21604dfb6084 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_d79bc687-be76-42f5-80d8-d2b1d661efcd\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_d79bc687-be76-42f5-80d8-d2b1d661efcd button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "comments = list(df[5])\n",
        "labels = list(df[0])"
      ],
      "metadata": {
        "id": "oBnw9yCUkwUe"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizing using RobertaTokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "encodings = tokenizer.batch_encode_plus(comments, max_length = 512, pad_to_max_length = True)\n",
        "\n",
        "print('tokenizer outputs: ', encodings.keys())"
      ],
      "metadata": {
        "id": "xudYd6k7fV5D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe623757-da26-4ce3-a2fc-bf13f256c137"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenizer outputs:  dict_keys(['input_ids', 'attention_mask'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# getting tokenized and encoded sentences and attention masks\n",
        "input_ids = encodings['input_ids']\n",
        "attention_masks = encodings['attention_mask']"
      ],
      "metadata": {
        "id": "HR1XOxsxdf3F"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks = train_test_split(input_ids, labels, attention_masks, random_state = 303, test_size = 500)\n",
        "\n",
        "print(\"x_train\", len(train_inputs))\n",
        "print(\"x_vad\", len(validation_inputs))\n",
        "print(\"y_train\", len(train_labels))\n",
        "print(\"y_vad\", len(validation_labels))"
      ],
      "metadata": {
        "id": "bd8zWZsSl3nb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "261d6f6a-28fe-4219-8618-8c9641e886b5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train 1100\n",
            "x_vad 500\n",
            "y_train 1100\n",
            "y_vad 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting all of our data into torch tensors, the required datatype for our model\n",
        "train_inputs = torch.tensor(input_ids)\n",
        "train_labels = torch.tensor(labels)\n",
        "train_masks = torch.tensor(attention_masks)\n",
        "\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "metadata": {
        "id": "J-S5u4Q9i2zr"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, an iterator the entire dataset does not need to be loaded into memory\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler = train_sampler, batch_size = 16)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler = validation_sampler, batch_size = 16)"
      ],
      "metadata": {
        "id": "k_ZAQECtlZGf"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model, the pretrained model will include a single linear classification layer on top for classification.\n",
        "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels = 2)\n",
        "model.cuda()"
      ],
      "metadata": {
        "id": "CgtsRhfnjDAa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9449a54-857b-41a5-f68d-d3e6605e2dc7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaForSequenceClassification(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): RobertaClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# setting custom optimization parameters.\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {\n",
        "        'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "        'weight_decay_rate': 0.01\n",
        "    },\n",
        "    {\n",
        "        'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "        'weight_decay_rate': 0.0\n",
        "    }\n",
        "]\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr = 2e-5, correct_bias = True)"
      ],
      "metadata": {
        "id": "Q_oW5rce4tO9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9deac6e7-b388-4427-b727-7fe04dd0577c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Model\n",
        "torch.cuda.memory_summary(device = None, abbreviated = False)"
      ],
      "metadata": {
        "id": "RWFZPk2Kkgin",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "a47eb060-1aeb-48fb-e3a5-a762844fe3b5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      | 488175 KiB | 488175 KiB | 488175 KiB |      0 B   |\\n|       from large pool | 487680 KiB | 487680 KiB | 487680 KiB |      0 B   |\\n|       from small pool |    495 KiB |    495 KiB |    495 KiB |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Active memory         | 488175 KiB | 488175 KiB | 488175 KiB |      0 B   |\\n|       from large pool | 487680 KiB | 487680 KiB | 487680 KiB |      0 B   |\\n|       from small pool |    495 KiB |    495 KiB |    495 KiB |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Requested memory      | 486911 KiB | 486911 KiB | 486911 KiB |      0 B   |\\n|       from large pool | 486417 KiB | 486417 KiB | 486417 KiB |      0 B   |\\n|       from small pool |    494 KiB |    494 KiB |    494 KiB |      0 B   |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   | 542720 KiB | 542720 KiB | 542720 KiB |      0 B   |\\n|       from large pool | 540672 KiB | 540672 KiB | 540672 KiB |      0 B   |\\n|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |  54544 KiB |  54554 KiB | 265207 KiB | 210662 KiB |\\n|       from large pool |  52992 KiB |  52992 KiB | 263162 KiB | 210170 KiB |\\n|       from small pool |   1552 KiB |   2045 KiB |   2045 KiB |    492 KiB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |     203    |     203    |     203    |       0    |\\n|       from large pool |      75    |      75    |      75    |       0    |\\n|       from small pool |     128    |     128    |     128    |       0    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |     203    |     203    |     203    |       0    |\\n|       from large pool |      75    |      75    |      75    |       0    |\\n|       from small pool |     128    |     128    |     128    |       0    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |      21    |      21    |      21    |       0    |\\n|       from large pool |      20    |      20    |      20    |       0    |\\n|       from small pool |       1    |       1    |       1    |       0    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |      19    |      19    |      20    |       1    |\\n|       from large pool |      18    |      18    |      19    |       1    |\\n|       from small pool |       1    |       1    |       1    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Store our loss and accuracy for plotting\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "train_loss_set = []\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 3\n",
        "\n",
        "# trange is a tqdm wrapper around the normal python range\n",
        "for _ in trange(epochs, desc = \"Epoch\"):\n",
        "\n",
        "    # Training\n",
        "\n",
        "    # Set our model to training mode (as opposed to evaluation mode)\n",
        "    model.train()\n",
        "\n",
        "    # Tracking variables\n",
        "    tr_loss = 0 #running loss\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "\n",
        "    # Train the data for one epoch\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        # print(b_input_ids)\n",
        "        # Clear out the gradients (by default they accumulate)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # # Forward pass for multiclass classification\n",
        "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "        # print(\"output type: \", type(outputs))\n",
        "        loss = outputs[0].float()\n",
        "        # print(\"loss type: \", type(loss.item()))\n",
        "        logits = outputs[1].float()\n",
        "\n",
        "        # Forward pass for multilabel classification\n",
        "        # outputs = model(b_input_ids, attention_mask = b_input_mask)\n",
        "        # logits = outputs[0]\n",
        "        # loss_func = BCEWithLogitsLoss()\n",
        "        # loss = loss_func(logits.view(-1,2), b_labels.type_as(logits).view(-1,2)) #convert labels to float for calculation\n",
        "        # loss_func = BCELoss()\n",
        "        # loss = loss_func(torch.sigmoid(logits.view(-1,num_labels)),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation\n",
        "        # train_loss_set.append(loss.item())\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient\n",
        "        optimizer.step()\n",
        "        # scheduler.step()\n",
        "\n",
        "        # Update tracking variables\n",
        "        tr_loss += loss.item()\n",
        "        nb_tr_examples += b_input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "\n",
        "        print(\"Train loss: {}\".format(tr_loss / nb_tr_steps))\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "    # Validation\n",
        "\n",
        "    # Put model in evaluation mode to evaluate loss on the validation set\n",
        "    model.eval()\n",
        "\n",
        "    # Variables to gather full output\n",
        "    logit_preds, true_labels, pred_labels, tokenized_texts = [],[],[],[]\n",
        "\n",
        "    # Predict\n",
        "    for i, batch in enumerate(validation_dataloader):\n",
        "\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        # print(b_input_ids)\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # Forward pass\n",
        "            outs = model(b_input_ids, attention_mask=b_input_mask)\n",
        "            b_logit_pred = outs[0]\n",
        "            pred_label = torch.sigmoid(b_logit_pred)\n",
        "\n",
        "            b_logit_pred = b_logit_pred.detach().cpu().numpy()\n",
        "            pred_label = pred_label.to('cpu').numpy()\n",
        "            b_labels = b_labels.to('cpu').numpy()\n",
        "\n",
        "        tokenized_texts.append(b_input_ids)\n",
        "        logit_preds.append(b_logit_pred)\n",
        "        true_labels.append(b_labels)\n",
        "        pred_labels.append(pred_label)\n",
        "\n",
        "    # Flatten outputs\n",
        "    pred_labels = [item for sublist in pred_labels for item in sublist]\n",
        "    true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "    # Calculate Accuracy\n",
        "    threshold = 0.50\n",
        "    pred_bools = [pl > threshold for pl in pred_labels]\n",
        "    true_bools = [tl == 1 for tl in true_labels]\n",
        "    print(true_bools)\n",
        "    print(pred_bools)\n",
        "    #val_f1_accuracy = f1_score(true_bools,pred_bools,average='micro')*100\n",
        "    #val_flat_accuracy = accuracy_score(true_bools, pred_bools)*100\n",
        "\n",
        "  #print('F1 Validation Accuracy: ', val_f1_accuracy)\n",
        "  #print('Flat Validation Accuracy: ', val_flat_accuracy)"
      ],
      "metadata": {
        "id": "_Z2OHWLy5ZIE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56927ff8-597f-4188-e06b-6c16ab58e8a8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:   0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.6501819491386414\n",
            "Train loss: 0.6806412935256958\n",
            "Train loss: 0.692043681939443\n",
            "Train loss: 0.6987106502056122\n",
            "Train loss: 0.6867193937301636\n",
            "Train loss: 0.6954268217086792\n",
            "Train loss: 0.6897677523749215\n",
            "Train loss: 0.6909393519163132\n",
            "Train loss: 0.688995467291938\n",
            "Train loss: 0.6890218198299408\n",
            "Train loss: 0.6899993473833258\n",
            "Train loss: 0.6927060981591543\n",
            "Train loss: 0.6912597326131967\n",
            "Train loss: 0.6926233768463135\n",
            "Train loss: 0.692995572090149\n",
            "Train loss: 0.6919585764408112\n",
            "Train loss: 0.6881395753692178\n",
            "Train loss: 0.6883550021383498\n",
            "Train loss: 0.6867475509643555\n",
            "Train loss: 0.6880679816007614\n",
            "Train loss: 0.6893828738303411\n",
            "Train loss: 0.6911164359612898\n",
            "Train loss: 0.6910263455432394\n",
            "Train loss: 0.6913411517937978\n",
            "Train loss: 0.6918106436729431\n",
            "Train loss: 0.6917380186227652\n",
            "Train loss: 0.6911037740884004\n",
            "Train loss: 0.6909978943211692\n",
            "Train loss: 0.6897773722122456\n",
            "Train loss: 0.6887851496537526\n",
            "Train loss: 0.6878763302679984\n",
            "Train loss: 0.6877161078155041\n",
            "Train loss: 0.6862621487993182\n",
            "Train loss: 0.6856650236774894\n",
            "Train loss: 0.6852846741676331\n",
            "Train loss: 0.6855530159340965\n",
            "Train loss: 0.6842017399298178\n",
            "Train loss: 0.6848495273213637\n",
            "Train loss: 0.6832628188989102\n",
            "Train loss: 0.6832494914531708\n",
            "Train loss: 0.6834864165724778\n",
            "Train loss: 0.6825667491980961\n",
            "Train loss: 0.6817810660184815\n",
            "Train loss: 0.6807322583415292\n",
            "Train loss: 0.6796471728218927\n",
            "Train loss: 0.678914127142533\n",
            "Train loss: 0.6769144332155268\n",
            "Train loss: 0.6748982792099317\n",
            "Train loss: 0.6726707803959749\n",
            "Train loss: 0.6720264208316803\n",
            "Train loss: 0.6678043831797207\n",
            "Train loss: 0.665567068526378\n",
            "Train loss: 0.6656821139578549\n",
            "Train loss: 0.6637565322496273\n",
            "Train loss: 0.6604147569699721\n",
            "Train loss: 0.659821060619184\n",
            "Train loss: 0.6583725731623801\n",
            "Train loss: 0.6581331054712164\n",
            "Train loss: 0.6541449846857685\n",
            "Train loss: 0.648805928726991\n",
            "Train loss: 0.6479601669506948\n",
            "Train loss: 0.6434706156292269\n",
            "Train loss: 0.6400619560763949\n",
            "Train loss: 0.6357571054250002\n",
            "Train loss: 0.6334854873327108\n",
            "Train loss: 0.6324315933567105\n",
            "Train loss: 0.6289522616720912\n",
            "Train loss: 0.6265376175151152\n",
            "Train loss: 0.6236878205900607\n",
            "Train loss: 0.6224079093762807\n",
            "Train loss: 0.6217811162203131\n",
            "Train loss: 0.6195487301382754\n",
            "Train loss: 0.6197615931295368\n",
            "Train loss: 0.6143523660060521\n",
            "Train loss: 0.6118404285113017\n",
            "Train loss: 0.6094160534833607\n",
            "Train loss: 0.6141446441799016\n",
            "Train loss: 0.6125905211919394\n",
            "Train loss: 0.6116109109377559\n",
            "Train loss: 0.6081431325525045\n",
            "Train loss: 0.6074865508226701\n",
            "Train loss: 0.6041559264427279\n",
            "Train loss: 0.6024557628545416\n",
            "Train loss: 0.5977548614499115\n",
            "Train loss: 0.5975963745047065\n",
            "Train loss: 0.5976018141521964\n",
            "Train loss: 0.594326551104414\n",
            "Train loss: 0.5914725080471147\n",
            "Train loss: 0.5906509711836161\n",
            "Train loss: 0.5895959054430325\n",
            "Train loss: 0.5895611278630875\n",
            "Train loss: 0.58855258009356\n",
            "Train loss: 0.5860783526653884\n",
            "Train loss: 0.5821603558799053\n",
            "Train loss: 0.580976803992924\n",
            "Train loss: 0.5780522096902132\n",
            "Train loss: 0.5748258300663269\n",
            "Train loss: 0.5718288290865567\n",
            "Train loss: 0.5714379467747428\n",
            "Train loss: 0.5715299525856972\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  33%|███▎      | 1/3 [02:24<04:48, 144.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[False, True, False, False, True, True, True, True, False, True, True, False, False, False, True, True, True, False, True, False, False, True, False, False, True, False, True, True, True, False, False, True, False, False, False, True, True, False, False, False, False, False, True, False, True, False, True, True, True, True, False, False, False, True, False, False, True, False, False, True, True, True, False, True, True, True, False, False, False, False, True, True, True, True, True, True, False, False, False, False, True, False, True, False, True, False, False, True, False, True, False, False, True, True, False, False, False, True, False, False, False, False, False, True, False, False, False, False, True, False, False, True, True, True, True, True, False, True, False, False, True, True, True, False, False, False, True, False, False, False, False, True, False, False, True, True, False, False, False, True, True, False, False, True, False, False, True, False, False, False, True, False, True, True, False, True, False, True, True, True, True, True, True, True, False, True, True, False, False, False, True, False, False, True, False, False, True, False, False, True, False, False, True, False, False, False, False, True, False, False, True, True, False, False, False, False, True, True, False, True, False, True, False, False, True, False, False, False, True, True, True, False, False, True, False, False, False, True, True, True, False, False, True, True, False, True, True, True, False, False, False, False, False, True, True, True, False, True, True, False, False, False, False, False, True, True, True, False, False, False, False, False, True, True, False, False, False, False, True, False, True, False, False, True, True, True, True, True, False, False, False, False, True, True, True, True, False, True, True, True, False, False, False, True, True, False, False, False, True, True, True, False, False, True, False, True, False, True, False, True, True, True, False, False, True, False, False, False, True, True, False, False, False, True, True, True, True, False, False, False, False, False, False, False, False, False, False, True, True, False, False, True, True, False, True, False, True, True, False, True, False, False, True, False, False, True, False, True, False, True, False, True, True, True, False, False, False, True, True, False, False, True, False, False, False, False, True, True, True, True, True, True, True, True, True, True, False, False, True, True, True, False, False, True, True, False, True, True, True, False, True, False, True, True, True, False, True, False, True, False, False, False, False, True, False, False, True, True, False, False, True, False, True, True, False, False, True, False, True, False, True, False, False, True, True, True, False, True, True, True, False, False, True, False, True, True, True, True, False, False, False, True, True, False, False, True, True, True, True, False, False, False, True, True, True, True, False, True, False, True, False, True, False, False, True, False, True, True, True, False, False, True, True, False, True, True, False, False, True, False, True, False, False, False, False, True, True, False, False, True, True, True, False, False, True, False, False, True, False, False]\n",
            "[array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([False, False]), array([False, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([False, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([False, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([False, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([False, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([False, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([False,  True]), array([False, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([False, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([False, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([False, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([False, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([False, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([False, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False])]\n",
            "Train loss: 0.3330604135990143\n",
            "Train loss: 0.35344433784484863\n",
            "Train loss: 0.3533478379249573\n",
            "Train loss: 0.34975526481866837\n",
            "Train loss: 0.35735536813735963\n",
            "Train loss: 0.40055279930432636\n",
            "Train loss: 0.3884436232703073\n",
            "Train loss: 0.36313855461776257\n",
            "Train loss: 0.3526431636677848\n",
            "Train loss: 0.34319734424352644\n",
            "Train loss: 0.35199105875058606\n",
            "Train loss: 0.38772768899798393\n",
            "Train loss: 0.4133010105444835\n",
            "Train loss: 0.40915815851518084\n",
            "Train loss: 0.4128403057654699\n",
            "Train loss: 0.40502425003796816\n",
            "Train loss: 0.40573977459879484\n",
            "Train loss: 0.3982830039328999\n",
            "Train loss: 0.4047331049254066\n",
            "Train loss: 0.3975356765091419\n",
            "Train loss: 0.3966149595521745\n",
            "Train loss: 0.4033449014479464\n",
            "Train loss: 0.40472802843736566\n",
            "Train loss: 0.3994701970368624\n",
            "Train loss: 0.3931557887792587\n",
            "Train loss: 0.3907185305769627\n",
            "Train loss: 0.3922230684094959\n",
            "Train loss: 0.3940693645605019\n",
            "Train loss: 0.40265501521784686\n",
            "Train loss: 0.4009726946552595\n",
            "Train loss: 0.4017827054185252\n",
            "Train loss: 0.40619291784241796\n",
            "Train loss: 0.40735698513912433\n",
            "Train loss: 0.41099614299395504\n",
            "Train loss: 0.4155386247805187\n",
            "Train loss: 0.4146534192065398\n",
            "Train loss: 0.412153884768486\n",
            "Train loss: 0.41057592120609787\n",
            "Train loss: 0.4087115759268785\n",
            "Train loss: 0.40788710974156855\n",
            "Train loss: 0.4053825776024563\n",
            "Train loss: 0.40272288272778195\n",
            "Train loss: 0.40058850029180215\n",
            "Train loss: 0.4032755863260139\n",
            "Train loss: 0.3981028315093782\n",
            "Train loss: 0.39662259072065353\n",
            "Train loss: 0.39183410748522335\n",
            "Train loss: 0.3879315402979652\n",
            "Train loss: 0.39311751753700025\n",
            "Train loss: 0.3929078021645546\n",
            "Train loss: 0.39439567719020097\n",
            "Train loss: 0.3963160632321468\n",
            "Train loss: 0.4025066374045498\n",
            "Train loss: 0.40126530412170625\n",
            "Train loss: 0.4038266948678277\n",
            "Train loss: 0.40284802418734345\n",
            "Train loss: 0.40122156629436895\n",
            "Train loss: 0.4011632499509844\n",
            "Train loss: 0.39990182877597164\n",
            "Train loss: 0.40051340286930404\n",
            "Train loss: 0.4038181874107142\n",
            "Train loss: 0.4054640668534463\n",
            "Train loss: 0.4056282130971787\n",
            "Train loss: 0.40131488000042737\n",
            "Train loss: 0.405353447336417\n",
            "Train loss: 0.410675139363968\n",
            "Train loss: 0.41616793644072403\n",
            "Train loss: 0.4202195153955151\n",
            "Train loss: 0.4186102249052214\n",
            "Train loss: 0.41861149264233455\n",
            "Train loss: 0.4180364346420261\n",
            "Train loss: 0.4183848299500015\n",
            "Train loss: 0.4184656063579533\n",
            "Train loss: 0.4163024785953599\n",
            "Train loss: 0.4153732095162074\n",
            "Train loss: 0.4158393075983775\n",
            "Train loss: 0.4138940373411426\n",
            "Train loss: 0.4120493135773219\n",
            "Train loss: 0.4118916846906083\n",
            "Train loss: 0.4106890870258212\n",
            "Train loss: 0.410902382231053\n",
            "Train loss: 0.40908464625841234\n",
            "Train loss: 0.4107202648757452\n",
            "Train loss: 0.4091857432254723\n",
            "Train loss: 0.40989582310704625\n",
            "Train loss: 0.4092306295453116\n",
            "Train loss: 0.40783240106599083\n",
            "Train loss: 0.41306930289349775\n",
            "Train loss: 0.4098418222719364\n",
            "Train loss: 0.40906086017688115\n",
            "Train loss: 0.4067086115643218\n",
            "Train loss: 0.40695583464010904\n",
            "Train loss: 0.4049797923334183\n",
            "Train loss: 0.4028709148789974\n",
            "Train loss: 0.4030984952261573\n",
            "Train loss: 0.4000708628445864\n",
            "Train loss: 0.3985303818565054\n",
            "Train loss: 0.39743421454818884\n",
            "Train loss: 0.40036433573925134\n",
            "Train loss: 0.4015069556236267\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  67%|██████▋   | 2/3 [04:56<02:28, 148.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[False, True, False, False, True, True, True, True, False, True, True, False, False, False, True, True, True, False, True, False, False, True, False, False, True, False, True, True, True, False, False, True, False, False, False, True, True, False, False, False, False, False, True, False, True, False, True, True, True, True, False, False, False, True, False, False, True, False, False, True, True, True, False, True, True, True, False, False, False, False, True, True, True, True, True, True, False, False, False, False, True, False, True, False, True, False, False, True, False, True, False, False, True, True, False, False, False, True, False, False, False, False, False, True, False, False, False, False, True, False, False, True, True, True, True, True, False, True, False, False, True, True, True, False, False, False, True, False, False, False, False, True, False, False, True, True, False, False, False, True, True, False, False, True, False, False, True, False, False, False, True, False, True, True, False, True, False, True, True, True, True, True, True, True, False, True, True, False, False, False, True, False, False, True, False, False, True, False, False, True, False, False, True, False, False, False, False, True, False, False, True, True, False, False, False, False, True, True, False, True, False, True, False, False, True, False, False, False, True, True, True, False, False, True, False, False, False, True, True, True, False, False, True, True, False, True, True, True, False, False, False, False, False, True, True, True, False, True, True, False, False, False, False, False, True, True, True, False, False, False, False, False, True, True, False, False, False, False, True, False, True, False, False, True, True, True, True, True, False, False, False, False, True, True, True, True, False, True, True, True, False, False, False, True, True, False, False, False, True, True, True, False, False, True, False, True, False, True, False, True, True, True, False, False, True, False, False, False, True, True, False, False, False, True, True, True, True, False, False, False, False, False, False, False, False, False, False, True, True, False, False, True, True, False, True, False, True, True, False, True, False, False, True, False, False, True, False, True, False, True, False, True, True, True, False, False, False, True, True, False, False, True, False, False, False, False, True, True, True, True, True, True, True, True, True, True, False, False, True, True, True, False, False, True, True, False, True, True, True, False, True, False, True, True, True, False, True, False, True, False, False, False, False, True, False, False, True, True, False, False, True, False, True, True, False, False, True, False, True, False, True, False, False, True, True, True, False, True, True, True, False, False, True, False, True, True, True, True, False, False, False, True, True, False, False, True, True, True, True, False, False, False, True, True, True, True, False, True, False, True, False, True, False, False, True, False, True, True, True, False, False, True, True, False, True, True, False, False, True, False, True, False, False, False, False, True, True, False, False, True, True, True, False, False, True, False, False, True, False, False]\n",
            "[array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([False, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([False, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([False, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([False, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([False, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([False, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([False, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([False, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([False, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([False, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False])]\n",
            "Train loss: 0.34351488947868347\n",
            "Train loss: 0.2784159332513809\n",
            "Train loss: 0.3650992413361867\n",
            "Train loss: 0.35354233533143997\n",
            "Train loss: 0.344635009765625\n",
            "Train loss: 0.324876569211483\n",
            "Train loss: 0.3258401176759175\n",
            "Train loss: 0.30370594561100006\n",
            "Train loss: 0.2897008392545912\n",
            "Train loss: 0.29359208047389984\n",
            "Train loss: 0.2846493883566423\n",
            "Train loss: 0.29343479375044507\n",
            "Train loss: 0.28336501121520996\n",
            "Train loss: 0.2823349280016763\n",
            "Train loss: 0.2735657513141632\n",
            "Train loss: 0.26452526450157166\n",
            "Train loss: 0.25593746453523636\n",
            "Train loss: 0.25405988552504116\n",
            "Train loss: 0.25639216327353526\n",
            "Train loss: 0.24959458000957965\n",
            "Train loss: 0.25279502393234343\n",
            "Train loss: 0.26114673404531047\n",
            "Train loss: 0.2619204213437827\n",
            "Train loss: 0.2671991117919485\n",
            "Train loss: 0.2677908769249916\n",
            "Train loss: 0.26064140320970464\n",
            "Train loss: 0.2658141302289786\n",
            "Train loss: 0.2675870627697025\n",
            "Train loss: 0.2642375223081687\n",
            "Train loss: 0.26372546578447026\n",
            "Train loss: 0.2576907356419871\n",
            "Train loss: 0.25257837073877454\n",
            "Train loss: 0.24747381246451175\n",
            "Train loss: 0.2447656147620257\n",
            "Train loss: 0.2387829396341528\n",
            "Train loss: 0.2377088780825337\n",
            "Train loss: 0.23252665120604876\n",
            "Train loss: 0.22920928160218815\n",
            "Train loss: 0.23436062353161666\n",
            "Train loss: 0.23032991020008922\n",
            "Train loss: 0.23065223226823459\n",
            "Train loss: 0.2318579904912483\n",
            "Train loss: 0.23105691226069316\n",
            "Train loss: 0.23870681928978724\n",
            "Train loss: 0.23705742582678796\n",
            "Train loss: 0.2379854213770317\n",
            "Train loss: 0.2354019009210962\n",
            "Train loss: 0.2352094651044657\n",
            "Train loss: 0.23806359398425841\n",
            "Train loss: 0.23530636809766292\n",
            "Train loss: 0.23415587659852177\n",
            "Train loss: 0.23152818910491008\n",
            "Train loss: 0.2285278319328461\n",
            "Train loss: 0.23031813382274574\n",
            "Train loss: 0.2273949415168979\n",
            "Train loss: 0.23046500481931226\n",
            "Train loss: 0.2348167219836461\n",
            "Train loss: 0.23630718662050262\n",
            "Train loss: 0.23471606314434842\n",
            "Train loss: 0.23167680222541093\n",
            "Train loss: 0.22961384467169887\n",
            "Train loss: 0.22709189072972344\n",
            "Train loss: 0.23029769229747\n",
            "Train loss: 0.22870136966230348\n",
            "Train loss: 0.23162874883184068\n",
            "Train loss: 0.23235804186851688\n",
            "Train loss: 0.23294622401025758\n",
            "Train loss: 0.23279054665609317\n",
            "Train loss: 0.23524873266401497\n",
            "Train loss: 0.23504341462893144\n",
            "Train loss: 0.23330525796807988\n",
            "Train loss: 0.23453142343916827\n",
            "Train loss: 0.23251979874625597\n",
            "Train loss: 0.2308071641805204\n",
            "Train loss: 0.23068904613455138\n",
            "Train loss: 0.23347558370350222\n",
            "Train loss: 0.23315095587016701\n",
            "Train loss: 0.2382998779320564\n",
            "Train loss: 0.2380290753384934\n",
            "Train loss: 0.23622247180901468\n",
            "Train loss: 0.23722398405273756\n",
            "Train loss: 0.23562131999287664\n",
            "Train loss: 0.23433887182050442\n",
            "Train loss: 0.23491894218715884\n",
            "Train loss: 0.2331758849322796\n",
            "Train loss: 0.2337314971224513\n",
            "Train loss: 0.23353276971256595\n",
            "Train loss: 0.2322460403242572\n",
            "Train loss: 0.23381712735536392\n",
            "Train loss: 0.2341105405241251\n",
            "Train loss: 0.23523733056672327\n",
            "Train loss: 0.23505185597130787\n",
            "Train loss: 0.2342126409933772\n",
            "Train loss: 0.2336746031378812\n",
            "Train loss: 0.2330009805920877\n",
            "Train loss: 0.2315923971667265\n",
            "Train loss: 0.23245714462755881\n",
            "Train loss: 0.23560990997571118\n",
            "Train loss: 0.23579869091962324\n",
            "Train loss: 0.2348823107406497\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 3/3 [07:28<00:00, 149.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[False, True, False, False, True, True, True, True, False, True, True, False, False, False, True, True, True, False, True, False, False, True, False, False, True, False, True, True, True, False, False, True, False, False, False, True, True, False, False, False, False, False, True, False, True, False, True, True, True, True, False, False, False, True, False, False, True, False, False, True, True, True, False, True, True, True, False, False, False, False, True, True, True, True, True, True, False, False, False, False, True, False, True, False, True, False, False, True, False, True, False, False, True, True, False, False, False, True, False, False, False, False, False, True, False, False, False, False, True, False, False, True, True, True, True, True, False, True, False, False, True, True, True, False, False, False, True, False, False, False, False, True, False, False, True, True, False, False, False, True, True, False, False, True, False, False, True, False, False, False, True, False, True, True, False, True, False, True, True, True, True, True, True, True, False, True, True, False, False, False, True, False, False, True, False, False, True, False, False, True, False, False, True, False, False, False, False, True, False, False, True, True, False, False, False, False, True, True, False, True, False, True, False, False, True, False, False, False, True, True, True, False, False, True, False, False, False, True, True, True, False, False, True, True, False, True, True, True, False, False, False, False, False, True, True, True, False, True, True, False, False, False, False, False, True, True, True, False, False, False, False, False, True, True, False, False, False, False, True, False, True, False, False, True, True, True, True, True, False, False, False, False, True, True, True, True, False, True, True, True, False, False, False, True, True, False, False, False, True, True, True, False, False, True, False, True, False, True, False, True, True, True, False, False, True, False, False, False, True, True, False, False, False, True, True, True, True, False, False, False, False, False, False, False, False, False, False, True, True, False, False, True, True, False, True, False, True, True, False, True, False, False, True, False, False, True, False, True, False, True, False, True, True, True, False, False, False, True, True, False, False, True, False, False, False, False, True, True, True, True, True, True, True, True, True, True, False, False, True, True, True, False, False, True, True, False, True, True, True, False, True, False, True, True, True, False, True, False, True, False, False, False, False, True, False, False, True, True, False, False, True, False, True, True, False, False, True, False, True, False, True, False, False, True, True, True, False, True, True, True, False, False, True, False, True, True, True, True, False, False, False, True, True, False, False, True, True, True, True, False, False, False, True, True, True, True, False, True, False, True, False, True, False, False, True, False, True, True, True, False, False, True, True, False, True, True, False, False, True, False, True, False, False, False, False, True, True, False, False, True, True, True, False, False, True, False, False, True, False, False]\n",
            "[array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([False, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([False, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([False, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([False, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([False, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([False,  True]), array([ True, False]), array([False, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([ True, False]), array([ True, False]), array([False,  True]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False]), array([False,  True]), array([ True, False]), array([ True, False])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = [\"@piamagalona hey ms. P! ur such a FAB mommie!!\"]\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "encodings = tokenizer(sample_text, pad_to_max_length = True)\n",
        "input_ids = torch.tensor(encodings['input_ids'])\n",
        "attention_masks = torch.tensor(encodings['attention_mask'] )\n",
        "input_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5ob7FbOFAoT",
        "outputId": "a612fd83-c9c5-4341-ddd1-9437664a3ecc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[    0,  1039,   642,  6009,  1073,   337,  4488, 17232, 43601,     4,\n",
              "           221,   328, 11540,   215,    10,   274,  4546,  3795, 23738, 12846,\n",
              "             2]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outs = model(input_ids.cuda(), attention_mask = attention_masks.cuda())\n",
        "b_logit_pred = outs[0]\n",
        "pred_label = torch.sigmoid(b_logit_pred)\n",
        "print(pred_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gnk-CWtI6YYp",
        "outputId": "c0fb9317-27b4-433b-8a40-7d0aadfafab5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1436, 0.8637]], device='cuda:0', grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Homework"
      ],
      "metadata": {
        "id": "TxzQHfuq9qzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Multiclass Classification Using Roberta (You should have minimum of 3 classes)"
      ],
      "metadata": {
        "id": "VA18NvJg9pDL"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset: https://www.kaggle.com/datasets/andrewmvd/cyberbullying-classification/\n",
        "df_2 = pd.read_csv(\"/content/drive/My Drive/cyberbullying_tweets.csv\", encoding='mac_roman', header = None)\n",
        "sentiment_classes = list(df_2[1].unique())\n",
        "df_2[1] = df_2[1].apply(sentiment_classes.index)\n",
        "df_2 = df_2.sample(10000)\n",
        "df_2"
      ],
      "metadata": {
        "id": "ydpx32HFOUx7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "a25453da-cd1e-42e5-f9c0-c434f10df19a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                       0  1\n",
              "2894                              Mama dukes is the best  1\n",
              "30333  IT IS NOT A BLOCK BOT IT IS A TOOL TO SHARE BL...  4\n",
              "1715                            Success. #stopWadhwa2015  1\n",
              "13007  RT @A24Brett I'm not sexist but diva matches a...  2\n",
              "39719  You can‚Äôt blame them. I mean, given the numb...  5\n",
              "...                                                  ... ..\n",
              "7682   @edgeofthesandbx @batchelorshow @FearDept Are ...  1\n",
              "42634  @BomacBrian You a lame ass nigger for talking ...  6\n",
              "1706   @PeerWorker @EvvyKube it's about ethics in jou...  1\n",
              "33722  He was chased and bullied by his junior high s...  5\n",
              "35574  Me: can we have a movie about the struggles of...  5\n",
              "\n",
              "[10000 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-48156ed5-7b1d-4eec-bb8f-49533616bd25\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2894</th>\n",
              "      <td>Mama dukes is the best</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30333</th>\n",
              "      <td>IT IS NOT A BLOCK BOT IT IS A TOOL TO SHARE BL...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1715</th>\n",
              "      <td>Success. #stopWadhwa2015</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13007</th>\n",
              "      <td>RT @A24Brett I'm not sexist but diva matches a...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39719</th>\n",
              "      <td>You can‚Äôt blame them. I mean, given the numb...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7682</th>\n",
              "      <td>@edgeofthesandbx @batchelorshow @FearDept Are ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42634</th>\n",
              "      <td>@BomacBrian You a lame ass nigger for talking ...</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1706</th>\n",
              "      <td>@PeerWorker @EvvyKube it's about ethics in jou...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33722</th>\n",
              "      <td>He was chased and bullied by his junior high s...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35574</th>\n",
              "      <td>Me: can we have a movie about the struggles of...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-48156ed5-7b1d-4eec-bb8f-49533616bd25')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-48156ed5-7b1d-4eec-bb8f-49533616bd25 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-48156ed5-7b1d-4eec-bb8f-49533616bd25');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2231448c-6f56-43ec-b5d8-c0bfe7c4e7c0\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2231448c-6f56-43ec-b5d8-c0bfe7c4e7c0')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2231448c-6f56-43ec-b5d8-c0bfe7c4e7c0 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_da613a26-bce9-4dee-a421-d5bfa194542d\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_2')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_da613a26-bce9-4dee-a421-d5bfa194542d button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_2');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweets = list(df_2[0])\n",
        "labels = list(df_2[1])"
      ],
      "metadata": {
        "id": "EosPv0OLIxO8"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encodings = tokenizer.batch_encode_plus(tweets, truncation=True, max_length=512, padding='longest')\n",
        "\n",
        "print('tokenizer outputs: ', encodings.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPVeGD27I8r1",
        "outputId": "8c6557e4-f603-4266-a90a-2467437a8daf"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenizer outputs:  dict_keys(['input_ids', 'attention_mask'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = encodings['input_ids']\n",
        "attention_masks = encodings['attention_mask']"
      ],
      "metadata": {
        "id": "yZA3wSRSJEk0"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tr_inp, val_inp, tr_lbl, val_lbl, tr_mk, val_mk = train_test_split(input_ids, labels, attention_masks, random_state=303, test_size=0.1)\n",
        "\n",
        "print(\"x_train\", len(tr_inp))\n",
        "print(\"x_vad\", len(val_inp))\n",
        "print(\"y_train\", len(tr_lbl))\n",
        "print(\"y_vad\", len(val_lbl))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJP0_-rvLi7R",
        "outputId": "bcb7ff2f-3d92-4245-ce51-ce131e7062b4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train 9000\n",
            "x_vad 1000\n",
            "y_train 9000\n",
            "y_vad 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tr_inp = torch.tensor(input_ids)\n",
        "tr_lbl = torch.tensor(labels)\n",
        "tr_mk = torch.tensor(attention_masks)\n",
        "\n",
        "val_inp = torch.tensor(val_inp)\n",
        "val_lbl = torch.tensor(val_lbl)\n",
        "val_mk = torch.tensor(val_mk)"
      ],
      "metadata": {
        "id": "H5jPhYBQMr5T"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tr_data = TensorDataset(tr_inp, tr_mk, tr_lbl)\n",
        "tr_sampler = RandomSampler(tr_data)\n",
        "tr_dloader = DataLoader(tr_data, sampler=tr_sampler, batch_size=16)\n",
        "\n",
        "val_data = TensorDataset(val_inp, val_mk, val_lbl)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dloader = DataLoader(val_data, sampler=val_sampler, batch_size=16)"
      ],
      "metadata": {
        "id": "Lb00vPPINDhO"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sen_model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=len(sentiment_classes))\n",
        "sen_model.cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWPAndFlNydx",
        "outputId": "d8aba228-bf9e-4121-f11f-20de008f819a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaForSequenceClassification(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): RobertaClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=7, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "param_optimizer = list(sen_model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {\n",
        "        'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "        'weight_decay_rate': 0.01\n",
        "    },\n",
        "    {\n",
        "        'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "        'weight_decay_rate': 0.0\n",
        "    }\n",
        "]\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, correct_bias=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wHToeFZOOfB",
        "outputId": "815251e4-7b8a-4334-aacf-f8d2f60d3516"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Model\n",
        "torch.cuda.memory_summary(device=None, abbreviated=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "sr-OO9RqOWBZ",
        "outputId": "3b790aaf-31a7-42fb-b0b3-6c1acfcfcf57"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |   2420 MiB |  11561 MiB |  13270 GiB |  13268 GiB |\\n|       from large pool |   2404 MiB |  11556 MiB |  13240 GiB |  13238 GiB |\\n|       from small pool |     16 MiB |     17 MiB |     30 GiB |     30 GiB |\\n|---------------------------------------------------------------------------|\\n| Active memory         |   2420 MiB |  11561 MiB |  13270 GiB |  13268 GiB |\\n|       from large pool |   2404 MiB |  11556 MiB |  13240 GiB |  13238 GiB |\\n|       from small pool |     16 MiB |     17 MiB |     30 GiB |     30 GiB |\\n|---------------------------------------------------------------------------|\\n| Requested memory      |   2407 MiB |  11558 MiB |  13267 GiB |  13265 GiB |\\n|       from large pool |   2391 MiB |  11552 MiB |  13237 GiB |  13235 GiB |\\n|       from small pool |     16 MiB |     17 MiB |     30 GiB |     30 GiB |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |  11874 MiB |  11874 MiB |  11874 MiB |      0 B   |\\n|       from large pool |  11856 MiB |  11856 MiB |  11856 MiB |      0 B   |\\n|       from small pool |     18 MiB |     18 MiB |     18 MiB |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory | 390415 KiB | 529268 KiB |   3027 GiB |   3027 GiB |\\n|       from large pool | 388970 KiB | 528384 KiB |   2997 GiB |   2996 GiB |\\n|       from small pool |   1445 KiB |   4058 KiB |     30 GiB |     30 GiB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |    1236    |    1236    |  442627    |  441391    |\\n|       from large pool |     377    |     413    |  271022    |  270645    |\\n|       from small pool |     859    |     859    |  171605    |  170746    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |    1236    |    1236    |  442627    |  441391    |\\n|       from large pool |     377    |     413    |  271022    |  270645    |\\n|       from small pool |     859    |     859    |  171605    |  170746    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |     195    |     195    |     195    |       0    |\\n|       from large pool |     186    |     186    |     186    |       0    |\\n|       from small pool |       9    |       9    |       9    |       0    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |      83    |     102    |  164609    |  164526    |\\n|       from large pool |      71    |      71    |   90566    |   90495    |\\n|       from small pool |      12    |      48    |   74043    |   74031    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "train_loss_set = []\n",
        "\n",
        "epochs = 3\n",
        "\n",
        "for _ in trange(epochs, desc=\"Epoch\"):\n",
        "\n",
        "    # Training\n",
        "    sen_model.train()\n",
        "\n",
        "    tr_loss = 0 #running loss\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "\n",
        "    # Train the data for one epoch\n",
        "    for step, batch in enumerate(tr_dloader):\n",
        "\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = sen_model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "        loss = outputs[0].float()\n",
        "        logits = outputs[1].float()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        tr_loss += loss.item()\n",
        "        nb_tr_examples += b_input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "\n",
        "        print(\"Iteration {} - Train loss: {}\".format(nb_tr_steps, tr_loss / nb_tr_steps))\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "    # Validation\n",
        "    sen_model.eval()\n",
        "\n",
        "    logit_preds, true_labels, pred_labels, tokenized_texts = [],[],[],[]\n",
        "\n",
        "    # Predict\n",
        "    for i, batch in enumerate(val_dloader):\n",
        "\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        with torch.no_grad():\n",
        "            outs = sen_model(b_input_ids, attention_mask=b_input_mask)\n",
        "            b_logit_pred = outs[0]\n",
        "            pred_label = torch.softmax(b_logit_pred, dim=1)\n",
        "\n",
        "            b_logit_pred = b_logit_pred.detach().cpu().numpy()\n",
        "            pred_label = pred_label.to('cpu').numpy()\n",
        "            b_labels = b_labels.to('cpu').numpy()\n",
        "\n",
        "        tokenized_texts.append(b_input_ids)\n",
        "        logit_preds.append(b_logit_pred)\n",
        "        true_labels.append(b_labels)\n",
        "        pred_labels.append(pred_label)\n",
        "\n",
        "    # Flatten outputs\n",
        "    pred_labels = [np.argmax(item) for sublist in pred_labels for item in sublist]\n",
        "    true_labels = [item for sublist in true_labels for item in sublist]\n",
        "    print(true_labels)\n",
        "    print(pred_labels)\n",
        "\n",
        "    # Calculate Accuracy\n",
        "    # threshold = 0.50\n",
        "    # pred_bools = [pl > threshold for pl in pred_labels]\n",
        "    # true_bools = [tl == 1 for tl in true_labels]\n",
        "    #val_f1_accuracy = f1_score(true_bools,pred_bools,average='micro')*100\n",
        "    #val_flat_accuracy = accuracy_score(true_bools, pred_bools)*100\n",
        "\n",
        "  #print('F1 Validation Accuracy: ', val_f1_accuracy)\n",
        "  #print('Flat Validation Accuracy: ', val_flat_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D69hMnUKOZMZ",
        "outputId": "7415a25f-f5f0-4c2a-94a4-ce44560d9202"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:   0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1 - Train loss: 0.358257532119751\n",
            "Iteration 2 - Train loss: 0.32740503549575806\n",
            "Iteration 3 - Train loss: 0.39285608132680255\n",
            "Iteration 4 - Train loss: 0.348288431763649\n",
            "Iteration 5 - Train loss: 0.33591177463531496\n",
            "Iteration 6 - Train loss: 0.3368089447418849\n",
            "Iteration 7 - Train loss: 0.3127781025000981\n",
            "Iteration 8 - Train loss: 0.33049315959215164\n",
            "Iteration 9 - Train loss: 0.34827351570129395\n",
            "Iteration 10 - Train loss: 0.33415647149086\n",
            "Iteration 11 - Train loss: 0.32591859183528205\n",
            "Iteration 12 - Train loss: 0.30937038734555244\n",
            "Iteration 13 - Train loss: 0.31716974767354816\n",
            "Iteration 14 - Train loss: 0.3221028670668602\n",
            "Iteration 15 - Train loss: 0.3464658945798874\n",
            "Iteration 16 - Train loss: 0.344036647118628\n",
            "Iteration 17 - Train loss: 0.3472887584391762\n",
            "Iteration 18 - Train loss: 0.35431865768300164\n",
            "Iteration 19 - Train loss: 0.37684538725175354\n",
            "Iteration 20 - Train loss: 0.381226959079504\n",
            "Iteration 21 - Train loss: 0.37194051487105234\n",
            "Iteration 22 - Train loss: 0.3786446763710542\n",
            "Iteration 23 - Train loss: 0.3795469636502473\n",
            "Iteration 24 - Train loss: 0.3815428155163924\n",
            "Iteration 25 - Train loss: 0.38303906679153443\n",
            "Iteration 26 - Train loss: 0.3967633545398712\n",
            "Iteration 27 - Train loss: 0.3983109670656699\n",
            "Iteration 28 - Train loss: 0.39328011338199886\n",
            "Iteration 29 - Train loss: 0.39143829099063215\n",
            "Iteration 30 - Train loss: 0.3863477796316147\n",
            "Iteration 31 - Train loss: 0.3799838029569195\n",
            "Iteration 32 - Train loss: 0.3793503437191248\n",
            "Iteration 33 - Train loss: 0.38694129206917505\n",
            "Iteration 34 - Train loss: 0.3930403099340551\n",
            "Iteration 35 - Train loss: 0.39408106548445565\n",
            "Iteration 36 - Train loss: 0.3873267033033901\n",
            "Iteration 37 - Train loss: 0.3798806061213081\n",
            "Iteration 38 - Train loss: 0.38177952307619545\n",
            "Iteration 39 - Train loss: 0.3786518136278177\n",
            "Iteration 40 - Train loss: 0.37471672538667916\n",
            "Iteration 41 - Train loss: 0.3764791795756759\n",
            "Iteration 42 - Train loss: 0.37569059520250275\n",
            "Iteration 43 - Train loss: 0.37380290880452754\n",
            "Iteration 44 - Train loss: 0.3695536487820474\n",
            "Iteration 45 - Train loss: 0.3640582167439991\n",
            "Iteration 46 - Train loss: 0.3699761463896088\n",
            "Iteration 47 - Train loss: 0.36989897584661524\n",
            "Iteration 48 - Train loss: 0.37070144930233556\n",
            "Iteration 49 - Train loss: 0.37148705885118366\n",
            "Iteration 50 - Train loss: 0.37105951458215714\n",
            "Iteration 51 - Train loss: 0.3703683828021966\n",
            "Iteration 52 - Train loss: 0.3665179143158289\n",
            "Iteration 53 - Train loss: 0.3650695470706472\n",
            "Iteration 54 - Train loss: 0.36008977407106646\n",
            "Iteration 55 - Train loss: 0.36166309023445303\n",
            "Iteration 56 - Train loss: 0.3571094827992575\n",
            "Iteration 57 - Train loss: 0.3558497261582759\n",
            "Iteration 58 - Train loss: 0.35839828129472406\n",
            "Iteration 59 - Train loss: 0.35782236315436283\n",
            "Iteration 60 - Train loss: 0.366578653951486\n",
            "Iteration 61 - Train loss: 0.36596300709443014\n",
            "Iteration 62 - Train loss: 0.36325216894188234\n",
            "Iteration 63 - Train loss: 0.3632630395983893\n",
            "Iteration 64 - Train loss: 0.3606698107905686\n",
            "Iteration 65 - Train loss: 0.3589192080956239\n",
            "Iteration 66 - Train loss: 0.3595096313140609\n",
            "Iteration 67 - Train loss: 0.3601619154214859\n",
            "Iteration 68 - Train loss: 0.3576180246822974\n",
            "Iteration 69 - Train loss: 0.3589632943056632\n",
            "Iteration 70 - Train loss: 0.3599941236632211\n",
            "Iteration 71 - Train loss: 0.3601128253298746\n",
            "Iteration 72 - Train loss: 0.3602345109813743\n",
            "Iteration 73 - Train loss: 0.35748400406478203\n",
            "Iteration 74 - Train loss: 0.3563051294233348\n",
            "Iteration 75 - Train loss: 0.35712876935799914\n",
            "Iteration 76 - Train loss: 0.35721369107302864\n",
            "Iteration 77 - Train loss: 0.35436602130338746\n",
            "Iteration 78 - Train loss: 0.35837338883907366\n",
            "Iteration 79 - Train loss: 0.35758033489124685\n",
            "Iteration 80 - Train loss: 0.3568879825994372\n",
            "Iteration 81 - Train loss: 0.35761750756222527\n",
            "Iteration 82 - Train loss: 0.3597788161984304\n",
            "Iteration 83 - Train loss: 0.3593314536005618\n",
            "Iteration 84 - Train loss: 0.3576051582183157\n",
            "Iteration 85 - Train loss: 0.35731588006019593\n",
            "Iteration 86 - Train loss: 0.3563636506712714\n",
            "Iteration 87 - Train loss: 0.3559044195317674\n",
            "Iteration 88 - Train loss: 0.3573615242811767\n",
            "Iteration 89 - Train loss: 0.35677894045797626\n",
            "Iteration 90 - Train loss: 0.35779527458879684\n",
            "Iteration 91 - Train loss: 0.35867136880591677\n",
            "Iteration 92 - Train loss: 0.356416786008555\n",
            "Iteration 93 - Train loss: 0.3534750214187048\n",
            "Iteration 94 - Train loss: 0.3534913091583455\n",
            "Iteration 95 - Train loss: 0.3561040523805116\n",
            "Iteration 96 - Train loss: 0.3538591166337331\n",
            "Iteration 97 - Train loss: 0.3548935535027809\n",
            "Iteration 98 - Train loss: 0.35510068341177337\n",
            "Iteration 99 - Train loss: 0.35555499611478864\n",
            "Iteration 100 - Train loss: 0.3580144941806793\n",
            "Iteration 101 - Train loss: 0.3579482182417766\n",
            "Iteration 102 - Train loss: 0.35995590044002906\n",
            "Iteration 103 - Train loss: 0.36235137587612115\n",
            "Iteration 104 - Train loss: 0.36099250242114067\n",
            "Iteration 105 - Train loss: 0.36054582794507345\n",
            "Iteration 106 - Train loss: 0.35943498082880704\n",
            "Iteration 107 - Train loss: 0.36034511210762454\n",
            "Iteration 108 - Train loss: 0.3603827721542782\n",
            "Iteration 109 - Train loss: 0.3583877543243793\n",
            "Iteration 110 - Train loss: 0.3580277464606545\n",
            "Iteration 111 - Train loss: 0.3602061824755626\n",
            "Iteration 112 - Train loss: 0.36095260695687365\n",
            "Iteration 113 - Train loss: 0.36146153470056247\n",
            "Iteration 114 - Train loss: 0.3605207024436248\n",
            "Iteration 115 - Train loss: 0.3619891977828482\n",
            "Iteration 116 - Train loss: 0.36410109056481\n",
            "Iteration 117 - Train loss: 0.36232604302911675\n",
            "Iteration 118 - Train loss: 0.36125034194881633\n",
            "Iteration 119 - Train loss: 0.3615775516554087\n",
            "Iteration 120 - Train loss: 0.3607619494199753\n",
            "Iteration 121 - Train loss: 0.36229634876093586\n",
            "Iteration 122 - Train loss: 0.3622219755512769\n",
            "Iteration 123 - Train loss: 0.36452132100012247\n",
            "Iteration 124 - Train loss: 0.3633306680187102\n",
            "Iteration 125 - Train loss: 0.3631373791694641\n",
            "Iteration 126 - Train loss: 0.36457793769382296\n",
            "Iteration 127 - Train loss: 0.365938780345316\n",
            "Iteration 128 - Train loss: 0.365046106511727\n",
            "Iteration 129 - Train loss: 0.363347466486369\n",
            "Iteration 130 - Train loss: 0.3622905146617156\n",
            "Iteration 131 - Train loss: 0.36377394085622017\n",
            "Iteration 132 - Train loss: 0.36275567057909386\n",
            "Iteration 133 - Train loss: 0.3634472338104607\n",
            "Iteration 134 - Train loss: 0.3663778457401404\n",
            "Iteration 135 - Train loss: 0.36665208836396534\n",
            "Iteration 136 - Train loss: 0.3656963039846981\n",
            "Iteration 137 - Train loss: 0.3651192795186147\n",
            "Iteration 138 - Train loss: 0.36555611223414325\n",
            "Iteration 139 - Train loss: 0.3652524849493727\n",
            "Iteration 140 - Train loss: 0.3637852023754801\n",
            "Iteration 141 - Train loss: 0.3630925419060051\n",
            "Iteration 142 - Train loss: 0.3624249822656873\n",
            "Iteration 143 - Train loss: 0.3613489325438346\n",
            "Iteration 144 - Train loss: 0.36268840119656587\n",
            "Iteration 145 - Train loss: 0.36538363294354803\n",
            "Iteration 146 - Train loss: 0.3672928970358143\n",
            "Iteration 147 - Train loss: 0.3662804829425552\n",
            "Iteration 148 - Train loss: 0.36524117174180776\n",
            "Iteration 149 - Train loss: 0.36653454251737405\n",
            "Iteration 150 - Train loss: 0.36696282585461937\n",
            "Iteration 151 - Train loss: 0.3654619607309632\n",
            "Iteration 152 - Train loss: 0.36559325809541504\n",
            "Iteration 153 - Train loss: 0.366503463851081\n",
            "Iteration 154 - Train loss: 0.3685877996605712\n",
            "Iteration 155 - Train loss: 0.3683501870401444\n",
            "Iteration 156 - Train loss: 0.36900911709437\n",
            "Iteration 157 - Train loss: 0.36777024151413307\n",
            "Iteration 158 - Train loss: 0.368008614340915\n",
            "Iteration 159 - Train loss: 0.36793020048981195\n",
            "Iteration 160 - Train loss: 0.3677704270929098\n",
            "Iteration 161 - Train loss: 0.36875810904532486\n",
            "Iteration 162 - Train loss: 0.36873390976293585\n",
            "Iteration 163 - Train loss: 0.36866665910358076\n",
            "Iteration 164 - Train loss: 0.36673722820492777\n",
            "Iteration 165 - Train loss: 0.3670665916619879\n",
            "Iteration 166 - Train loss: 0.36672447500638217\n",
            "Iteration 167 - Train loss: 0.36640697507979625\n",
            "Iteration 168 - Train loss: 0.366938389527301\n",
            "Iteration 169 - Train loss: 0.3664397103987502\n",
            "Iteration 170 - Train loss: 0.3646892294725951\n",
            "Iteration 171 - Train loss: 0.3648340882875069\n",
            "Iteration 172 - Train loss: 0.3641460791316836\n",
            "Iteration 173 - Train loss: 0.36555600205081046\n",
            "Iteration 174 - Train loss: 0.3647802684927124\n",
            "Iteration 175 - Train loss: 0.3637599725808416\n",
            "Iteration 176 - Train loss: 0.36268506228754466\n",
            "Iteration 177 - Train loss: 0.3615731697550601\n",
            "Iteration 178 - Train loss: 0.36040703998355383\n",
            "Iteration 179 - Train loss: 0.36007973281198374\n",
            "Iteration 180 - Train loss: 0.3612733130239778\n",
            "Iteration 181 - Train loss: 0.36023723342306707\n",
            "Iteration 182 - Train loss: 0.3587486591231037\n",
            "Iteration 183 - Train loss: 0.3577579423948064\n",
            "Iteration 184 - Train loss: 0.3586346563356726\n",
            "Iteration 185 - Train loss: 0.3584104825918739\n",
            "Iteration 186 - Train loss: 0.35828139012058574\n",
            "Iteration 187 - Train loss: 0.3579793565652587\n",
            "Iteration 188 - Train loss: 0.3575656335959409\n",
            "Iteration 189 - Train loss: 0.35817980368143665\n",
            "Iteration 190 - Train loss: 0.3573701629905324\n",
            "Iteration 191 - Train loss: 0.3563655711983511\n",
            "Iteration 192 - Train loss: 0.3562117239537959\n",
            "Iteration 193 - Train loss: 0.35484048284088393\n",
            "Iteration 194 - Train loss: 0.35576911570177866\n",
            "Iteration 195 - Train loss: 0.35608962583236203\n",
            "Iteration 196 - Train loss: 0.3567447565800073\n",
            "Iteration 197 - Train loss: 0.3560704113868287\n",
            "Iteration 198 - Train loss: 0.35574689281709265\n",
            "Iteration 199 - Train loss: 0.35512900000541053\n",
            "Iteration 200 - Train loss: 0.35546879209578036\n",
            "Iteration 201 - Train loss: 0.3551096773117929\n",
            "Iteration 202 - Train loss: 0.3546733277121393\n",
            "Iteration 203 - Train loss: 0.3539803942729687\n",
            "Iteration 204 - Train loss: 0.35330840010269016\n",
            "Iteration 205 - Train loss: 0.35213855348709155\n",
            "Iteration 206 - Train loss: 0.3521323362094106\n",
            "Iteration 207 - Train loss: 0.3520206295421734\n",
            "Iteration 208 - Train loss: 0.35119101179477114\n",
            "Iteration 209 - Train loss: 0.35296155195059387\n",
            "Iteration 210 - Train loss: 0.3527020163834095\n",
            "Iteration 211 - Train loss: 0.35464762839802066\n",
            "Iteration 212 - Train loss: 0.35417619449490645\n",
            "Iteration 213 - Train loss: 0.3544164084842507\n",
            "Iteration 214 - Train loss: 0.3561745977081428\n",
            "Iteration 215 - Train loss: 0.35573559129653975\n",
            "Iteration 216 - Train loss: 0.3557878192169247\n",
            "Iteration 217 - Train loss: 0.3563170889349577\n",
            "Iteration 218 - Train loss: 0.3577697936342944\n",
            "Iteration 219 - Train loss: 0.35754517132424873\n",
            "Iteration 220 - Train loss: 0.35905675932087683\n",
            "Iteration 221 - Train loss: 0.35957708176040004\n",
            "Iteration 222 - Train loss: 0.3604640965056312\n",
            "Iteration 223 - Train loss: 0.3597311198110003\n",
            "Iteration 224 - Train loss: 0.3593313286546618\n",
            "Iteration 225 - Train loss: 0.3596719149417347\n",
            "Iteration 226 - Train loss: 0.3608324096200213\n",
            "Iteration 227 - Train loss: 0.3595943594031397\n",
            "Iteration 228 - Train loss: 0.3600359512002845\n",
            "Iteration 229 - Train loss: 0.3602597458393813\n",
            "Iteration 230 - Train loss: 0.3622777638228043\n",
            "Iteration 231 - Train loss: 0.3642296035052378\n",
            "Iteration 232 - Train loss: 0.3668083242815116\n",
            "Iteration 233 - Train loss: 0.3661117686094644\n",
            "Iteration 234 - Train loss: 0.3662726041725558\n",
            "Iteration 235 - Train loss: 0.36809033187145884\n",
            "Iteration 236 - Train loss: 0.36726788254612586\n",
            "Iteration 237 - Train loss: 0.36754473855223835\n",
            "Iteration 238 - Train loss: 0.36683279482506903\n",
            "Iteration 239 - Train loss: 0.36639130844980106\n",
            "Iteration 240 - Train loss: 0.3660037688290079\n",
            "Iteration 241 - Train loss: 0.3664759215106608\n",
            "Iteration 242 - Train loss: 0.3660007671506937\n",
            "Iteration 243 - Train loss: 0.366497331624659\n",
            "Iteration 244 - Train loss: 0.36726893771623004\n",
            "Iteration 245 - Train loss: 0.36703582959515707\n",
            "Iteration 246 - Train loss: 0.3661565030735683\n",
            "Iteration 247 - Train loss: 0.3667825449574814\n",
            "Iteration 248 - Train loss: 0.36563049662377567\n",
            "Iteration 249 - Train loss: 0.3664186958029088\n",
            "Iteration 250 - Train loss: 0.36562633195519445\n",
            "Iteration 251 - Train loss: 0.36530350264443817\n",
            "Iteration 252 - Train loss: 0.3644749078426569\n",
            "Iteration 253 - Train loss: 0.3643931437563519\n",
            "Iteration 254 - Train loss: 0.3636081092294276\n",
            "Iteration 255 - Train loss: 0.3633039514515914\n",
            "Iteration 256 - Train loss: 0.3637762550788466\n",
            "Iteration 257 - Train loss: 0.36333286046170077\n",
            "Iteration 258 - Train loss: 0.3627639999396579\n",
            "Iteration 259 - Train loss: 0.36212463605012674\n",
            "Iteration 260 - Train loss: 0.3617263641781532\n",
            "Iteration 261 - Train loss: 0.36186428662118325\n",
            "Iteration 262 - Train loss: 0.36086463777504807\n",
            "Iteration 263 - Train loss: 0.3603904694035479\n",
            "Iteration 264 - Train loss: 0.36145346682292945\n",
            "Iteration 265 - Train loss: 0.361844117714549\n",
            "Iteration 266 - Train loss: 0.36126092762539264\n",
            "Iteration 267 - Train loss: 0.3618256082751331\n",
            "Iteration 268 - Train loss: 0.36122969140423766\n",
            "Iteration 269 - Train loss: 0.36079611357813873\n",
            "Iteration 270 - Train loss: 0.3608262208048944\n",
            "Iteration 271 - Train loss: 0.36063939984544174\n",
            "Iteration 272 - Train loss: 0.35961168793523135\n",
            "Iteration 273 - Train loss: 0.35968583492023165\n",
            "Iteration 274 - Train loss: 0.3602758452133106\n",
            "Iteration 275 - Train loss: 0.359571136480028\n",
            "Iteration 276 - Train loss: 0.35977456041112327\n",
            "Iteration 277 - Train loss: 0.3592883468158409\n",
            "Iteration 278 - Train loss: 0.35901491829066823\n",
            "Iteration 279 - Train loss: 0.3610708726616743\n",
            "Iteration 280 - Train loss: 0.36070223155298403\n",
            "Iteration 281 - Train loss: 0.3602624148713736\n",
            "Iteration 282 - Train loss: 0.3612138222348183\n",
            "Iteration 283 - Train loss: 0.36182745514509956\n",
            "Iteration 284 - Train loss: 0.361459393657639\n",
            "Iteration 285 - Train loss: 0.3609195869481354\n",
            "Iteration 286 - Train loss: 0.36103546955472937\n",
            "Iteration 287 - Train loss: 0.36095409186148064\n",
            "Iteration 288 - Train loss: 0.360674033795173\n",
            "Iteration 289 - Train loss: 0.36079207818091535\n",
            "Iteration 290 - Train loss: 0.36131096752039316\n",
            "Iteration 291 - Train loss: 0.36182589959545236\n",
            "Iteration 292 - Train loss: 0.3618267651341141\n",
            "Iteration 293 - Train loss: 0.36229136822675273\n",
            "Iteration 294 - Train loss: 0.36302298289679347\n",
            "Iteration 295 - Train loss: 0.3627123111637972\n",
            "Iteration 296 - Train loss: 0.36283243671563026\n",
            "Iteration 297 - Train loss: 0.36268409527211076\n",
            "Iteration 298 - Train loss: 0.3623192673611561\n",
            "Iteration 299 - Train loss: 0.36197941172182757\n",
            "Iteration 300 - Train loss: 0.3613916353136301\n",
            "Iteration 301 - Train loss: 0.3614132511011786\n",
            "Iteration 302 - Train loss: 0.3621036891658969\n",
            "Iteration 303 - Train loss: 0.36132272809940597\n",
            "Iteration 304 - Train loss: 0.36133695705058544\n",
            "Iteration 305 - Train loss: 0.36095152143083636\n",
            "Iteration 306 - Train loss: 0.3609347724008794\n",
            "Iteration 307 - Train loss: 0.3603643099094835\n",
            "Iteration 308 - Train loss: 0.36013075948825907\n",
            "Iteration 309 - Train loss: 0.3606726101638816\n",
            "Iteration 310 - Train loss: 0.3608946661554998\n",
            "Iteration 311 - Train loss: 0.36034356045665467\n",
            "Iteration 312 - Train loss: 0.3607756947525419\n",
            "Iteration 313 - Train loss: 0.3604695756262103\n",
            "Iteration 314 - Train loss: 0.3610832259580968\n",
            "Iteration 315 - Train loss: 0.3609621215670828\n",
            "Iteration 316 - Train loss: 0.3616099349352755\n",
            "Iteration 317 - Train loss: 0.3607344578963349\n",
            "Iteration 318 - Train loss: 0.36059668974119163\n",
            "Iteration 319 - Train loss: 0.3610645917048649\n",
            "Iteration 320 - Train loss: 0.36088945767842234\n",
            "Iteration 321 - Train loss: 0.3603037806036316\n",
            "Iteration 322 - Train loss: 0.35977277006977093\n",
            "Iteration 323 - Train loss: 0.35950070273580936\n",
            "Iteration 324 - Train loss: 0.35895054235502527\n",
            "Iteration 325 - Train loss: 0.35870805740356443\n",
            "Iteration 326 - Train loss: 0.3580795762820478\n",
            "Iteration 327 - Train loss: 0.3583237014323564\n",
            "Iteration 328 - Train loss: 0.35832924082330087\n",
            "Iteration 329 - Train loss: 0.3585708647241708\n",
            "Iteration 330 - Train loss: 0.3584205813931696\n",
            "Iteration 331 - Train loss: 0.35814112861531017\n",
            "Iteration 332 - Train loss: 0.35849491654928906\n",
            "Iteration 333 - Train loss: 0.35778461061082445\n",
            "Iteration 334 - Train loss: 0.3597143777830158\n",
            "Iteration 335 - Train loss: 0.36022028869657374\n",
            "Iteration 336 - Train loss: 0.3598180094379045\n",
            "Iteration 337 - Train loss: 0.3596422671829558\n",
            "Iteration 338 - Train loss: 0.360579621112911\n",
            "Iteration 339 - Train loss: 0.3609772674014083\n",
            "Iteration 340 - Train loss: 0.36082569628077393\n",
            "Iteration 341 - Train loss: 0.36077082843200203\n",
            "Iteration 342 - Train loss: 0.36019134826478905\n",
            "Iteration 343 - Train loss: 0.36011515544732875\n",
            "Iteration 344 - Train loss: 0.35950948726819004\n",
            "Iteration 345 - Train loss: 0.35900280734767087\n",
            "Iteration 346 - Train loss: 0.35925876086502406\n",
            "Iteration 347 - Train loss: 0.36002705703551213\n",
            "Iteration 348 - Train loss: 0.35999718882229137\n",
            "Iteration 349 - Train loss: 0.36070579059827634\n",
            "Iteration 350 - Train loss: 0.36135957164423804\n",
            "Iteration 351 - Train loss: 0.3612388290910639\n",
            "Iteration 352 - Train loss: 0.360938111574135\n",
            "Iteration 353 - Train loss: 0.36047723083084093\n",
            "Iteration 354 - Train loss: 0.36029033998479953\n",
            "Iteration 355 - Train loss: 0.359773610847097\n",
            "Iteration 356 - Train loss: 0.3594024092210143\n",
            "Iteration 357 - Train loss: 0.35926997699156527\n",
            "Iteration 358 - Train loss: 0.3591332878064177\n",
            "Iteration 359 - Train loss: 0.3588870422172679\n",
            "Iteration 360 - Train loss: 0.3589023045781586\n",
            "Iteration 361 - Train loss: 0.35889350851982255\n",
            "Iteration 362 - Train loss: 0.3580192700645871\n",
            "Iteration 363 - Train loss: 0.35793497561176946\n",
            "Iteration 364 - Train loss: 0.3576644057469381\n",
            "Iteration 365 - Train loss: 0.35775082554310966\n",
            "Iteration 366 - Train loss: 0.3581180988805867\n",
            "Iteration 367 - Train loss: 0.3589633764338753\n",
            "Iteration 368 - Train loss: 0.35889249386103905\n",
            "Iteration 369 - Train loss: 0.35829477938936977\n",
            "Iteration 370 - Train loss: 0.35838560971456607\n",
            "Iteration 371 - Train loss: 0.3586867970798536\n",
            "Iteration 372 - Train loss: 0.35859765835426827\n",
            "Iteration 373 - Train loss: 0.357878550008219\n",
            "Iteration 374 - Train loss: 0.3575671276944206\n",
            "Iteration 375 - Train loss: 0.3575607353846232\n",
            "Iteration 376 - Train loss: 0.35726121464308275\n",
            "Iteration 377 - Train loss: 0.35800819735312017\n",
            "Iteration 378 - Train loss: 0.3577153122929669\n",
            "Iteration 379 - Train loss: 0.35736265043312765\n",
            "Iteration 380 - Train loss: 0.3575881821152411\n",
            "Iteration 381 - Train loss: 0.357305072541312\n",
            "Iteration 382 - Train loss: 0.356613729831748\n",
            "Iteration 383 - Train loss: 0.3565342106100162\n",
            "Iteration 384 - Train loss: 0.3567535554757342\n",
            "Iteration 385 - Train loss: 0.3564386101899209\n",
            "Iteration 386 - Train loss: 0.3558238337463048\n",
            "Iteration 387 - Train loss: 0.35576180166514343\n",
            "Iteration 388 - Train loss: 0.35579786576407474\n",
            "Iteration 389 - Train loss: 0.35507416460845037\n",
            "Iteration 390 - Train loss: 0.35541249891886345\n",
            "Iteration 391 - Train loss: 0.3562987310730893\n",
            "Iteration 392 - Train loss: 0.35568310225344435\n",
            "Iteration 393 - Train loss: 0.3553490083257054\n",
            "Iteration 394 - Train loss: 0.35525521806200144\n",
            "Iteration 395 - Train loss: 0.3556093056367922\n",
            "Iteration 396 - Train loss: 0.35542096637866716\n",
            "Iteration 397 - Train loss: 0.3552071857887792\n",
            "Iteration 398 - Train loss: 0.3548301185046009\n",
            "Iteration 399 - Train loss: 0.35573817158402654\n",
            "Iteration 400 - Train loss: 0.35548008888959887\n",
            "Iteration 401 - Train loss: 0.3547396476280659\n",
            "Iteration 402 - Train loss: 0.3544462401401344\n",
            "Iteration 403 - Train loss: 0.35473045333295544\n",
            "Iteration 404 - Train loss: 0.3552627509256991\n",
            "Iteration 405 - Train loss: 0.35524154838956434\n",
            "Iteration 406 - Train loss: 0.35571321577127346\n",
            "Iteration 407 - Train loss: 0.35579719557369666\n",
            "Iteration 408 - Train loss: 0.35550716322134523\n",
            "Iteration 409 - Train loss: 0.3551825076560228\n",
            "Iteration 410 - Train loss: 0.3550806698275775\n",
            "Iteration 411 - Train loss: 0.3550109497822114\n",
            "Iteration 412 - Train loss: 0.3544663253773763\n",
            "Iteration 413 - Train loss: 0.35418814160921963\n",
            "Iteration 414 - Train loss: 0.3544544597754732\n",
            "Iteration 415 - Train loss: 0.3542822858655309\n",
            "Iteration 416 - Train loss: 0.3548648458404037\n",
            "Iteration 417 - Train loss: 0.3546085471658112\n",
            "Iteration 418 - Train loss: 0.35431847471797295\n",
            "Iteration 419 - Train loss: 0.3544476203318143\n",
            "Iteration 420 - Train loss: 0.3553993655457383\n",
            "Iteration 421 - Train loss: 0.3553466942347144\n",
            "Iteration 422 - Train loss: 0.35521108431132487\n",
            "Iteration 423 - Train loss: 0.35516161392343804\n",
            "Iteration 424 - Train loss: 0.3556855787997538\n",
            "Iteration 425 - Train loss: 0.35579828840844774\n",
            "Iteration 426 - Train loss: 0.3560440324491738\n",
            "Iteration 427 - Train loss: 0.3566279575682356\n",
            "Iteration 428 - Train loss: 0.3571107403906149\n",
            "Iteration 429 - Train loss: 0.35707132622495397\n",
            "Iteration 430 - Train loss: 0.35697146186301876\n",
            "Iteration 431 - Train loss: 0.35786010013793845\n",
            "Iteration 432 - Train loss: 0.35774287543501015\n",
            "Iteration 433 - Train loss: 0.35770308444874394\n",
            "Iteration 434 - Train loss: 0.35778312711259735\n",
            "Iteration 435 - Train loss: 0.35747093075308306\n",
            "Iteration 436 - Train loss: 0.3568289160830985\n",
            "Iteration 437 - Train loss: 0.3564859883224664\n",
            "Iteration 438 - Train loss: 0.35612154638046\n",
            "Iteration 439 - Train loss: 0.35684912845254757\n",
            "Iteration 440 - Train loss: 0.356891248696907\n",
            "Iteration 441 - Train loss: 0.35656350835853695\n",
            "Iteration 442 - Train loss: 0.3567819055759799\n",
            "Iteration 443 - Train loss: 0.35653346223634735\n",
            "Iteration 444 - Train loss: 0.35640901667779096\n",
            "Iteration 445 - Train loss: 0.35585112616922077\n",
            "Iteration 446 - Train loss: 0.3557656473342346\n",
            "Iteration 447 - Train loss: 0.3552900481244062\n",
            "Iteration 448 - Train loss: 0.35536618070078213\n",
            "Iteration 449 - Train loss: 0.3550970944802055\n",
            "Iteration 450 - Train loss: 0.3548138582209746\n",
            "Iteration 451 - Train loss: 0.35430707865107613\n",
            "Iteration 452 - Train loss: 0.3539521061048835\n",
            "Iteration 453 - Train loss: 0.3538826386758823\n",
            "Iteration 454 - Train loss: 0.3537759844394245\n",
            "Iteration 455 - Train loss: 0.35381467504488245\n",
            "Iteration 456 - Train loss: 0.35455971333737435\n",
            "Iteration 457 - Train loss: 0.35411573247066996\n",
            "Iteration 458 - Train loss: 0.3542115273336396\n",
            "Iteration 459 - Train loss: 0.35403351380219905\n",
            "Iteration 460 - Train loss: 0.35394683592021464\n",
            "Iteration 461 - Train loss: 0.3544794102068061\n",
            "Iteration 462 - Train loss: 0.35496607198666186\n",
            "Iteration 463 - Train loss: 0.3550479400009882\n",
            "Iteration 464 - Train loss: 0.3546700508911805\n",
            "Iteration 465 - Train loss: 0.3547448218990398\n",
            "Iteration 466 - Train loss: 0.3550552685980889\n",
            "Iteration 467 - Train loss: 0.35492782341017204\n",
            "Iteration 468 - Train loss: 0.3550559194901815\n",
            "Iteration 469 - Train loss: 0.355460998647884\n",
            "Iteration 470 - Train loss: 0.3559814821373909\n",
            "Iteration 471 - Train loss: 0.3553759181860146\n",
            "Iteration 472 - Train loss: 0.35558013581686604\n",
            "Iteration 473 - Train loss: 0.3553930850065536\n",
            "Iteration 474 - Train loss: 0.354907906209492\n",
            "Iteration 475 - Train loss: 0.35445924729108813\n",
            "Iteration 476 - Train loss: 0.3543965695191081\n",
            "Iteration 477 - Train loss: 0.35393987600720905\n",
            "Iteration 478 - Train loss: 0.3541245803579875\n",
            "Iteration 479 - Train loss: 0.35363208917834815\n",
            "Iteration 480 - Train loss: 0.35313702882267534\n",
            "Iteration 481 - Train loss: 0.35317532044376504\n",
            "Iteration 482 - Train loss: 0.35272610618798567\n",
            "Iteration 483 - Train loss: 0.35289497095050276\n",
            "Iteration 484 - Train loss: 0.35261634042994544\n",
            "Iteration 485 - Train loss: 0.3527341075318376\n",
            "Iteration 486 - Train loss: 0.35321724986649833\n",
            "Iteration 487 - Train loss: 0.3528913973201472\n",
            "Iteration 488 - Train loss: 0.35257559894110824\n",
            "Iteration 489 - Train loss: 0.3524069916647392\n",
            "Iteration 490 - Train loss: 0.3523200220143308\n",
            "Iteration 491 - Train loss: 0.35235507675501576\n",
            "Iteration 492 - Train loss: 0.35246309463873626\n",
            "Iteration 493 - Train loss: 0.35208217341747533\n",
            "Iteration 494 - Train loss: 0.35180376727993673\n",
            "Iteration 495 - Train loss: 0.35173799520490145\n",
            "Iteration 496 - Train loss: 0.3515991567213449\n",
            "Iteration 497 - Train loss: 0.3531000301002017\n",
            "Iteration 498 - Train loss: 0.35278337546081906\n",
            "Iteration 499 - Train loss: 0.3523331872656016\n",
            "Iteration 500 - Train loss: 0.3525191162079573\n",
            "Iteration 501 - Train loss: 0.3530462094379994\n",
            "Iteration 502 - Train loss: 0.35339883852943005\n",
            "Iteration 503 - Train loss: 0.35379560145125477\n",
            "Iteration 504 - Train loss: 0.3535268409768977\n",
            "Iteration 505 - Train loss: 0.35314698899441427\n",
            "Iteration 506 - Train loss: 0.35381953172356245\n",
            "Iteration 507 - Train loss: 0.35389214585107226\n",
            "Iteration 508 - Train loss: 0.3544458673724274\n",
            "Iteration 509 - Train loss: 0.3539156650132655\n",
            "Iteration 510 - Train loss: 0.35347736841615507\n",
            "Iteration 511 - Train loss: 0.35401188803803896\n",
            "Iteration 512 - Train loss: 0.35387556547357235\n",
            "Iteration 513 - Train loss: 0.35337203253082367\n",
            "Iteration 514 - Train loss: 0.35354778854763463\n",
            "Iteration 515 - Train loss: 0.35436233478842427\n",
            "Iteration 516 - Train loss: 0.3546097986111345\n",
            "Iteration 517 - Train loss: 0.3544179365413562\n",
            "Iteration 518 - Train loss: 0.3540617770094669\n",
            "Iteration 519 - Train loss: 0.35375210573907534\n",
            "Iteration 520 - Train loss: 0.3546336113833464\n",
            "Iteration 521 - Train loss: 0.35481372927521104\n",
            "Iteration 522 - Train loss: 0.35490745648570443\n",
            "Iteration 523 - Train loss: 0.355717935935945\n",
            "Iteration 524 - Train loss: 0.35588808812712897\n",
            "Iteration 525 - Train loss: 0.3560065091223944\n",
            "Iteration 526 - Train loss: 0.35559796913041813\n",
            "Iteration 527 - Train loss: 0.3557239993134305\n",
            "Iteration 528 - Train loss: 0.35557960092344065\n",
            "Iteration 529 - Train loss: 0.3554877960501637\n",
            "Iteration 530 - Train loss: 0.3556275969406344\n",
            "Iteration 531 - Train loss: 0.356116559882622\n",
            "Iteration 532 - Train loss: 0.3560541270482809\n",
            "Iteration 533 - Train loss: 0.35609446541453393\n",
            "Iteration 534 - Train loss: 0.35642453676529146\n",
            "Iteration 535 - Train loss: 0.3561486359790107\n",
            "Iteration 536 - Train loss: 0.35625545310773954\n",
            "Iteration 537 - Train loss: 0.3560495250329403\n",
            "Iteration 538 - Train loss: 0.35628304292610585\n",
            "Iteration 539 - Train loss: 0.35581338335111984\n",
            "Iteration 540 - Train loss: 0.35554234985676075\n",
            "Iteration 541 - Train loss: 0.35564153861812214\n",
            "Iteration 542 - Train loss: 0.3555318121952984\n",
            "Iteration 543 - Train loss: 0.3560782096196055\n",
            "Iteration 544 - Train loss: 0.35657860331839936\n",
            "Iteration 545 - Train loss: 0.3569201394368749\n",
            "Iteration 546 - Train loss: 0.3571522539761259\n",
            "Iteration 547 - Train loss: 0.35722445301588973\n",
            "Iteration 548 - Train loss: 0.357656574311809\n",
            "Iteration 549 - Train loss: 0.357313659008192\n",
            "Iteration 550 - Train loss: 0.3571836305071007\n",
            "Iteration 551 - Train loss: 0.35760752641732807\n",
            "Iteration 552 - Train loss: 0.35731732649593684\n",
            "Iteration 553 - Train loss: 0.3573877117706035\n",
            "Iteration 554 - Train loss: 0.3573302437117599\n",
            "Iteration 555 - Train loss: 0.357394279593283\n",
            "Iteration 556 - Train loss: 0.35782013097844967\n",
            "Iteration 557 - Train loss: 0.3582985593102993\n",
            "Iteration 558 - Train loss: 0.35867921803747455\n",
            "Iteration 559 - Train loss: 0.3585625170248566\n",
            "Iteration 560 - Train loss: 0.3584936556406319\n",
            "Iteration 561 - Train loss: 0.3581155611471059\n",
            "Iteration 562 - Train loss: 0.35785743637918577\n",
            "Iteration 563 - Train loss: 0.3576348164974055\n",
            "Iteration 564 - Train loss: 0.3580767980889014\n",
            "Iteration 565 - Train loss: 0.35784672460461087\n",
            "Iteration 566 - Train loss: 0.35792827325817134\n",
            "Iteration 567 - Train loss: 0.35758649137337584\n",
            "Iteration 568 - Train loss: 0.35738904814852374\n",
            "Iteration 569 - Train loss: 0.357209213470847\n",
            "Iteration 570 - Train loss: 0.3569848494179416\n",
            "Iteration 571 - Train loss: 0.35713154872171515\n",
            "Iteration 572 - Train loss: 0.35730964290564293\n",
            "Iteration 573 - Train loss: 0.3575736043054812\n",
            "Iteration 574 - Train loss: 0.3573498168135977\n",
            "Iteration 575 - Train loss: 0.3572889889711919\n",
            "Iteration 576 - Train loss: 0.35777948650583213\n",
            "Iteration 577 - Train loss: 0.3577569950778728\n",
            "Iteration 578 - Train loss: 0.35785137553889446\n",
            "Iteration 579 - Train loss: 0.35773962199533543\n",
            "Iteration 580 - Train loss: 0.3574383384066409\n",
            "Iteration 581 - Train loss: 0.3569970568465694\n",
            "Iteration 582 - Train loss: 0.3567972831760895\n",
            "Iteration 583 - Train loss: 0.35644996449939176\n",
            "Iteration 584 - Train loss: 0.3564805785729869\n",
            "Iteration 585 - Train loss: 0.3563900858673275\n",
            "Iteration 586 - Train loss: 0.35630727884814195\n",
            "Iteration 587 - Train loss: 0.35600700451527584\n",
            "Iteration 588 - Train loss: 0.355627284290231\n",
            "Iteration 589 - Train loss: 0.3552983243315653\n",
            "Iteration 590 - Train loss: 0.35614256333496613\n",
            "Iteration 591 - Train loss: 0.35586148308820126\n",
            "Iteration 592 - Train loss: 0.3564228230231517\n",
            "Iteration 593 - Train loss: 0.3564428531299353\n",
            "Iteration 594 - Train loss: 0.3563409804394751\n",
            "Iteration 595 - Train loss: 0.3568611946927399\n",
            "Iteration 596 - Train loss: 0.35643783657992845\n",
            "Iteration 597 - Train loss: 0.3564388293342375\n",
            "Iteration 598 - Train loss: 0.35641586901401995\n",
            "Iteration 599 - Train loss: 0.35612669842121797\n",
            "Iteration 600 - Train loss: 0.35628144702563685\n",
            "Iteration 601 - Train loss: 0.3562573022061894\n",
            "Iteration 602 - Train loss: 0.3559814422604848\n",
            "Iteration 603 - Train loss: 0.3559374024791899\n",
            "Iteration 604 - Train loss: 0.35560188253737046\n",
            "Iteration 605 - Train loss: 0.355485548349944\n",
            "Iteration 606 - Train loss: 0.3554980479328349\n",
            "Iteration 607 - Train loss: 0.3555113547888187\n",
            "Iteration 608 - Train loss: 0.35530664444606946\n",
            "Iteration 609 - Train loss: 0.35558199213300823\n",
            "Iteration 610 - Train loss: 0.35563134786779765\n",
            "Iteration 611 - Train loss: 0.35530043647010273\n",
            "Iteration 612 - Train loss: 0.3551806867293088\n",
            "Iteration 613 - Train loss: 0.3552096872962707\n",
            "Iteration 614 - Train loss: 0.35499015619147095\n",
            "Iteration 615 - Train loss: 0.35487156392839864\n",
            "Iteration 616 - Train loss: 0.3548295356502587\n",
            "Iteration 617 - Train loss: 0.35468564204255804\n",
            "Iteration 618 - Train loss: 0.354721185302175\n",
            "Iteration 619 - Train loss: 0.354561319631887\n",
            "Iteration 620 - Train loss: 0.35411376795701444\n",
            "Iteration 621 - Train loss: 0.3538862501245574\n",
            "Iteration 622 - Train loss: 0.35381870924036984\n",
            "Iteration 623 - Train loss: 0.3535642054045564\n",
            "Iteration 624 - Train loss: 0.3533864071330008\n",
            "Iteration 625 - Train loss: 0.3530131373167038\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  33%|███▎      | 1/3 [14:49<29:39, 889.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6, 6, 5, 5, 3, 3, 3, 6, 2, 2, 5, 6, 2, 3, 5, 2, 2, 2, 6, 1, 1, 5, 1, 2, 4, 6, 5, 6, 6, 3, 1, 3, 5, 4, 4, 1, 3, 2, 3, 4, 6, 3, 3, 6, 5, 1, 3, 6, 6, 4, 3, 2, 6, 2, 6, 6, 1, 4, 5, 3, 2, 3, 4, 6, 6, 5, 4, 4, 3, 3, 1, 2, 5, 1, 4, 3, 1, 4, 6, 2, 3, 5, 1, 4, 1, 4, 3, 2, 2, 6, 6, 2, 6, 4, 6, 3, 3, 2, 4, 1, 5, 5, 4, 6, 5, 1, 5, 1, 1, 4, 4, 6, 3, 4, 2, 6, 3, 2, 1, 5, 6, 5, 5, 2, 5, 3, 2, 5, 5, 2, 2, 5, 5, 6, 6, 5, 1, 3, 1, 4, 6, 4, 6, 3, 4, 3, 6, 2, 2, 2, 6, 5, 6, 6, 1, 1, 3, 1, 4, 2, 6, 2, 1, 3, 2, 6, 3, 2, 5, 6, 1, 2, 3, 5, 2, 4, 4, 6, 6, 3, 2, 1, 3, 3, 2, 5, 4, 3, 5, 5, 3, 1, 4, 2, 5, 5, 4, 4, 4, 6, 3, 5, 6, 2, 3, 2, 5, 6, 1, 6, 3, 4, 6, 5, 3, 6, 4, 6, 6, 2, 4, 1, 3, 1, 3, 1, 1, 6, 4, 6, 3, 2, 2, 5, 5, 5, 4, 3, 6, 1, 5, 1, 1, 4, 3, 4, 2, 3, 6, 3, 3, 4, 4, 3, 6, 5, 5, 4, 1, 3, 2, 3, 5, 1, 2, 6, 6, 4, 6, 2, 2, 3, 4, 6, 3, 6, 3, 5, 5, 3, 2, 3, 1, 6, 5, 6, 6, 5, 2, 2, 4, 3, 5, 2, 3, 1, 2, 6, 5, 4, 4, 2, 6, 4, 3, 2, 6, 6, 3, 3, 5, 6, 2, 2, 6, 6, 5, 3, 4, 1, 1, 1, 5, 6, 2, 5, 1, 2, 5, 4, 4, 5, 4, 2, 3, 4, 5, 2, 2, 2, 6, 2, 3, 5, 6, 4, 4, 4, 1, 6, 2, 6, 2, 1, 1, 1, 4, 5, 4, 1, 3, 6, 6, 5, 6, 4, 6, 3, 6, 6, 1, 5, 4, 2, 3, 4, 3, 5, 1, 2, 1, 1, 5, 2, 5, 6, 2, 3, 3, 6, 4, 2, 5, 4, 4, 5, 3, 2, 4, 3, 6, 3, 3, 2, 3, 5, 3, 4, 6, 2, 4, 3, 4, 5, 2, 2, 2, 5, 4, 3, 4, 1, 5, 1, 6, 5, 2, 6, 1, 2, 4, 2, 6, 1, 6, 1, 4, 1, 4, 5, 3, 1, 1, 2, 1, 6, 4, 5, 4, 6, 4, 2, 6, 2, 6, 3, 4, 1, 5, 2, 2, 6, 2, 2, 5, 5, 5, 3, 6, 4, 3, 2, 6, 1, 2, 4, 5, 4, 5, 1, 5, 1, 4, 2, 4, 5, 3, 6, 2, 4, 5, 1, 6, 4, 5, 2, 3, 6, 5, 3, 2, 2, 5, 4, 1, 2, 6, 2, 6, 4, 2, 6, 1, 1, 3, 5, 2, 4, 3, 6, 2, 5, 5, 3, 1, 1, 5, 6, 5, 3, 2, 2, 6, 2, 4, 6, 3, 6, 3, 4, 3, 6, 4, 2, 2, 2, 2, 3, 3, 2, 1, 4, 5, 3, 3, 6, 2, 4, 4, 3, 1, 4, 5, 4, 2, 1, 2, 5, 4, 2, 3, 5, 3, 6, 3, 1, 4, 3, 4, 6, 6, 6, 6, 4, 4, 6, 1, 1, 1, 1, 3, 4, 5, 5, 1, 6, 6, 6, 3, 1, 5, 1, 1, 4, 1, 1, 3, 6, 5, 1, 6, 2, 3, 3, 6, 2, 3, 2, 3, 1, 4, 5, 3, 1, 1, 4, 4, 3, 3, 2, 4, 2, 4, 2, 5, 1, 3, 2, 4, 2, 1, 4, 3, 2, 5, 1, 3, 6, 1, 2, 1, 5, 6, 4, 5, 4, 6, 1, 6, 3, 3, 3, 4, 6, 3, 6, 2, 3, 5, 2, 3, 5, 5, 1, 4, 5, 6, 6, 2, 3, 5, 2, 2, 4, 1, 5, 1, 4, 1, 3, 5, 5, 3, 6, 5, 5, 5, 6, 5, 3, 4, 6, 5, 6, 4, 5, 6, 4, 4, 1, 2, 1, 3, 4, 1, 4, 2, 4, 2, 3, 6, 4, 3, 2, 1, 1, 5, 4, 4, 4, 5, 6, 5, 5, 5, 2, 3, 5, 5, 3, 5, 3, 5, 5, 1, 5, 6, 6, 6, 2, 4, 6, 1, 3, 4, 5, 1, 3, 1, 5, 4, 1, 2, 5, 2, 6, 6, 6, 6, 6, 6, 6, 1, 6, 3, 4, 1, 5, 6, 3, 6, 3, 1, 2, 5, 6, 6, 2, 1, 6, 6, 6, 6, 2, 3, 2, 5, 2, 6, 2, 2, 2, 4, 5, 2, 1, 3, 3, 4, 6, 1, 1, 4, 1, 2, 1, 5, 2, 6, 1, 3, 3, 1, 3, 4, 2, 6, 1, 6, 4, 4, 1, 6, 4, 4, 4, 4, 5, 3, 1, 2, 1, 2, 3, 2, 6, 5, 3, 3, 3, 2, 5, 3, 3, 2, 2, 4, 5, 1, 1, 1, 3, 1, 3, 3, 5, 5, 5, 2, 5, 1, 5, 2, 3, 3, 3, 3, 5, 2, 3, 4, 5, 4, 6, 6, 3, 3, 1, 4, 6, 6, 6, 1, 6, 1, 5, 5, 3, 2, 5, 6, 6, 3, 1, 3, 4, 1, 6, 5, 2, 6, 6, 5, 1, 2, 6, 1, 4, 2, 2, 3, 6, 4, 6, 6, 2, 4, 6, 5, 5, 3, 4, 5, 6, 5, 3, 2, 3, 6, 4, 4, 1, 6, 4, 5, 1, 4, 6, 1, 1, 1, 4, 4, 1, 3, 2, 5, 1, 1, 5, 3, 6, 6, 5, 3, 6, 5, 3, 6, 1, 6, 2, 4, 6, 5, 4, 5, 1, 1, 1, 3, 6, 2, 3, 1, 4, 5, 6, 6, 3, 3, 4, 4, 6, 3, 1, 3, 2, 3, 4]\n",
            "[6, 6, 5, 5, 3, 3, 3, 6, 2, 2, 5, 6, 2, 3, 5, 2, 2, 2, 6, 4, 4, 5, 4, 2, 4, 6, 5, 6, 6, 3, 1, 3, 5, 1, 4, 1, 3, 2, 3, 4, 6, 3, 3, 6, 5, 4, 3, 6, 6, 4, 3, 2, 6, 2, 6, 6, 4, 4, 5, 3, 2, 3, 1, 6, 6, 5, 4, 4, 3, 3, 1, 1, 5, 4, 4, 3, 3, 4, 6, 2, 3, 5, 4, 4, 4, 4, 3, 2, 4, 6, 6, 2, 6, 4, 6, 3, 3, 2, 4, 1, 5, 5, 4, 6, 5, 1, 5, 4, 1, 4, 4, 6, 3, 4, 2, 6, 3, 2, 1, 5, 6, 5, 5, 2, 5, 3, 2, 5, 5, 2, 4, 5, 5, 6, 6, 5, 1, 3, 4, 4, 6, 4, 6, 3, 4, 3, 6, 2, 2, 2, 6, 5, 6, 6, 1, 1, 3, 4, 4, 2, 6, 2, 1, 3, 2, 6, 3, 2, 5, 6, 1, 2, 3, 5, 2, 4, 4, 6, 6, 3, 2, 4, 3, 3, 2, 5, 4, 3, 5, 5, 3, 4, 4, 2, 5, 5, 4, 4, 4, 6, 3, 5, 6, 2, 3, 2, 4, 6, 1, 6, 3, 4, 6, 5, 3, 6, 4, 6, 6, 2, 4, 1, 3, 1, 3, 1, 1, 6, 4, 6, 3, 4, 2, 5, 1, 5, 4, 3, 6, 4, 5, 1, 4, 4, 3, 4, 2, 3, 6, 3, 3, 4, 4, 3, 6, 5, 5, 4, 1, 3, 2, 3, 5, 1, 4, 6, 6, 4, 6, 2, 2, 3, 4, 6, 3, 6, 3, 5, 5, 3, 2, 3, 1, 6, 5, 6, 6, 5, 2, 2, 4, 3, 5, 2, 3, 1, 2, 4, 5, 4, 4, 2, 6, 4, 3, 2, 6, 6, 3, 3, 5, 6, 2, 2, 6, 6, 5, 3, 4, 4, 4, 1, 5, 6, 2, 5, 4, 2, 5, 4, 4, 5, 4, 2, 3, 4, 5, 2, 2, 2, 6, 2, 3, 5, 6, 4, 4, 4, 3, 6, 2, 6, 2, 1, 1, 1, 4, 5, 4, 1, 3, 6, 6, 5, 6, 4, 6, 3, 6, 6, 4, 5, 4, 2, 3, 4, 3, 5, 4, 2, 4, 1, 5, 2, 5, 6, 2, 3, 3, 6, 1, 2, 5, 4, 4, 5, 3, 2, 4, 3, 6, 3, 3, 2, 3, 5, 3, 4, 6, 2, 4, 3, 4, 5, 2, 2, 2, 5, 5, 3, 4, 1, 5, 1, 6, 5, 2, 6, 3, 4, 4, 2, 6, 4, 6, 4, 4, 1, 4, 5, 3, 1, 1, 2, 1, 6, 4, 5, 4, 6, 4, 2, 6, 4, 6, 3, 4, 4, 5, 2, 2, 6, 2, 2, 5, 5, 5, 3, 6, 4, 3, 2, 6, 4, 2, 4, 5, 4, 5, 4, 5, 4, 4, 2, 4, 5, 3, 6, 2, 4, 5, 1, 6, 4, 5, 2, 3, 6, 5, 3, 1, 2, 5, 2, 1, 2, 6, 2, 6, 4, 2, 6, 4, 4, 3, 5, 2, 6, 3, 6, 4, 5, 5, 3, 1, 4, 5, 6, 5, 3, 2, 2, 6, 2, 1, 6, 3, 6, 3, 4, 3, 6, 5, 2, 2, 2, 4, 3, 3, 2, 4, 4, 5, 3, 3, 6, 2, 4, 4, 3, 1, 4, 5, 4, 2, 3, 2, 5, 4, 2, 3, 5, 3, 6, 3, 4, 4, 3, 4, 4, 6, 6, 6, 4, 4, 6, 1, 4, 4, 4, 3, 4, 5, 5, 1, 6, 6, 6, 3, 4, 5, 1, 4, 4, 4, 1, 3, 6, 5, 4, 6, 2, 3, 3, 6, 2, 3, 2, 3, 1, 4, 5, 3, 4, 1, 4, 4, 3, 3, 2, 4, 2, 4, 2, 5, 1, 3, 2, 4, 2, 4, 4, 3, 2, 5, 1, 3, 6, 1, 2, 1, 5, 6, 4, 5, 4, 6, 4, 6, 3, 3, 3, 4, 6, 3, 6, 2, 3, 5, 2, 3, 5, 5, 1, 4, 5, 6, 6, 2, 3, 5, 2, 2, 4, 1, 5, 3, 1, 1, 3, 5, 5, 3, 6, 5, 5, 5, 6, 5, 3, 4, 6, 5, 6, 4, 5, 6, 4, 4, 4, 2, 4, 3, 4, 1, 4, 2, 4, 2, 3, 6, 4, 3, 2, 1, 4, 5, 4, 4, 1, 5, 6, 5, 5, 5, 2, 3, 5, 5, 3, 5, 3, 5, 5, 1, 5, 6, 6, 6, 2, 4, 6, 3, 3, 4, 5, 4, 3, 4, 5, 4, 4, 2, 5, 2, 6, 6, 6, 6, 6, 6, 6, 1, 6, 3, 4, 1, 5, 6, 3, 6, 3, 4, 2, 5, 6, 6, 2, 1, 6, 6, 6, 6, 2, 3, 2, 5, 2, 6, 2, 2, 2, 4, 5, 2, 4, 3, 3, 4, 6, 1, 4, 4, 4, 2, 1, 5, 2, 6, 1, 3, 3, 4, 3, 4, 2, 6, 6, 6, 4, 4, 4, 6, 4, 4, 4, 4, 5, 3, 5, 2, 1, 2, 3, 2, 6, 5, 3, 3, 3, 2, 5, 3, 3, 2, 2, 4, 5, 1, 1, 1, 3, 3, 3, 3, 5, 5, 5, 2, 5, 4, 5, 2, 3, 3, 3, 3, 5, 2, 3, 4, 5, 4, 6, 6, 3, 3, 1, 4, 6, 6, 6, 4, 6, 1, 5, 5, 3, 1, 5, 6, 6, 3, 1, 3, 4, 1, 6, 5, 4, 6, 6, 5, 4, 2, 6, 4, 4, 2, 2, 3, 6, 4, 6, 6, 2, 4, 6, 5, 5, 3, 1, 5, 6, 5, 3, 2, 3, 6, 4, 4, 1, 6, 4, 5, 1, 4, 6, 2, 1, 1, 4, 4, 1, 3, 2, 5, 3, 1, 5, 3, 6, 6, 5, 3, 6, 5, 3, 6, 4, 6, 2, 4, 6, 5, 4, 5, 1, 4, 4, 3, 6, 2, 3, 4, 4, 5, 6, 6, 3, 3, 4, 4, 6, 3, 1, 3, 2, 3, 4]\n",
            "Iteration 1 - Train loss: 0.1471865028142929\n",
            "Iteration 2 - Train loss: 0.24025068432092667\n",
            "Iteration 3 - Train loss: 0.26212748388449353\n",
            "Iteration 4 - Train loss: 0.23619534447789192\n",
            "Iteration 5 - Train loss: 0.2196730077266693\n",
            "Iteration 6 - Train loss: 0.250837301214536\n",
            "Iteration 7 - Train loss: 0.238630296928542\n",
            "Iteration 8 - Train loss: 0.2555085141211748\n",
            "Iteration 9 - Train loss: 0.26104679538144004\n",
            "Iteration 10 - Train loss: 0.24591953083872795\n",
            "Iteration 11 - Train loss: 0.22420849486000158\n",
            "Iteration 12 - Train loss: 0.23629889299627393\n",
            "Iteration 13 - Train loss: 0.26187014038889456\n",
            "Iteration 14 - Train loss: 0.2742042768014861\n",
            "Iteration 15 - Train loss: 0.26134399650618434\n",
            "Iteration 16 - Train loss: 0.26239308042568155\n",
            "Iteration 17 - Train loss: 0.2587440099621959\n",
            "Iteration 18 - Train loss: 0.24832472062876654\n",
            "Iteration 19 - Train loss: 0.24556274028298886\n",
            "Iteration 20 - Train loss: 0.2482071684906259\n",
            "Iteration 21 - Train loss: 0.24426857153663323\n",
            "Iteration 22 - Train loss: 0.24318714120255952\n",
            "Iteration 23 - Train loss: 0.2405042689010177\n",
            "Iteration 24 - Train loss: 0.2487837276615513\n",
            "Iteration 25 - Train loss: 0.24588267521932722\n",
            "Iteration 26 - Train loss: 0.2523883301943827\n",
            "Iteration 27 - Train loss: 0.269658490066865\n",
            "Iteration 28 - Train loss: 0.2712770941434428\n",
            "Iteration 29 - Train loss: 0.27151149166105637\n",
            "Iteration 30 - Train loss: 0.26858134134672584\n",
            "Iteration 31 - Train loss: 0.2636465393157015\n",
            "Iteration 32 - Train loss: 0.26283280704228673\n",
            "Iteration 33 - Train loss: 0.25628267760586104\n",
            "Iteration 34 - Train loss: 0.261454041732256\n",
            "Iteration 35 - Train loss: 0.2573130776972643\n",
            "Iteration 36 - Train loss: 0.2515968620492559\n",
            "Iteration 37 - Train loss: 0.2645128182185864\n",
            "Iteration 38 - Train loss: 0.26244892475293263\n",
            "Iteration 39 - Train loss: 0.26755917207218516\n",
            "Iteration 40 - Train loss: 0.2707760503632016\n",
            "Iteration 41 - Train loss: 0.2756645437897887\n",
            "Iteration 42 - Train loss: 0.27115559827403296\n",
            "Iteration 43 - Train loss: 0.27075735948503366\n",
            "Iteration 44 - Train loss: 0.2699585316639224\n",
            "Iteration 45 - Train loss: 0.272442560404953\n",
            "Iteration 46 - Train loss: 0.2690851741936058\n",
            "Iteration 47 - Train loss: 0.2703311261126494\n",
            "Iteration 48 - Train loss: 0.26752285241188173\n",
            "Iteration 49 - Train loss: 0.266606971478964\n",
            "Iteration 50 - Train loss: 0.26636774436570704\n",
            "Iteration 51 - Train loss: 0.26617949047838063\n",
            "Iteration 52 - Train loss: 0.26556858980061054\n",
            "Iteration 53 - Train loss: 0.2650251024543255\n",
            "Iteration 54 - Train loss: 0.2632304290740716\n",
            "Iteration 55 - Train loss: 0.2604793707162819\n",
            "Iteration 56 - Train loss: 0.25953694979294334\n",
            "Iteration 57 - Train loss: 0.2602290136069713\n",
            "Iteration 58 - Train loss: 0.2610280237658399\n",
            "Iteration 59 - Train loss: 0.2596728335829231\n",
            "Iteration 60 - Train loss: 0.2584578711617117\n",
            "Iteration 61 - Train loss: 0.2594993973196652\n",
            "Iteration 62 - Train loss: 0.259576998778709\n",
            "Iteration 63 - Train loss: 0.2584008573377061\n",
            "Iteration 64 - Train loss: 0.25524323748686584\n",
            "Iteration 65 - Train loss: 0.25377317427012785\n",
            "Iteration 66 - Train loss: 0.2553879220222095\n",
            "Iteration 67 - Train loss: 0.2565699022944405\n",
            "Iteration 68 - Train loss: 0.26109838826061393\n",
            "Iteration 69 - Train loss: 0.2599323751553353\n",
            "Iteration 70 - Train loss: 0.26286138775758444\n",
            "Iteration 71 - Train loss: 0.26180247721773847\n",
            "Iteration 72 - Train loss: 0.26465189033317277\n",
            "Iteration 73 - Train loss: 0.26778615654561005\n",
            "Iteration 74 - Train loss: 0.265926386361841\n",
            "Iteration 75 - Train loss: 0.2651941530965269\n",
            "Iteration 76 - Train loss: 0.2649395304156075\n",
            "Iteration 77 - Train loss: 0.26414278275998576\n",
            "Iteration 78 - Train loss: 0.26568305999255526\n",
            "Iteration 79 - Train loss: 0.2633720023404288\n",
            "Iteration 80 - Train loss: 0.2645336384011898\n",
            "Iteration 81 - Train loss: 0.26209236516467765\n",
            "Iteration 82 - Train loss: 0.2614864574393212\n",
            "Iteration 83 - Train loss: 0.2598522066767047\n",
            "Iteration 84 - Train loss: 0.2603094111296481\n",
            "Iteration 85 - Train loss: 0.25929587554077016\n",
            "Iteration 86 - Train loss: 0.25844815270000593\n",
            "Iteration 87 - Train loss: 0.2616032755976521\n",
            "Iteration 88 - Train loss: 0.26119446416321973\n",
            "Iteration 89 - Train loss: 0.2604978483097021\n",
            "Iteration 90 - Train loss: 0.25857027488139767\n",
            "Iteration 91 - Train loss: 0.25747199831939827\n",
            "Iteration 92 - Train loss: 0.257787279789741\n",
            "Iteration 93 - Train loss: 0.25650244567202785\n",
            "Iteration 94 - Train loss: 0.2569830103807072\n",
            "Iteration 95 - Train loss: 0.2556928945264142\n",
            "Iteration 96 - Train loss: 0.2563375635121095\n",
            "Iteration 97 - Train loss: 0.2569631393470792\n",
            "Iteration 98 - Train loss: 0.2560989830918534\n",
            "Iteration 99 - Train loss: 0.25774184306330905\n",
            "Iteration 100 - Train loss: 0.25687038198579104\n",
            "Iteration 101 - Train loss: 0.2579154444554138\n",
            "Iteration 102 - Train loss: 0.25822169217281044\n",
            "Iteration 103 - Train loss: 0.2570175496743793\n",
            "Iteration 104 - Train loss: 0.2566949298220257\n",
            "Iteration 105 - Train loss: 0.2558741308438281\n",
            "Iteration 106 - Train loss: 0.25705579654665067\n",
            "Iteration 107 - Train loss: 0.25759967632818026\n",
            "Iteration 108 - Train loss: 0.2590219428185029\n",
            "Iteration 109 - Train loss: 0.2576266082917947\n",
            "Iteration 110 - Train loss: 0.2580274856945669\n",
            "Iteration 111 - Train loss: 0.2597901527182543\n",
            "Iteration 112 - Train loss: 0.26014079699442455\n",
            "Iteration 113 - Train loss: 0.26189082953666826\n",
            "Iteration 114 - Train loss: 0.2599612122213697\n",
            "Iteration 115 - Train loss: 0.258751282164746\n",
            "Iteration 116 - Train loss: 0.2581477372722443\n",
            "Iteration 117 - Train loss: 0.25734227469477516\n",
            "Iteration 118 - Train loss: 0.25640208710190227\n",
            "Iteration 119 - Train loss: 0.25506005380224406\n",
            "Iteration 120 - Train loss: 0.2564367829550368\n",
            "Iteration 121 - Train loss: 0.2568249617518534\n",
            "Iteration 122 - Train loss: 0.2554508195464789\n",
            "Iteration 123 - Train loss: 0.25586780270076986\n",
            "Iteration 124 - Train loss: 0.2557144099583609\n",
            "Iteration 125 - Train loss: 0.25444446748867633\n",
            "Iteration 126 - Train loss: 0.2529985211116986\n",
            "Iteration 127 - Train loss: 0.25198942358700893\n",
            "Iteration 128 - Train loss: 0.2528128771264164\n",
            "Iteration 129 - Train loss: 0.2534638556088646\n",
            "Iteration 130 - Train loss: 0.2540686445203252\n",
            "Iteration 131 - Train loss: 0.25265716107647496\n",
            "Iteration 132 - Train loss: 0.25349109608919895\n",
            "Iteration 133 - Train loss: 0.25479269285749334\n",
            "Iteration 134 - Train loss: 0.2606024002663291\n",
            "Iteration 135 - Train loss: 0.2595516781598605\n",
            "Iteration 136 - Train loss: 0.26138392644455\n",
            "Iteration 137 - Train loss: 0.2598017662199364\n",
            "Iteration 138 - Train loss: 0.2582465903192381\n",
            "Iteration 139 - Train loss: 0.257414192768658\n",
            "Iteration 140 - Train loss: 0.2616256328066811\n",
            "Iteration 141 - Train loss: 0.26270035728614066\n",
            "Iteration 142 - Train loss: 0.263279367451438\n",
            "Iteration 143 - Train loss: 0.26468921077952207\n",
            "Iteration 144 - Train loss: 0.2655005793947364\n",
            "Iteration 145 - Train loss: 0.26576744457064516\n",
            "Iteration 146 - Train loss: 0.2657424576408294\n",
            "Iteration 147 - Train loss: 0.2657342956109973\n",
            "Iteration 148 - Train loss: 0.26467129145434276\n",
            "Iteration 149 - Train loss: 0.264505266546328\n",
            "Iteration 150 - Train loss: 0.2644355110358447\n",
            "Iteration 151 - Train loss: 0.2642656909147685\n",
            "Iteration 152 - Train loss: 0.26282208864148215\n",
            "Iteration 153 - Train loss: 0.26353639345068264\n",
            "Iteration 154 - Train loss: 0.26248145553694624\n",
            "Iteration 155 - Train loss: 0.26294315981588534\n",
            "Iteration 156 - Train loss: 0.2633827027345363\n",
            "Iteration 157 - Train loss: 0.2635390609057893\n",
            "Iteration 158 - Train loss: 0.2651764843406604\n",
            "Iteration 159 - Train loss: 0.264397286256459\n",
            "Iteration 160 - Train loss: 0.2635371642682003\n",
            "Iteration 161 - Train loss: 0.26350274409300517\n",
            "Iteration 162 - Train loss: 0.262596899577428\n",
            "Iteration 163 - Train loss: 0.2618626795417738\n",
            "Iteration 164 - Train loss: 0.26315789129414663\n",
            "Iteration 165 - Train loss: 0.26433909142841444\n",
            "Iteration 166 - Train loss: 0.2643514044431364\n",
            "Iteration 167 - Train loss: 0.264765787136345\n",
            "Iteration 168 - Train loss: 0.264255736957282\n",
            "Iteration 169 - Train loss: 0.26486834130090103\n",
            "Iteration 170 - Train loss: 0.2655100367114167\n",
            "Iteration 171 - Train loss: 0.2641521426086581\n",
            "Iteration 172 - Train loss: 0.2642935551296876\n",
            "Iteration 173 - Train loss: 0.26442853464998\n",
            "Iteration 174 - Train loss: 0.2641155020542571\n",
            "Iteration 175 - Train loss: 0.2653906544218106\n",
            "Iteration 176 - Train loss: 0.26571738614372653\n",
            "Iteration 177 - Train loss: 0.2663271090010507\n",
            "Iteration 178 - Train loss: 0.26791548323189696\n",
            "Iteration 179 - Train loss: 0.26729057051043686\n",
            "Iteration 180 - Train loss: 0.26913147134344195\n",
            "Iteration 181 - Train loss: 0.2686426508271274\n",
            "Iteration 182 - Train loss: 0.2682428578206512\n",
            "Iteration 183 - Train loss: 0.267927316086153\n",
            "Iteration 184 - Train loss: 0.26953646674519405\n",
            "Iteration 185 - Train loss: 0.2705708547646331\n",
            "Iteration 186 - Train loss: 0.2707467587509503\n",
            "Iteration 187 - Train loss: 0.2714426054141141\n",
            "Iteration 188 - Train loss: 0.2707203078564217\n",
            "Iteration 189 - Train loss: 0.27032706161428777\n",
            "Iteration 190 - Train loss: 0.2694522276153102\n",
            "Iteration 191 - Train loss: 0.2696813208060778\n",
            "Iteration 192 - Train loss: 0.26848785950035864\n",
            "Iteration 193 - Train loss: 0.26858364916382443\n",
            "Iteration 194 - Train loss: 0.2689937537310395\n",
            "Iteration 195 - Train loss: 0.2684314296628611\n",
            "Iteration 196 - Train loss: 0.26767040369319445\n",
            "Iteration 197 - Train loss: 0.2673653213491635\n",
            "Iteration 198 - Train loss: 0.26780859108383986\n",
            "Iteration 199 - Train loss: 0.26719474386489345\n",
            "Iteration 200 - Train loss: 0.2678555631474592\n",
            "Iteration 201 - Train loss: 0.26849499283895933\n",
            "Iteration 202 - Train loss: 0.26865953456213937\n",
            "Iteration 203 - Train loss: 0.26860020473241586\n",
            "Iteration 204 - Train loss: 0.26914215349254433\n",
            "Iteration 205 - Train loss: 0.26819523614944846\n",
            "Iteration 206 - Train loss: 0.2687565752155898\n",
            "Iteration 207 - Train loss: 0.26909676091835477\n",
            "Iteration 208 - Train loss: 0.2687728052372292\n",
            "Iteration 209 - Train loss: 0.2696683472267499\n",
            "Iteration 210 - Train loss: 0.2700749380092713\n",
            "Iteration 211 - Train loss: 0.27022765949589167\n",
            "Iteration 212 - Train loss: 0.2695990441529572\n",
            "Iteration 213 - Train loss: 0.27064917000675215\n",
            "Iteration 214 - Train loss: 0.27153424213849287\n",
            "Iteration 215 - Train loss: 0.27352523809125606\n",
            "Iteration 216 - Train loss: 0.27360538122701217\n",
            "Iteration 217 - Train loss: 0.27553523004200947\n",
            "Iteration 218 - Train loss: 0.2750524841441235\n",
            "Iteration 219 - Train loss: 0.2758321530593891\n",
            "Iteration 220 - Train loss: 0.27633100313092157\n",
            "Iteration 221 - Train loss: 0.27593376179229956\n",
            "Iteration 222 - Train loss: 0.27652697142859634\n",
            "Iteration 223 - Train loss: 0.2762295551485311\n",
            "Iteration 224 - Train loss: 0.2759313685008757\n",
            "Iteration 225 - Train loss: 0.27601041931452025\n",
            "Iteration 226 - Train loss: 0.27490658883574065\n",
            "Iteration 227 - Train loss: 0.2767534153269284\n",
            "Iteration 228 - Train loss: 0.2767086138544408\n",
            "Iteration 229 - Train loss: 0.2767235644214571\n",
            "Iteration 230 - Train loss: 0.2768068297745903\n",
            "Iteration 231 - Train loss: 0.27663607731082546\n",
            "Iteration 232 - Train loss: 0.2764495552441202\n",
            "Iteration 233 - Train loss: 0.27621383904459457\n",
            "Iteration 234 - Train loss: 0.2757640296024167\n",
            "Iteration 235 - Train loss: 0.2771161004286656\n",
            "Iteration 236 - Train loss: 0.27676613992119553\n",
            "Iteration 237 - Train loss: 0.2776903873211252\n",
            "Iteration 238 - Train loss: 0.27709749500862\n",
            "Iteration 239 - Train loss: 0.27774010377519115\n",
            "Iteration 240 - Train loss: 0.2778185948438477\n",
            "Iteration 241 - Train loss: 0.27749158878118974\n",
            "Iteration 242 - Train loss: 0.2774059860870889\n",
            "Iteration 243 - Train loss: 0.27678920000737894\n",
            "Iteration 244 - Train loss: 0.27636532830723304\n",
            "Iteration 245 - Train loss: 0.2769360020775728\n",
            "Iteration 246 - Train loss: 0.2761300589332766\n",
            "Iteration 247 - Train loss: 0.27652755636261844\n",
            "Iteration 248 - Train loss: 0.27578838470189143\n",
            "Iteration 249 - Train loss: 0.2752620517986875\n",
            "Iteration 250 - Train loss: 0.2744960933048278\n",
            "Iteration 251 - Train loss: 0.2736649340207863\n",
            "Iteration 252 - Train loss: 0.273266025569423\n",
            "Iteration 253 - Train loss: 0.27401174820054014\n",
            "Iteration 254 - Train loss: 0.27523801772209894\n",
            "Iteration 255 - Train loss: 0.2747239939448442\n",
            "Iteration 256 - Train loss: 0.2763256427442684\n",
            "Iteration 257 - Train loss: 0.27676148953438096\n",
            "Iteration 258 - Train loss: 0.2760386139807804\n",
            "Iteration 259 - Train loss: 0.27529856033486577\n",
            "Iteration 260 - Train loss: 0.2747836936569701\n",
            "Iteration 261 - Train loss: 0.27525185733869445\n",
            "Iteration 262 - Train loss: 0.27543041172778615\n",
            "Iteration 263 - Train loss: 0.2751046602734623\n",
            "Iteration 264 - Train loss: 0.27485214337657177\n",
            "Iteration 265 - Train loss: 0.27446117963606737\n",
            "Iteration 266 - Train loss: 0.27507761165786787\n",
            "Iteration 267 - Train loss: 0.27611736679965915\n",
            "Iteration 268 - Train loss: 0.27570325297602355\n",
            "Iteration 269 - Train loss: 0.2754572830474537\n",
            "Iteration 270 - Train loss: 0.27535008677902323\n",
            "Iteration 271 - Train loss: 0.27514020455451305\n",
            "Iteration 272 - Train loss: 0.2754415549793253\n",
            "Iteration 273 - Train loss: 0.27645346533418413\n",
            "Iteration 274 - Train loss: 0.27605200477495084\n",
            "Iteration 275 - Train loss: 0.2757519956627353\n",
            "Iteration 276 - Train loss: 0.2761911978233147\n",
            "Iteration 277 - Train loss: 0.27563241188875015\n",
            "Iteration 278 - Train loss: 0.2750204344893129\n",
            "Iteration 279 - Train loss: 0.27589817222468155\n",
            "Iteration 280 - Train loss: 0.27552955608449076\n",
            "Iteration 281 - Train loss: 0.2759960740290953\n",
            "Iteration 282 - Train loss: 0.27685981471230214\n",
            "Iteration 283 - Train loss: 0.2764914856167253\n",
            "Iteration 284 - Train loss: 0.2760288471923354\n",
            "Iteration 285 - Train loss: 0.27533794921381693\n",
            "Iteration 286 - Train loss: 0.27766250696691025\n",
            "Iteration 287 - Train loss: 0.2781919452543484\n",
            "Iteration 288 - Train loss: 0.2787653924654781\n",
            "Iteration 289 - Train loss: 0.2787999564146939\n",
            "Iteration 290 - Train loss: 0.2788204075720418\n",
            "Iteration 291 - Train loss: 0.2789024094910172\n",
            "Iteration 292 - Train loss: 0.27955168993563123\n",
            "Iteration 293 - Train loss: 0.27917282713208436\n",
            "Iteration 294 - Train loss: 0.2785762243090058\n",
            "Iteration 295 - Train loss: 0.27846629642486825\n",
            "Iteration 296 - Train loss: 0.27870985515875035\n",
            "Iteration 297 - Train loss: 0.27794643655977513\n",
            "Iteration 298 - Train loss: 0.2775946997196707\n",
            "Iteration 299 - Train loss: 0.2773893591792114\n",
            "Iteration 300 - Train loss: 0.2778359193246191\n",
            "Iteration 301 - Train loss: 0.2788294936573609\n",
            "Iteration 302 - Train loss: 0.2791503744449664\n",
            "Iteration 303 - Train loss: 0.27899047600953736\n",
            "Iteration 304 - Train loss: 0.2788990120002142\n",
            "Iteration 305 - Train loss: 0.27891508864971704\n",
            "Iteration 306 - Train loss: 0.2793587450732422\n",
            "Iteration 307 - Train loss: 0.2791583195513764\n",
            "Iteration 308 - Train loss: 0.27925309027369777\n",
            "Iteration 309 - Train loss: 0.27904984308105313\n",
            "Iteration 310 - Train loss: 0.278763381429317\n",
            "Iteration 311 - Train loss: 0.27976066709145714\n",
            "Iteration 312 - Train loss: 0.28011182802042756\n",
            "Iteration 313 - Train loss: 0.2798701294923957\n",
            "Iteration 314 - Train loss: 0.27912436888529474\n",
            "Iteration 315 - Train loss: 0.27891572706903967\n",
            "Iteration 316 - Train loss: 0.2790041593495827\n",
            "Iteration 317 - Train loss: 0.2790508175559624\n",
            "Iteration 318 - Train loss: 0.2786548250843917\n",
            "Iteration 319 - Train loss: 0.27852786225076986\n",
            "Iteration 320 - Train loss: 0.2788418092080974\n",
            "Iteration 321 - Train loss: 0.27953742414486604\n",
            "Iteration 322 - Train loss: 0.2788560129138091\n",
            "Iteration 323 - Train loss: 0.27885749323824105\n",
            "Iteration 324 - Train loss: 0.2783145058431582\n",
            "Iteration 325 - Train loss: 0.27840988068769756\n",
            "Iteration 326 - Train loss: 0.2781829366051918\n",
            "Iteration 327 - Train loss: 0.27802140114327684\n",
            "Iteration 328 - Train loss: 0.2781426260693612\n",
            "Iteration 329 - Train loss: 0.27843455903622744\n",
            "Iteration 330 - Train loss: 0.2792283842184891\n",
            "Iteration 331 - Train loss: 0.2785888446728471\n",
            "Iteration 332 - Train loss: 0.2790745855042576\n",
            "Iteration 333 - Train loss: 0.2785482462453614\n",
            "Iteration 334 - Train loss: 0.27931894677758085\n",
            "Iteration 335 - Train loss: 0.27978892774640846\n",
            "Iteration 336 - Train loss: 0.27981005080315907\n",
            "Iteration 337 - Train loss: 0.28021473783684636\n",
            "Iteration 338 - Train loss: 0.28067290292606417\n",
            "Iteration 339 - Train loss: 0.2802351799678156\n",
            "Iteration 340 - Train loss: 0.28029172713183526\n",
            "Iteration 341 - Train loss: 0.2800177233389243\n",
            "Iteration 342 - Train loss: 0.2798192767625163\n",
            "Iteration 343 - Train loss: 0.27958760216272827\n",
            "Iteration 344 - Train loss: 0.2790402338940239\n",
            "Iteration 345 - Train loss: 0.279424230032263\n",
            "Iteration 346 - Train loss: 0.27960866779180493\n",
            "Iteration 347 - Train loss: 0.2794051195591452\n",
            "Iteration 348 - Train loss: 0.27916530077747104\n",
            "Iteration 349 - Train loss: 0.27950048552463125\n",
            "Iteration 350 - Train loss: 0.2792717951443046\n",
            "Iteration 351 - Train loss: 0.2791328853768352\n",
            "Iteration 352 - Train loss: 0.2793447095808171\n",
            "Iteration 353 - Train loss: 0.2793044338897187\n",
            "Iteration 354 - Train loss: 0.28013134751032176\n",
            "Iteration 355 - Train loss: 0.28070334514183265\n",
            "Iteration 356 - Train loss: 0.28069577825627173\n",
            "Iteration 357 - Train loss: 0.2811974068224451\n",
            "Iteration 358 - Train loss: 0.2813673471347767\n",
            "Iteration 359 - Train loss: 0.28080175961380754\n",
            "Iteration 360 - Train loss: 0.2808246497388205\n",
            "Iteration 361 - Train loss: 0.2810843943468607\n",
            "Iteration 362 - Train loss: 0.28143765624144335\n",
            "Iteration 363 - Train loss: 0.28131230658987544\n",
            "Iteration 364 - Train loss: 0.28202102426567455\n",
            "Iteration 365 - Train loss: 0.2826325496901082\n",
            "Iteration 366 - Train loss: 0.28213216805235164\n",
            "Iteration 367 - Train loss: 0.28233751119812317\n",
            "Iteration 368 - Train loss: 0.28317418946687173\n",
            "Iteration 369 - Train loss: 0.28366409578464097\n",
            "Iteration 370 - Train loss: 0.2832768141933893\n",
            "Iteration 371 - Train loss: 0.2835759523455204\n",
            "Iteration 372 - Train loss: 0.2833723778478421\n",
            "Iteration 373 - Train loss: 0.2830049042520051\n",
            "Iteration 374 - Train loss: 0.2829391375535173\n",
            "Iteration 375 - Train loss: 0.283008775404344\n",
            "Iteration 376 - Train loss: 0.2826345106896429\n",
            "Iteration 377 - Train loss: 0.28279341371058037\n",
            "Iteration 378 - Train loss: 0.2830659815433026\n",
            "Iteration 379 - Train loss: 0.28285453919847164\n",
            "Iteration 380 - Train loss: 0.28252183019649235\n",
            "Iteration 381 - Train loss: 0.2824998742905678\n",
            "Iteration 382 - Train loss: 0.2822683753253141\n",
            "Iteration 383 - Train loss: 0.28181618453166385\n",
            "Iteration 384 - Train loss: 0.2814457830306007\n",
            "Iteration 385 - Train loss: 0.2819471437544501\n",
            "Iteration 386 - Train loss: 0.28219630489236835\n",
            "Iteration 387 - Train loss: 0.28188050098510253\n",
            "Iteration 388 - Train loss: 0.28228419394227533\n",
            "Iteration 389 - Train loss: 0.28199001910299026\n",
            "Iteration 390 - Train loss: 0.28192265967074304\n",
            "Iteration 391 - Train loss: 0.28280633185987775\n",
            "Iteration 392 - Train loss: 0.2826416758281103\n",
            "Iteration 393 - Train loss: 0.2822124828660325\n",
            "Iteration 394 - Train loss: 0.2818629098642466\n",
            "Iteration 395 - Train loss: 0.28183937200191844\n",
            "Iteration 396 - Train loss: 0.2816837308751984\n",
            "Iteration 397 - Train loss: 0.28205985332996364\n",
            "Iteration 398 - Train loss: 0.28289902696137936\n",
            "Iteration 399 - Train loss: 0.28329948015758455\n",
            "Iteration 400 - Train loss: 0.2837332652427722\n",
            "Iteration 401 - Train loss: 0.28389779728328685\n",
            "Iteration 402 - Train loss: 0.28400610356860384\n",
            "Iteration 403 - Train loss: 0.2845250575840196\n",
            "Iteration 404 - Train loss: 0.28476502686522276\n",
            "Iteration 405 - Train loss: 0.28478773772647537\n",
            "Iteration 406 - Train loss: 0.2846848003742895\n",
            "Iteration 407 - Train loss: 0.28485581131551313\n",
            "Iteration 408 - Train loss: 0.2844508522676359\n",
            "Iteration 409 - Train loss: 0.2844510114965801\n",
            "Iteration 410 - Train loss: 0.28514806178580154\n",
            "Iteration 411 - Train loss: 0.28526147657216794\n",
            "Iteration 412 - Train loss: 0.2849043505096772\n",
            "Iteration 413 - Train loss: 0.28454797952289335\n",
            "Iteration 414 - Train loss: 0.28433502056177\n",
            "Iteration 415 - Train loss: 0.28415125185758416\n",
            "Iteration 416 - Train loss: 0.2836282241903693\n",
            "Iteration 417 - Train loss: 0.28355093104809487\n",
            "Iteration 418 - Train loss: 0.28341189559000995\n",
            "Iteration 419 - Train loss: 0.28304938788065653\n",
            "Iteration 420 - Train loss: 0.282893655033383\n",
            "Iteration 421 - Train loss: 0.28248817773730517\n",
            "Iteration 422 - Train loss: 0.282007733608212\n",
            "Iteration 423 - Train loss: 0.28149727082989634\n",
            "Iteration 424 - Train loss: 0.28099134683121385\n",
            "Iteration 425 - Train loss: 0.28126792780726273\n",
            "Iteration 426 - Train loss: 0.28129875617202355\n",
            "Iteration 427 - Train loss: 0.2821666980718439\n",
            "Iteration 428 - Train loss: 0.282362351900132\n",
            "Iteration 429 - Train loss: 0.28246671981818644\n",
            "Iteration 430 - Train loss: 0.2820672710891813\n",
            "Iteration 431 - Train loss: 0.2819050262388266\n",
            "Iteration 432 - Train loss: 0.28139579539398524\n",
            "Iteration 433 - Train loss: 0.28103749583180626\n",
            "Iteration 434 - Train loss: 0.28075197347748837\n",
            "Iteration 435 - Train loss: 0.2811749698968882\n",
            "Iteration 436 - Train loss: 0.2811694983403634\n",
            "Iteration 437 - Train loss: 0.281529193425453\n",
            "Iteration 438 - Train loss: 0.2822573873050698\n",
            "Iteration 439 - Train loss: 0.28225554727921626\n",
            "Iteration 440 - Train loss: 0.28185411214257\n",
            "Iteration 441 - Train loss: 0.28182285243662203\n",
            "Iteration 442 - Train loss: 0.2823299424758611\n",
            "Iteration 443 - Train loss: 0.28193122759129235\n",
            "Iteration 444 - Train loss: 0.2813251129181653\n",
            "Iteration 445 - Train loss: 0.2812688315088411\n",
            "Iteration 446 - Train loss: 0.2813419186386944\n",
            "Iteration 447 - Train loss: 0.28107995647307843\n",
            "Iteration 448 - Train loss: 0.2807677608621556\n",
            "Iteration 449 - Train loss: 0.28051970898171674\n",
            "Iteration 450 - Train loss: 0.28110252839720085\n",
            "Iteration 451 - Train loss: 0.2811893935319441\n",
            "Iteration 452 - Train loss: 0.2811475023204889\n",
            "Iteration 453 - Train loss: 0.2807098806107923\n",
            "Iteration 454 - Train loss: 0.28065095041747234\n",
            "Iteration 455 - Train loss: 0.280248940391159\n",
            "Iteration 456 - Train loss: 0.28005927746294457\n",
            "Iteration 457 - Train loss: 0.28013359147880057\n",
            "Iteration 458 - Train loss: 0.2800888193102236\n",
            "Iteration 459 - Train loss: 0.2804366750531061\n",
            "Iteration 460 - Train loss: 0.28033993808532376\n",
            "Iteration 461 - Train loss: 0.2800624116905712\n",
            "Iteration 462 - Train loss: 0.27961265228716936\n",
            "Iteration 463 - Train loss: 0.2791412413818259\n",
            "Iteration 464 - Train loss: 0.27894515454154944\n",
            "Iteration 465 - Train loss: 0.27884824224597504\n",
            "Iteration 466 - Train loss: 0.27870816420743744\n",
            "Iteration 467 - Train loss: 0.2789841467859642\n",
            "Iteration 468 - Train loss: 0.27857142825248754\n",
            "Iteration 469 - Train loss: 0.2792675464477605\n",
            "Iteration 470 - Train loss: 0.2790516391862184\n",
            "Iteration 471 - Train loss: 0.2785987844502834\n",
            "Iteration 472 - Train loss: 0.2789155175994245\n",
            "Iteration 473 - Train loss: 0.2787296253760556\n",
            "Iteration 474 - Train loss: 0.2787860074554047\n",
            "Iteration 475 - Train loss: 0.27891578689903807\n",
            "Iteration 476 - Train loss: 0.27936212364642904\n",
            "Iteration 477 - Train loss: 0.2789073759051473\n",
            "Iteration 478 - Train loss: 0.2783416137420748\n",
            "Iteration 479 - Train loss: 0.2781940086099216\n",
            "Iteration 480 - Train loss: 0.27845488938910423\n",
            "Iteration 481 - Train loss: 0.27822610209337795\n",
            "Iteration 482 - Train loss: 0.2784363418456198\n",
            "Iteration 483 - Train loss: 0.2790213720901921\n",
            "Iteration 484 - Train loss: 0.27969305808482736\n",
            "Iteration 485 - Train loss: 0.280292731585922\n",
            "Iteration 486 - Train loss: 0.2803203642009968\n",
            "Iteration 487 - Train loss: 0.2808875767734032\n",
            "Iteration 488 - Train loss: 0.2814214777394167\n",
            "Iteration 489 - Train loss: 0.2811388451394631\n",
            "Iteration 490 - Train loss: 0.2813780952482579\n",
            "Iteration 491 - Train loss: 0.28134995699023674\n",
            "Iteration 492 - Train loss: 0.2810922696347901\n",
            "Iteration 493 - Train loss: 0.2809830818176043\n",
            "Iteration 494 - Train loss: 0.28084592759582827\n",
            "Iteration 495 - Train loss: 0.28044719483910335\n",
            "Iteration 496 - Train loss: 0.28090712132266604\n",
            "Iteration 497 - Train loss: 0.2809513844334159\n",
            "Iteration 498 - Train loss: 0.2810088108680828\n",
            "Iteration 499 - Train loss: 0.28079096732646197\n",
            "Iteration 500 - Train loss: 0.2807938585439697\n",
            "Iteration 501 - Train loss: 0.28117362255260286\n",
            "Iteration 502 - Train loss: 0.2813232857669032\n",
            "Iteration 503 - Train loss: 0.28174013925982905\n",
            "Iteration 504 - Train loss: 0.28140225128496127\n",
            "Iteration 505 - Train loss: 0.2812411261971413\n",
            "Iteration 506 - Train loss: 0.28162443040126545\n",
            "Iteration 507 - Train loss: 0.2820304406768251\n",
            "Iteration 508 - Train loss: 0.2821986329435613\n",
            "Iteration 509 - Train loss: 0.2820435963829061\n",
            "Iteration 510 - Train loss: 0.28182637320520976\n",
            "Iteration 511 - Train loss: 0.281749220132106\n",
            "Iteration 512 - Train loss: 0.28183628953502193\n",
            "Iteration 513 - Train loss: 0.2818149539994232\n",
            "Iteration 514 - Train loss: 0.28144346329869147\n",
            "Iteration 515 - Train loss: 0.2812709434961593\n",
            "Iteration 516 - Train loss: 0.2821686671434923\n",
            "Iteration 517 - Train loss: 0.28206993877343706\n",
            "Iteration 518 - Train loss: 0.2823188404299132\n",
            "Iteration 519 - Train loss: 0.28211372282610836\n",
            "Iteration 520 - Train loss: 0.28254472219403115\n",
            "Iteration 521 - Train loss: 0.28289243711995216\n",
            "Iteration 522 - Train loss: 0.2827015484181871\n",
            "Iteration 523 - Train loss: 0.2825070063861347\n",
            "Iteration 524 - Train loss: 0.2823746113243463\n",
            "Iteration 525 - Train loss: 0.2821316841537399\n",
            "Iteration 526 - Train loss: 0.28216691863432725\n",
            "Iteration 527 - Train loss: 0.2819977073711069\n",
            "Iteration 528 - Train loss: 0.28157661534990464\n",
            "Iteration 529 - Train loss: 0.2815190041147257\n",
            "Iteration 530 - Train loss: 0.2815240868007026\n",
            "Iteration 531 - Train loss: 0.28152666299949397\n",
            "Iteration 532 - Train loss: 0.28115942572940256\n",
            "Iteration 533 - Train loss: 0.28136134609024394\n",
            "Iteration 534 - Train loss: 0.28116970475892145\n",
            "Iteration 535 - Train loss: 0.2814358225779904\n",
            "Iteration 536 - Train loss: 0.28131588142618896\n",
            "Iteration 537 - Train loss: 0.28182529620704844\n",
            "Iteration 538 - Train loss: 0.28242765555459465\n",
            "Iteration 539 - Train loss: 0.2824548103048333\n",
            "Iteration 540 - Train loss: 0.2824167619157514\n",
            "Iteration 541 - Train loss: 0.282224165452145\n",
            "Iteration 542 - Train loss: 0.2822575328598397\n",
            "Iteration 543 - Train loss: 0.28186233311593123\n",
            "Iteration 544 - Train loss: 0.2824603908649057\n",
            "Iteration 545 - Train loss: 0.28261153982405407\n",
            "Iteration 546 - Train loss: 0.2829627868465247\n",
            "Iteration 547 - Train loss: 0.2828300935539742\n",
            "Iteration 548 - Train loss: 0.28378585810402585\n",
            "Iteration 549 - Train loss: 0.28350211889016524\n",
            "Iteration 550 - Train loss: 0.28370189737274565\n",
            "Iteration 551 - Train loss: 0.2838372647662574\n",
            "Iteration 552 - Train loss: 0.2834925101403538\n",
            "Iteration 553 - Train loss: 0.2837250701027163\n",
            "Iteration 554 - Train loss: 0.2835656763583973\n",
            "Iteration 555 - Train loss: 0.28348294177117783\n",
            "Iteration 556 - Train loss: 0.2831780835592503\n",
            "Iteration 557 - Train loss: 0.2833260284541125\n",
            "Iteration 558 - Train loss: 0.28389707652394625\n",
            "Iteration 559 - Train loss: 0.28387183789413734\n",
            "Iteration 560 - Train loss: 0.2838028257587991\n",
            "Iteration 561 - Train loss: 0.28396214400094383\n",
            "Iteration 562 - Train loss: 0.2841496899917894\n",
            "Iteration 563 - Train loss: 0.28501940531934056\n",
            "Iteration 564 - Train loss: 0.28493943552945655\n",
            "Iteration 565 - Train loss: 0.284733482497225\n",
            "Iteration 566 - Train loss: 0.28461697514748113\n",
            "Iteration 567 - Train loss: 0.28450365773298675\n",
            "Iteration 568 - Train loss: 0.2841461064354648\n",
            "Iteration 569 - Train loss: 0.2839380124418526\n",
            "Iteration 570 - Train loss: 0.28408791866039107\n",
            "Iteration 571 - Train loss: 0.28389416085784497\n",
            "Iteration 572 - Train loss: 0.2835780537167851\n",
            "Iteration 573 - Train loss: 0.28374156941063244\n",
            "Iteration 574 - Train loss: 0.28390754434890164\n",
            "Iteration 575 - Train loss: 0.2835606691508513\n",
            "Iteration 576 - Train loss: 0.2838545531016684\n",
            "Iteration 577 - Train loss: 0.28357697004594384\n",
            "Iteration 578 - Train loss: 0.28401133507106674\n",
            "Iteration 579 - Train loss: 0.28378824229611144\n",
            "Iteration 580 - Train loss: 0.2836950674275707\n",
            "Iteration 581 - Train loss: 0.28345172080370035\n",
            "Iteration 582 - Train loss: 0.2830208370668487\n",
            "Iteration 583 - Train loss: 0.2832558783510856\n",
            "Iteration 584 - Train loss: 0.2831860423695666\n",
            "Iteration 585 - Train loss: 0.2829713953046017\n",
            "Iteration 586 - Train loss: 0.28289026006143986\n",
            "Iteration 587 - Train loss: 0.28318850251289124\n",
            "Iteration 588 - Train loss: 0.2831029882789178\n",
            "Iteration 589 - Train loss: 0.28316051460540076\n",
            "Iteration 590 - Train loss: 0.2831161674751379\n",
            "Iteration 591 - Train loss: 0.28275011070844813\n",
            "Iteration 592 - Train loss: 0.283373334892629\n",
            "Iteration 593 - Train loss: 0.28310521824668594\n",
            "Iteration 594 - Train loss: 0.28306626220647657\n",
            "Iteration 595 - Train loss: 0.2832941486625423\n",
            "Iteration 596 - Train loss: 0.2832090275419951\n",
            "Iteration 597 - Train loss: 0.28298437239140445\n",
            "Iteration 598 - Train loss: 0.2827614902722708\n",
            "Iteration 599 - Train loss: 0.28274338995378606\n",
            "Iteration 600 - Train loss: 0.28292025621437156\n",
            "Iteration 601 - Train loss: 0.2826766261711605\n",
            "Iteration 602 - Train loss: 0.2828537275298403\n",
            "Iteration 603 - Train loss: 0.28280252500654973\n",
            "Iteration 604 - Train loss: 0.28316697572737265\n",
            "Iteration 605 - Train loss: 0.282965441683138\n",
            "Iteration 606 - Train loss: 0.2826759743853088\n",
            "Iteration 607 - Train loss: 0.2827849454350794\n",
            "Iteration 608 - Train loss: 0.28323545340696765\n",
            "Iteration 609 - Train loss: 0.2830813076838007\n",
            "Iteration 610 - Train loss: 0.2827422600002868\n",
            "Iteration 611 - Train loss: 0.28257210354614887\n",
            "Iteration 612 - Train loss: 0.28276462769638017\n",
            "Iteration 613 - Train loss: 0.28338708461841133\n",
            "Iteration 614 - Train loss: 0.28322350255104467\n",
            "Iteration 615 - Train loss: 0.2835238085476666\n",
            "Iteration 616 - Train loss: 0.2834983585567165\n",
            "Iteration 617 - Train loss: 0.28361190992635427\n",
            "Iteration 618 - Train loss: 0.2841884018770422\n",
            "Iteration 619 - Train loss: 0.2839096169113836\n",
            "Iteration 620 - Train loss: 0.28416353702740443\n",
            "Iteration 621 - Train loss: 0.28465721369368563\n",
            "Iteration 622 - Train loss: 0.2845862001200114\n",
            "Iteration 623 - Train loss: 0.28465575174193825\n",
            "Iteration 624 - Train loss: 0.28483699933843665\n",
            "Iteration 625 - Train loss: 0.28566125996783376\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  67%|██████▋   | 2/3 [29:43<14:51, 891.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6, 6, 5, 5, 3, 3, 3, 6, 2, 2, 5, 6, 2, 3, 5, 2, 2, 2, 6, 1, 1, 5, 1, 2, 4, 6, 5, 6, 6, 3, 1, 3, 5, 4, 4, 1, 3, 2, 3, 4, 6, 3, 3, 6, 5, 1, 3, 6, 6, 4, 3, 2, 6, 2, 6, 6, 1, 4, 5, 3, 2, 3, 4, 6, 6, 5, 4, 4, 3, 3, 1, 2, 5, 1, 4, 3, 1, 4, 6, 2, 3, 5, 1, 4, 1, 4, 3, 2, 2, 6, 6, 2, 6, 4, 6, 3, 3, 2, 4, 1, 5, 5, 4, 6, 5, 1, 5, 1, 1, 4, 4, 6, 3, 4, 2, 6, 3, 2, 1, 5, 6, 5, 5, 2, 5, 3, 2, 5, 5, 2, 2, 5, 5, 6, 6, 5, 1, 3, 1, 4, 6, 4, 6, 3, 4, 3, 6, 2, 2, 2, 6, 5, 6, 6, 1, 1, 3, 1, 4, 2, 6, 2, 1, 3, 2, 6, 3, 2, 5, 6, 1, 2, 3, 5, 2, 4, 4, 6, 6, 3, 2, 1, 3, 3, 2, 5, 4, 3, 5, 5, 3, 1, 4, 2, 5, 5, 4, 4, 4, 6, 3, 5, 6, 2, 3, 2, 5, 6, 1, 6, 3, 4, 6, 5, 3, 6, 4, 6, 6, 2, 4, 1, 3, 1, 3, 1, 1, 6, 4, 6, 3, 2, 2, 5, 5, 5, 4, 3, 6, 1, 5, 1, 1, 4, 3, 4, 2, 3, 6, 3, 3, 4, 4, 3, 6, 5, 5, 4, 1, 3, 2, 3, 5, 1, 2, 6, 6, 4, 6, 2, 2, 3, 4, 6, 3, 6, 3, 5, 5, 3, 2, 3, 1, 6, 5, 6, 6, 5, 2, 2, 4, 3, 5, 2, 3, 1, 2, 6, 5, 4, 4, 2, 6, 4, 3, 2, 6, 6, 3, 3, 5, 6, 2, 2, 6, 6, 5, 3, 4, 1, 1, 1, 5, 6, 2, 5, 1, 2, 5, 4, 4, 5, 4, 2, 3, 4, 5, 2, 2, 2, 6, 2, 3, 5, 6, 4, 4, 4, 1, 6, 2, 6, 2, 1, 1, 1, 4, 5, 4, 1, 3, 6, 6, 5, 6, 4, 6, 3, 6, 6, 1, 5, 4, 2, 3, 4, 3, 5, 1, 2, 1, 1, 5, 2, 5, 6, 2, 3, 3, 6, 4, 2, 5, 4, 4, 5, 3, 2, 4, 3, 6, 3, 3, 2, 3, 5, 3, 4, 6, 2, 4, 3, 4, 5, 2, 2, 2, 5, 4, 3, 4, 1, 5, 1, 6, 5, 2, 6, 1, 2, 4, 2, 6, 1, 6, 1, 4, 1, 4, 5, 3, 1, 1, 2, 1, 6, 4, 5, 4, 6, 4, 2, 6, 2, 6, 3, 4, 1, 5, 2, 2, 6, 2, 2, 5, 5, 5, 3, 6, 4, 3, 2, 6, 1, 2, 4, 5, 4, 5, 1, 5, 1, 4, 2, 4, 5, 3, 6, 2, 4, 5, 1, 6, 4, 5, 2, 3, 6, 5, 3, 2, 2, 5, 4, 1, 2, 6, 2, 6, 4, 2, 6, 1, 1, 3, 5, 2, 4, 3, 6, 2, 5, 5, 3, 1, 1, 5, 6, 5, 3, 2, 2, 6, 2, 4, 6, 3, 6, 3, 4, 3, 6, 4, 2, 2, 2, 2, 3, 3, 2, 1, 4, 5, 3, 3, 6, 2, 4, 4, 3, 1, 4, 5, 4, 2, 1, 2, 5, 4, 2, 3, 5, 3, 6, 3, 1, 4, 3, 4, 6, 6, 6, 6, 4, 4, 6, 1, 1, 1, 1, 3, 4, 5, 5, 1, 6, 6, 6, 3, 1, 5, 1, 1, 4, 1, 1, 3, 6, 5, 1, 6, 2, 3, 3, 6, 2, 3, 2, 3, 1, 4, 5, 3, 1, 1, 4, 4, 3, 3, 2, 4, 2, 4, 2, 5, 1, 3, 2, 4, 2, 1, 4, 3, 2, 5, 1, 3, 6, 1, 2, 1, 5, 6, 4, 5, 4, 6, 1, 6, 3, 3, 3, 4, 6, 3, 6, 2, 3, 5, 2, 3, 5, 5, 1, 4, 5, 6, 6, 2, 3, 5, 2, 2, 4, 1, 5, 1, 4, 1, 3, 5, 5, 3, 6, 5, 5, 5, 6, 5, 3, 4, 6, 5, 6, 4, 5, 6, 4, 4, 1, 2, 1, 3, 4, 1, 4, 2, 4, 2, 3, 6, 4, 3, 2, 1, 1, 5, 4, 4, 4, 5, 6, 5, 5, 5, 2, 3, 5, 5, 3, 5, 3, 5, 5, 1, 5, 6, 6, 6, 2, 4, 6, 1, 3, 4, 5, 1, 3, 1, 5, 4, 1, 2, 5, 2, 6, 6, 6, 6, 6, 6, 6, 1, 6, 3, 4, 1, 5, 6, 3, 6, 3, 1, 2, 5, 6, 6, 2, 1, 6, 6, 6, 6, 2, 3, 2, 5, 2, 6, 2, 2, 2, 4, 5, 2, 1, 3, 3, 4, 6, 1, 1, 4, 1, 2, 1, 5, 2, 6, 1, 3, 3, 1, 3, 4, 2, 6, 1, 6, 4, 4, 1, 6, 4, 4, 4, 4, 5, 3, 1, 2, 1, 2, 3, 2, 6, 5, 3, 3, 3, 2, 5, 3, 3, 2, 2, 4, 5, 1, 1, 1, 3, 1, 3, 3, 5, 5, 5, 2, 5, 1, 5, 2, 3, 3, 3, 3, 5, 2, 3, 4, 5, 4, 6, 6, 3, 3, 1, 4, 6, 6, 6, 1, 6, 1, 5, 5, 3, 2, 5, 6, 6, 3, 1, 3, 4, 1, 6, 5, 2, 6, 6, 5, 1, 2, 6, 1, 4, 2, 2, 3, 6, 4, 6, 6, 2, 4, 6, 5, 5, 3, 4, 5, 6, 5, 3, 2, 3, 6, 4, 4, 1, 6, 4, 5, 1, 4, 6, 1, 1, 1, 4, 4, 1, 3, 2, 5, 1, 1, 5, 3, 6, 6, 5, 3, 6, 5, 3, 6, 1, 6, 2, 4, 6, 5, 4, 5, 1, 1, 1, 3, 6, 2, 3, 1, 4, 5, 6, 6, 3, 3, 4, 4, 6, 3, 1, 3, 2, 3, 4]\n",
            "[6, 6, 5, 5, 3, 3, 3, 6, 2, 2, 5, 6, 2, 3, 5, 2, 2, 2, 6, 4, 4, 5, 1, 2, 4, 6, 5, 6, 6, 3, 1, 3, 5, 1, 4, 1, 3, 2, 3, 4, 6, 3, 3, 6, 5, 1, 3, 6, 6, 4, 3, 2, 6, 2, 6, 6, 1, 4, 5, 1, 2, 3, 1, 6, 6, 5, 4, 4, 3, 3, 1, 1, 5, 4, 4, 3, 1, 4, 6, 2, 3, 5, 1, 4, 1, 4, 3, 2, 2, 6, 6, 2, 6, 4, 6, 3, 3, 2, 4, 1, 5, 5, 4, 6, 5, 4, 5, 1, 1, 4, 4, 6, 3, 4, 2, 6, 3, 2, 1, 5, 6, 5, 5, 2, 5, 3, 2, 5, 5, 2, 4, 5, 5, 6, 6, 5, 1, 3, 4, 4, 6, 4, 6, 3, 4, 3, 6, 2, 2, 2, 6, 5, 6, 6, 1, 2, 3, 1, 4, 2, 6, 2, 1, 3, 2, 6, 3, 2, 5, 6, 1, 2, 3, 5, 2, 4, 4, 6, 6, 3, 2, 4, 3, 3, 2, 5, 4, 3, 5, 5, 3, 1, 4, 2, 5, 5, 4, 4, 4, 6, 3, 5, 6, 2, 3, 2, 4, 6, 4, 6, 3, 4, 6, 5, 3, 6, 4, 6, 6, 2, 4, 1, 3, 1, 3, 1, 1, 6, 4, 6, 3, 2, 2, 5, 1, 5, 4, 3, 6, 4, 5, 1, 4, 4, 3, 4, 2, 3, 6, 3, 3, 4, 4, 3, 6, 5, 5, 4, 1, 3, 2, 3, 5, 1, 2, 6, 6, 4, 6, 2, 2, 3, 4, 6, 3, 6, 3, 5, 5, 3, 2, 3, 1, 6, 5, 6, 6, 5, 2, 2, 4, 3, 5, 2, 3, 1, 2, 4, 5, 4, 4, 2, 6, 4, 3, 2, 6, 6, 3, 3, 5, 6, 2, 2, 6, 6, 5, 3, 4, 4, 4, 1, 5, 6, 2, 5, 4, 2, 5, 4, 4, 5, 4, 2, 3, 4, 5, 2, 2, 2, 6, 2, 3, 5, 6, 4, 4, 4, 1, 6, 2, 6, 2, 1, 1, 1, 4, 5, 4, 1, 3, 6, 6, 5, 6, 4, 6, 3, 6, 6, 4, 5, 4, 2, 3, 4, 3, 5, 4, 2, 4, 1, 5, 2, 5, 6, 2, 3, 3, 6, 1, 2, 5, 4, 4, 5, 3, 2, 4, 3, 6, 3, 3, 2, 3, 5, 3, 4, 6, 2, 4, 3, 4, 5, 2, 2, 2, 5, 5, 3, 4, 1, 5, 1, 6, 5, 2, 6, 1, 2, 4, 2, 6, 1, 6, 4, 4, 1, 4, 5, 3, 2, 1, 2, 1, 6, 4, 5, 4, 6, 4, 2, 6, 2, 6, 3, 4, 4, 5, 2, 2, 6, 2, 2, 5, 5, 5, 3, 6, 4, 3, 2, 6, 4, 2, 4, 5, 4, 5, 4, 5, 4, 4, 2, 4, 5, 3, 6, 2, 4, 5, 1, 6, 4, 5, 2, 3, 6, 5, 3, 2, 2, 5, 1, 1, 2, 6, 2, 6, 4, 2, 6, 1, 1, 3, 5, 2, 4, 3, 6, 2, 5, 5, 3, 1, 4, 5, 6, 5, 3, 2, 2, 6, 2, 1, 6, 3, 6, 3, 4, 3, 6, 5, 2, 2, 2, 2, 3, 3, 2, 4, 4, 5, 3, 3, 6, 2, 4, 4, 3, 1, 4, 5, 4, 2, 1, 2, 5, 4, 2, 3, 5, 3, 6, 3, 4, 4, 3, 4, 6, 6, 6, 6, 4, 4, 6, 1, 4, 4, 4, 3, 4, 5, 5, 1, 6, 6, 6, 3, 1, 5, 1, 1, 4, 1, 1, 3, 6, 5, 4, 6, 2, 3, 3, 6, 2, 3, 2, 3, 1, 4, 5, 3, 1, 1, 4, 4, 3, 3, 2, 4, 2, 4, 2, 5, 1, 3, 2, 4, 2, 4, 4, 3, 2, 5, 1, 3, 6, 1, 2, 1, 5, 6, 4, 5, 4, 6, 4, 6, 3, 3, 3, 4, 6, 3, 6, 2, 3, 5, 2, 3, 5, 5, 1, 4, 5, 6, 6, 2, 3, 5, 2, 2, 4, 1, 5, 1, 1, 1, 3, 5, 5, 3, 6, 5, 5, 5, 6, 5, 3, 4, 6, 5, 6, 4, 5, 6, 4, 4, 4, 2, 4, 3, 4, 1, 4, 2, 4, 2, 3, 6, 4, 3, 2, 1, 4, 5, 4, 4, 1, 5, 6, 5, 5, 5, 2, 3, 5, 5, 3, 5, 3, 5, 5, 1, 5, 6, 6, 6, 2, 4, 6, 1, 3, 4, 5, 4, 3, 4, 5, 4, 4, 2, 5, 2, 6, 6, 6, 6, 6, 6, 6, 1, 6, 3, 4, 1, 5, 6, 3, 6, 3, 4, 2, 5, 6, 6, 2, 1, 6, 6, 6, 6, 2, 3, 2, 5, 2, 6, 2, 2, 2, 4, 5, 2, 1, 3, 3, 4, 6, 1, 4, 4, 1, 2, 1, 5, 2, 6, 1, 3, 3, 4, 3, 4, 2, 6, 1, 6, 4, 4, 4, 6, 4, 4, 4, 4, 5, 3, 4, 2, 1, 2, 3, 2, 6, 5, 3, 3, 3, 2, 5, 3, 3, 2, 2, 4, 5, 1, 1, 1, 3, 1, 3, 3, 5, 5, 5, 2, 5, 4, 5, 2, 3, 3, 3, 3, 5, 2, 3, 4, 5, 4, 6, 4, 3, 3, 1, 4, 6, 6, 6, 4, 6, 1, 5, 5, 3, 2, 5, 6, 6, 3, 1, 3, 4, 1, 6, 5, 4, 6, 6, 5, 4, 2, 6, 1, 4, 2, 2, 3, 6, 2, 6, 6, 2, 4, 6, 5, 5, 3, 1, 5, 6, 5, 3, 2, 3, 6, 4, 4, 1, 6, 4, 5, 1, 4, 6, 1, 1, 4, 4, 4, 1, 3, 2, 5, 1, 1, 5, 3, 6, 6, 5, 3, 6, 5, 3, 6, 1, 6, 2, 4, 6, 5, 4, 5, 1, 4, 1, 3, 6, 2, 3, 1, 4, 5, 6, 6, 3, 3, 4, 4, 6, 3, 1, 3, 2, 3, 4]\n",
            "Iteration 1 - Train loss: 0.22080442309379578\n",
            "Iteration 2 - Train loss: 0.16633402556180954\n",
            "Iteration 3 - Train loss: 0.19066589574019113\n",
            "Iteration 4 - Train loss: 0.18048151582479477\n",
            "Iteration 5 - Train loss: 0.15844241976737977\n",
            "Iteration 6 - Train loss: 0.1545352339744568\n",
            "Iteration 7 - Train loss: 0.14886259926216944\n",
            "Iteration 8 - Train loss: 0.16835738252848387\n",
            "Iteration 9 - Train loss: 0.18983396804995006\n",
            "Iteration 10 - Train loss: 0.2040087454020977\n",
            "Iteration 11 - Train loss: 0.21149482984434476\n",
            "Iteration 12 - Train loss: 0.21052000982065996\n",
            "Iteration 13 - Train loss: 0.21456381048147494\n",
            "Iteration 14 - Train loss: 0.2214531541935035\n",
            "Iteration 15 - Train loss: 0.22152119229237238\n",
            "Iteration 16 - Train loss: 0.22761629289016128\n",
            "Iteration 17 - Train loss: 0.23308510070337968\n",
            "Iteration 18 - Train loss: 0.23801924081312287\n",
            "Iteration 19 - Train loss: 0.2430470770126895\n",
            "Iteration 20 - Train loss: 0.2487927194684744\n",
            "Iteration 21 - Train loss: 0.23920234789450964\n",
            "Iteration 22 - Train loss: 0.2414563193239949\n",
            "Iteration 23 - Train loss: 0.24040170953325604\n",
            "Iteration 24 - Train loss: 0.2341490937396884\n",
            "Iteration 25 - Train loss: 0.22940711736679076\n",
            "Iteration 26 - Train loss: 0.2338347973731848\n",
            "Iteration 27 - Train loss: 0.23608925386711402\n",
            "Iteration 28 - Train loss: 0.23724864849022456\n",
            "Iteration 29 - Train loss: 0.23407047884217624\n",
            "Iteration 30 - Train loss: 0.23437156627575556\n",
            "Iteration 31 - Train loss: 0.23083073621796024\n",
            "Iteration 32 - Train loss: 0.22526027355343103\n",
            "Iteration 33 - Train loss: 0.2282808224360148\n",
            "Iteration 34 - Train loss: 0.22670746945282994\n",
            "Iteration 35 - Train loss: 0.23493631950446536\n",
            "Iteration 36 - Train loss: 0.23347816285159853\n",
            "Iteration 37 - Train loss: 0.231164448970073\n",
            "Iteration 38 - Train loss: 0.2312706724593514\n",
            "Iteration 39 - Train loss: 0.22854096767229912\n",
            "Iteration 40 - Train loss: 0.22553804721683263\n",
            "Iteration 41 - Train loss: 0.2257810066022524\n",
            "Iteration 42 - Train loss: 0.22778967147072157\n",
            "Iteration 43 - Train loss: 0.2274600574096968\n",
            "Iteration 44 - Train loss: 0.2238958848809654\n",
            "Iteration 45 - Train loss: 0.22003428902890948\n",
            "Iteration 46 - Train loss: 0.221438003299029\n",
            "Iteration 47 - Train loss: 0.22007554484174607\n",
            "Iteration 48 - Train loss: 0.2219408064459761\n",
            "Iteration 49 - Train loss: 0.22523500390198767\n",
            "Iteration 50 - Train loss: 0.22303732424974443\n",
            "Iteration 51 - Train loss: 0.2205284750636886\n",
            "Iteration 52 - Train loss: 0.2210869097079222\n",
            "Iteration 53 - Train loss: 0.2181481711988179\n",
            "Iteration 54 - Train loss: 0.21522285892731613\n",
            "Iteration 55 - Train loss: 0.2132864360782233\n",
            "Iteration 56 - Train loss: 0.21239465974005206\n",
            "Iteration 57 - Train loss: 0.21093707093805597\n",
            "Iteration 58 - Train loss: 0.2122917603701353\n",
            "Iteration 59 - Train loss: 0.21633011778279887\n",
            "Iteration 60 - Train loss: 0.21841433215886355\n",
            "Iteration 61 - Train loss: 0.21635650165501188\n",
            "Iteration 62 - Train loss: 0.2179339027573024\n",
            "Iteration 63 - Train loss: 0.21767828595780192\n",
            "Iteration 64 - Train loss: 0.21810102317249402\n",
            "Iteration 65 - Train loss: 0.216990593706186\n",
            "Iteration 66 - Train loss: 0.2165964696782105\n",
            "Iteration 67 - Train loss: 0.2166931472941121\n",
            "Iteration 68 - Train loss: 0.2139450238798471\n",
            "Iteration 69 - Train loss: 0.21216650614919869\n",
            "Iteration 70 - Train loss: 0.21003495023718902\n",
            "Iteration 71 - Train loss: 0.208163452452757\n",
            "Iteration 72 - Train loss: 0.20883870543912053\n",
            "Iteration 73 - Train loss: 0.20976968298423782\n",
            "Iteration 74 - Train loss: 0.20792919734643922\n",
            "Iteration 75 - Train loss: 0.20549073157211145\n",
            "Iteration 76 - Train loss: 0.2047371566050539\n",
            "Iteration 77 - Train loss: 0.2037103266830181\n",
            "Iteration 78 - Train loss: 0.20174034797132778\n",
            "Iteration 79 - Train loss: 0.20234583598809153\n",
            "Iteration 80 - Train loss: 0.20090381612535566\n",
            "Iteration 81 - Train loss: 0.19978561627184166\n",
            "Iteration 82 - Train loss: 0.19766623195169902\n",
            "Iteration 83 - Train loss: 0.19763764016420007\n",
            "Iteration 84 - Train loss: 0.1964387617384394\n",
            "Iteration 85 - Train loss: 0.19737906285068568\n",
            "Iteration 86 - Train loss: 0.198218903959144\n",
            "Iteration 87 - Train loss: 0.19967981252348285\n",
            "Iteration 88 - Train loss: 0.1991608964533291\n",
            "Iteration 89 - Train loss: 0.20352193030915902\n",
            "Iteration 90 - Train loss: 0.20405703969299793\n",
            "Iteration 91 - Train loss: 0.20467044646432112\n",
            "Iteration 92 - Train loss: 0.20503171229654032\n",
            "Iteration 93 - Train loss: 0.20639503214468238\n",
            "Iteration 94 - Train loss: 0.20466179063821094\n",
            "Iteration 95 - Train loss: 0.2049720955130301\n",
            "Iteration 96 - Train loss: 0.2033605333029603\n",
            "Iteration 97 - Train loss: 0.20518602718858375\n",
            "Iteration 98 - Train loss: 0.20443107199151905\n",
            "Iteration 99 - Train loss: 0.2027526106873546\n",
            "Iteration 100 - Train loss: 0.2018043801560998\n",
            "Iteration 101 - Train loss: 0.199999482951837\n",
            "Iteration 102 - Train loss: 0.19891495319704214\n",
            "Iteration 103 - Train loss: 0.19725939879211987\n",
            "Iteration 104 - Train loss: 0.19824060584561756\n",
            "Iteration 105 - Train loss: 0.20134176883314336\n",
            "Iteration 106 - Train loss: 0.2016673913436397\n",
            "Iteration 107 - Train loss: 0.20286395687992884\n",
            "Iteration 108 - Train loss: 0.2050200150417233\n",
            "Iteration 109 - Train loss: 0.20409141660351818\n",
            "Iteration 110 - Train loss: 0.20334351071241227\n",
            "Iteration 111 - Train loss: 0.2033665624052823\n",
            "Iteration 112 - Train loss: 0.20309154351707548\n",
            "Iteration 113 - Train loss: 0.20336366288232066\n",
            "Iteration 114 - Train loss: 0.20239965815358518\n",
            "Iteration 115 - Train loss: 0.20405047213577707\n",
            "Iteration 116 - Train loss: 0.20500649346424074\n",
            "Iteration 117 - Train loss: 0.20459033626840156\n",
            "Iteration 118 - Train loss: 0.20352416152481811\n",
            "Iteration 119 - Train loss: 0.2037822801557158\n",
            "Iteration 120 - Train loss: 0.20326680089347066\n",
            "Iteration 121 - Train loss: 0.20309888770080303\n",
            "Iteration 122 - Train loss: 0.2036000401789292\n",
            "Iteration 123 - Train loss: 0.20612827011543075\n",
            "Iteration 124 - Train loss: 0.20795239215236036\n",
            "Iteration 125 - Train loss: 0.20799388583004474\n",
            "Iteration 126 - Train loss: 0.20884722928028732\n",
            "Iteration 127 - Train loss: 0.2096289512004674\n",
            "Iteration 128 - Train loss: 0.20870086953800637\n",
            "Iteration 129 - Train loss: 0.2081161764731934\n",
            "Iteration 130 - Train loss: 0.2073783291790348\n",
            "Iteration 131 - Train loss: 0.2079347235129307\n",
            "Iteration 132 - Train loss: 0.20806572796551115\n",
            "Iteration 133 - Train loss: 0.20914484802773572\n",
            "Iteration 134 - Train loss: 0.20855426917603212\n",
            "Iteration 135 - Train loss: 0.20812585095840472\n",
            "Iteration 136 - Train loss: 0.20804205269771903\n",
            "Iteration 137 - Train loss: 0.20981779929767125\n",
            "Iteration 138 - Train loss: 0.21259570938359568\n",
            "Iteration 139 - Train loss: 0.21141960575051016\n",
            "Iteration 140 - Train loss: 0.21046614456655724\n",
            "Iteration 141 - Train loss: 0.2101172473123099\n",
            "Iteration 142 - Train loss: 0.20991051072438419\n",
            "Iteration 143 - Train loss: 0.20870726994544894\n",
            "Iteration 144 - Train loss: 0.20796281700798622\n",
            "Iteration 145 - Train loss: 0.20826561390582857\n",
            "Iteration 146 - Train loss: 0.2094670746552005\n",
            "Iteration 147 - Train loss: 0.2096514661422595\n",
            "Iteration 148 - Train loss: 0.2092412309595258\n",
            "Iteration 149 - Train loss: 0.20899619093327315\n",
            "Iteration 150 - Train loss: 0.2089182704811295\n",
            "Iteration 151 - Train loss: 0.20775980910668704\n",
            "Iteration 152 - Train loss: 0.20799884051819773\n",
            "Iteration 153 - Train loss: 0.20778663932449287\n",
            "Iteration 154 - Train loss: 0.2077048894912972\n",
            "Iteration 155 - Train loss: 0.20750582053536368\n",
            "Iteration 156 - Train loss: 0.2069708985181\n",
            "Iteration 157 - Train loss: 0.20665700908062185\n",
            "Iteration 158 - Train loss: 0.20677477285218768\n",
            "Iteration 159 - Train loss: 0.2076870144327294\n",
            "Iteration 160 - Train loss: 0.20684878941392526\n",
            "Iteration 161 - Train loss: 0.2062888102756339\n",
            "Iteration 162 - Train loss: 0.2059028648920817\n",
            "Iteration 163 - Train loss: 0.20588053871471823\n",
            "Iteration 164 - Train loss: 0.2051605300455377\n",
            "Iteration 165 - Train loss: 0.20406244732439519\n",
            "Iteration 166 - Train loss: 0.20634124249623842\n",
            "Iteration 167 - Train loss: 0.20704683939184615\n",
            "Iteration 168 - Train loss: 0.20686004932836763\n",
            "Iteration 169 - Train loss: 0.20686939985838515\n",
            "Iteration 170 - Train loss: 0.20700238812714816\n",
            "Iteration 171 - Train loss: 0.20668639461591579\n",
            "Iteration 172 - Train loss: 0.20604018175021507\n",
            "Iteration 173 - Train loss: 0.20663214006563488\n",
            "Iteration 174 - Train loss: 0.20638951934317404\n",
            "Iteration 175 - Train loss: 0.2067470155549901\n",
            "Iteration 176 - Train loss: 0.20638233280359683\n",
            "Iteration 177 - Train loss: 0.2063244133451059\n",
            "Iteration 178 - Train loss: 0.206020360257937\n",
            "Iteration 179 - Train loss: 0.2098177415136685\n",
            "Iteration 180 - Train loss: 0.20981050573496354\n",
            "Iteration 181 - Train loss: 0.20928989557641975\n",
            "Iteration 182 - Train loss: 0.20966714670738348\n",
            "Iteration 183 - Train loss: 0.2089650720622533\n",
            "Iteration 184 - Train loss: 0.20864741440417003\n",
            "Iteration 185 - Train loss: 0.20895559627663446\n",
            "Iteration 186 - Train loss: 0.20885761499765418\n",
            "Iteration 187 - Train loss: 0.20826214082778138\n",
            "Iteration 188 - Train loss: 0.2081637881161209\n",
            "Iteration 189 - Train loss: 0.20780890912960762\n",
            "Iteration 190 - Train loss: 0.2084323054963821\n",
            "Iteration 191 - Train loss: 0.20793957864933926\n",
            "Iteration 192 - Train loss: 0.20723531400047554\n",
            "Iteration 193 - Train loss: 0.20708701019344244\n",
            "Iteration 194 - Train loss: 0.2069099403092081\n",
            "Iteration 195 - Train loss: 0.2069262741945493\n",
            "Iteration 196 - Train loss: 0.2065082071253992\n",
            "Iteration 197 - Train loss: 0.20735520885567071\n",
            "Iteration 198 - Train loss: 0.20839369346886272\n",
            "Iteration 199 - Train loss: 0.20840327364004138\n",
            "Iteration 200 - Train loss: 0.20839316924102605\n",
            "Iteration 201 - Train loss: 0.2076680359722518\n",
            "Iteration 202 - Train loss: 0.20762631404997395\n",
            "Iteration 203 - Train loss: 0.20747933801942564\n",
            "Iteration 204 - Train loss: 0.2088965295594843\n",
            "Iteration 205 - Train loss: 0.20901284084269187\n",
            "Iteration 206 - Train loss: 0.21003427679279765\n",
            "Iteration 207 - Train loss: 0.21044554627064058\n",
            "Iteration 208 - Train loss: 0.21084058826538518\n",
            "Iteration 209 - Train loss: 0.21074829950180066\n",
            "Iteration 210 - Train loss: 0.210187139796714\n",
            "Iteration 211 - Train loss: 0.2102825757186701\n",
            "Iteration 212 - Train loss: 0.20985321827970868\n",
            "Iteration 213 - Train loss: 0.20952780496661372\n",
            "Iteration 214 - Train loss: 0.21023771613314052\n",
            "Iteration 215 - Train loss: 0.20969977480207766\n",
            "Iteration 216 - Train loss: 0.20956459914817027\n",
            "Iteration 217 - Train loss: 0.2091801616058509\n",
            "Iteration 218 - Train loss: 0.2089290512691534\n",
            "Iteration 219 - Train loss: 0.21076724876449804\n",
            "Iteration 220 - Train loss: 0.21064268558032134\n",
            "Iteration 221 - Train loss: 0.21070097182030323\n",
            "Iteration 222 - Train loss: 0.21101543942449597\n",
            "Iteration 223 - Train loss: 0.21076337302379278\n",
            "Iteration 224 - Train loss: 0.21156523108116485\n",
            "Iteration 225 - Train loss: 0.21204688056475587\n",
            "Iteration 226 - Train loss: 0.21179715475578487\n",
            "Iteration 227 - Train loss: 0.21213163301709728\n",
            "Iteration 228 - Train loss: 0.21210379301357948\n",
            "Iteration 229 - Train loss: 0.21253648039978404\n",
            "Iteration 230 - Train loss: 0.21280476532714523\n",
            "Iteration 231 - Train loss: 0.21233516677536748\n",
            "Iteration 232 - Train loss: 0.21216227214558628\n",
            "Iteration 233 - Train loss: 0.21204223484600307\n",
            "Iteration 234 - Train loss: 0.21160786166691628\n",
            "Iteration 235 - Train loss: 0.21175685570119543\n",
            "Iteration 236 - Train loss: 0.21113919578019089\n",
            "Iteration 237 - Train loss: 0.21036023352656685\n",
            "Iteration 238 - Train loss: 0.21139871145935119\n",
            "Iteration 239 - Train loss: 0.2113316564598592\n",
            "Iteration 240 - Train loss: 0.21215472660648327\n",
            "Iteration 241 - Train loss: 0.21167430307917082\n",
            "Iteration 242 - Train loss: 0.21157362036530145\n",
            "Iteration 243 - Train loss: 0.2108357891348409\n",
            "Iteration 244 - Train loss: 0.2123800043902192\n",
            "Iteration 245 - Train loss: 0.21198009974798376\n",
            "Iteration 246 - Train loss: 0.21177660587539032\n",
            "Iteration 247 - Train loss: 0.21188515833454577\n",
            "Iteration 248 - Train loss: 0.2116355374906092\n",
            "Iteration 249 - Train loss: 0.21212689469317836\n",
            "Iteration 250 - Train loss: 0.21191502474248408\n",
            "Iteration 251 - Train loss: 0.21207179228563708\n",
            "Iteration 252 - Train loss: 0.21333495845338182\n",
            "Iteration 253 - Train loss: 0.21270387185362016\n",
            "Iteration 254 - Train loss: 0.212671864323142\n",
            "Iteration 255 - Train loss: 0.21359411441520149\n",
            "Iteration 256 - Train loss: 0.21362546195450705\n",
            "Iteration 257 - Train loss: 0.2132028963286357\n",
            "Iteration 258 - Train loss: 0.2124896337704141\n",
            "Iteration 259 - Train loss: 0.21206747950504184\n",
            "Iteration 260 - Train loss: 0.21158506420369333\n",
            "Iteration 261 - Train loss: 0.21087095414741513\n",
            "Iteration 262 - Train loss: 0.21111989213964183\n",
            "Iteration 263 - Train loss: 0.21150117556603462\n",
            "Iteration 264 - Train loss: 0.21089213569366344\n",
            "Iteration 265 - Train loss: 0.21027849632192333\n",
            "Iteration 266 - Train loss: 0.21037634132303915\n",
            "Iteration 267 - Train loss: 0.2103968754982792\n",
            "Iteration 268 - Train loss: 0.21160233588611235\n",
            "Iteration 269 - Train loss: 0.21142379873774975\n",
            "Iteration 270 - Train loss: 0.21170892136654368\n",
            "Iteration 271 - Train loss: 0.21146824665202646\n",
            "Iteration 272 - Train loss: 0.2122663077145048\n",
            "Iteration 273 - Train loss: 0.21272954329056837\n",
            "Iteration 274 - Train loss: 0.2120958581689174\n",
            "Iteration 275 - Train loss: 0.21161114815283905\n",
            "Iteration 276 - Train loss: 0.21139590046031104\n",
            "Iteration 277 - Train loss: 0.21169599119425897\n",
            "Iteration 278 - Train loss: 0.21127604902889446\n",
            "Iteration 279 - Train loss: 0.21065510478952237\n",
            "Iteration 280 - Train loss: 0.21076321150176228\n",
            "Iteration 281 - Train loss: 0.2116857904516611\n",
            "Iteration 282 - Train loss: 0.21134649640517245\n",
            "Iteration 283 - Train loss: 0.21134377879752075\n",
            "Iteration 284 - Train loss: 0.2116567328323046\n",
            "Iteration 285 - Train loss: 0.21167587455558148\n",
            "Iteration 286 - Train loss: 0.21137407459188384\n",
            "Iteration 287 - Train loss: 0.21136553077358194\n",
            "Iteration 288 - Train loss: 0.21168979263165966\n",
            "Iteration 289 - Train loss: 0.2125525610218514\n",
            "Iteration 290 - Train loss: 0.21377584559778715\n",
            "Iteration 291 - Train loss: 0.21362674528816106\n",
            "Iteration 292 - Train loss: 0.21370177910573881\n",
            "Iteration 293 - Train loss: 0.2143200331847517\n",
            "Iteration 294 - Train loss: 0.21398042434459033\n",
            "Iteration 295 - Train loss: 0.21350594077701285\n",
            "Iteration 296 - Train loss: 0.21322516820745896\n",
            "Iteration 297 - Train loss: 0.2126682472888769\n",
            "Iteration 298 - Train loss: 0.21251132138598486\n",
            "Iteration 299 - Train loss: 0.21239268709360357\n",
            "Iteration 300 - Train loss: 0.21263503660137453\n",
            "Iteration 301 - Train loss: 0.21221700723459355\n",
            "Iteration 302 - Train loss: 0.21185417445561547\n",
            "Iteration 303 - Train loss: 0.21140297017253862\n",
            "Iteration 304 - Train loss: 0.21234688191601125\n",
            "Iteration 305 - Train loss: 0.21178199931612757\n",
            "Iteration 306 - Train loss: 0.21138297619748544\n",
            "Iteration 307 - Train loss: 0.2109541860181559\n",
            "Iteration 308 - Train loss: 0.21135390696853593\n",
            "Iteration 309 - Train loss: 0.21192596396461466\n",
            "Iteration 310 - Train loss: 0.21256886299459204\n",
            "Iteration 311 - Train loss: 0.21233799686457758\n",
            "Iteration 312 - Train loss: 0.2125303757388909\n",
            "Iteration 313 - Train loss: 0.21297151028229216\n",
            "Iteration 314 - Train loss: 0.2137618608428699\n",
            "Iteration 315 - Train loss: 0.21395630142873243\n",
            "Iteration 316 - Train loss: 0.21361874559705582\n",
            "Iteration 317 - Train loss: 0.2132624495798011\n",
            "Iteration 318 - Train loss: 0.21422474854186457\n",
            "Iteration 319 - Train loss: 0.21520202501230293\n",
            "Iteration 320 - Train loss: 0.21509257568395695\n",
            "Iteration 321 - Train loss: 0.21543428148041446\n",
            "Iteration 322 - Train loss: 0.21491650808371188\n",
            "Iteration 323 - Train loss: 0.2149630590477503\n",
            "Iteration 324 - Train loss: 0.21565455825493476\n",
            "Iteration 325 - Train loss: 0.21570804588496684\n",
            "Iteration 326 - Train loss: 0.21556334377500724\n",
            "Iteration 327 - Train loss: 0.2174311539710753\n",
            "Iteration 328 - Train loss: 0.21695818057533625\n",
            "Iteration 329 - Train loss: 0.21645281295713864\n",
            "Iteration 330 - Train loss: 0.2160139552515113\n",
            "Iteration 331 - Train loss: 0.2157196966351609\n",
            "Iteration 332 - Train loss: 0.21562754301668471\n",
            "Iteration 333 - Train loss: 0.21581877586250012\n",
            "Iteration 334 - Train loss: 0.21591068792552826\n",
            "Iteration 335 - Train loss: 0.21619687807092916\n",
            "Iteration 336 - Train loss: 0.2158447538801868\n",
            "Iteration 337 - Train loss: 0.2155842696778056\n",
            "Iteration 338 - Train loss: 0.21499708520810631\n",
            "Iteration 339 - Train loss: 0.21480714119860908\n",
            "Iteration 340 - Train loss: 0.21472459997543517\n",
            "Iteration 341 - Train loss: 0.2148319259176681\n",
            "Iteration 342 - Train loss: 0.21429813226238328\n",
            "Iteration 343 - Train loss: 0.21441687547952024\n",
            "Iteration 344 - Train loss: 0.21396129890236742\n",
            "Iteration 345 - Train loss: 0.21428165427152662\n",
            "Iteration 346 - Train loss: 0.2139934343366609\n",
            "Iteration 347 - Train loss: 0.2139418350387719\n",
            "Iteration 348 - Train loss: 0.2137450128151425\n",
            "Iteration 349 - Train loss: 0.2133224160973527\n",
            "Iteration 350 - Train loss: 0.2139705487872873\n",
            "Iteration 351 - Train loss: 0.21449076978654263\n",
            "Iteration 352 - Train loss: 0.21399807881309904\n",
            "Iteration 353 - Train loss: 0.21388516351369555\n",
            "Iteration 354 - Train loss: 0.2140070892617864\n",
            "Iteration 355 - Train loss: 0.21391205269266184\n",
            "Iteration 356 - Train loss: 0.21496230817996384\n",
            "Iteration 357 - Train loss: 0.21500087617074742\n",
            "Iteration 358 - Train loss: 0.2156813599002761\n",
            "Iteration 359 - Train loss: 0.2166703290054393\n",
            "Iteration 360 - Train loss: 0.21630937591609029\n",
            "Iteration 361 - Train loss: 0.21648064645786363\n",
            "Iteration 362 - Train loss: 0.21598877743380504\n",
            "Iteration 363 - Train loss: 0.21597966796028384\n",
            "Iteration 364 - Train loss: 0.21562173121332467\n",
            "Iteration 365 - Train loss: 0.21573385924917376\n",
            "Iteration 366 - Train loss: 0.21649967087178282\n",
            "Iteration 367 - Train loss: 0.21600189326934982\n",
            "Iteration 368 - Train loss: 0.21593243232928216\n",
            "Iteration 369 - Train loss: 0.21675008399094023\n",
            "Iteration 370 - Train loss: 0.21661622597760447\n",
            "Iteration 371 - Train loss: 0.2162763233073156\n",
            "Iteration 372 - Train loss: 0.216473143758072\n",
            "Iteration 373 - Train loss: 0.21638158615928232\n",
            "Iteration 374 - Train loss: 0.21654659482765007\n",
            "Iteration 375 - Train loss: 0.21638029926021893\n",
            "Iteration 376 - Train loss: 0.21626525654635848\n",
            "Iteration 377 - Train loss: 0.21601012423239274\n",
            "Iteration 378 - Train loss: 0.2165022866416072\n",
            "Iteration 379 - Train loss: 0.21639553415232723\n",
            "Iteration 380 - Train loss: 0.21659363769975148\n",
            "Iteration 381 - Train loss: 0.21664303196031903\n",
            "Iteration 382 - Train loss: 0.2163833655046387\n",
            "Iteration 383 - Train loss: 0.21587696486680377\n",
            "Iteration 384 - Train loss: 0.21682646864792332\n",
            "Iteration 385 - Train loss: 0.21707805520915366\n",
            "Iteration 386 - Train loss: 0.2173853996233928\n",
            "Iteration 387 - Train loss: 0.2178760838023452\n",
            "Iteration 388 - Train loss: 0.21771217043482766\n",
            "Iteration 389 - Train loss: 0.21808346330276798\n",
            "Iteration 390 - Train loss: 0.21826976619851896\n",
            "Iteration 391 - Train loss: 0.21828570883825918\n",
            "Iteration 392 - Train loss: 0.21782427444597896\n",
            "Iteration 393 - Train loss: 0.2181167520260386\n",
            "Iteration 394 - Train loss: 0.2178101953743073\n",
            "Iteration 395 - Train loss: 0.21758176940905896\n",
            "Iteration 396 - Train loss: 0.21754685364136792\n",
            "Iteration 397 - Train loss: 0.21802504856130037\n",
            "Iteration 398 - Train loss: 0.2176630976399285\n",
            "Iteration 399 - Train loss: 0.21766575698491028\n",
            "Iteration 400 - Train loss: 0.21724836968816816\n",
            "Iteration 401 - Train loss: 0.21759273688729266\n",
            "Iteration 402 - Train loss: 0.21735771321946412\n",
            "Iteration 403 - Train loss: 0.21721229821480534\n",
            "Iteration 404 - Train loss: 0.21683995509612383\n",
            "Iteration 405 - Train loss: 0.2177679875290688\n",
            "Iteration 406 - Train loss: 0.2173231771224972\n",
            "Iteration 407 - Train loss: 0.2179070905272\n",
            "Iteration 408 - Train loss: 0.21803063272005496\n",
            "Iteration 409 - Train loss: 0.2182324583434971\n",
            "Iteration 410 - Train loss: 0.21815629431568995\n",
            "Iteration 411 - Train loss: 0.2187446675324527\n",
            "Iteration 412 - Train loss: 0.2182495051393629\n",
            "Iteration 413 - Train loss: 0.21797463058586486\n",
            "Iteration 414 - Train loss: 0.21782740949459625\n",
            "Iteration 415 - Train loss: 0.2175683461825352\n",
            "Iteration 416 - Train loss: 0.2180381999776448\n",
            "Iteration 417 - Train loss: 0.2184020153691264\n",
            "Iteration 418 - Train loss: 0.21868292096797287\n",
            "Iteration 419 - Train loss: 0.2183668168955138\n",
            "Iteration 420 - Train loss: 0.21813942907777215\n",
            "Iteration 421 - Train loss: 0.21822232394446034\n",
            "Iteration 422 - Train loss: 0.21804430594191096\n",
            "Iteration 423 - Train loss: 0.21904019085606713\n",
            "Iteration 424 - Train loss: 0.21873194160915138\n",
            "Iteration 425 - Train loss: 0.21845872451934745\n",
            "Iteration 426 - Train loss: 0.21841784210508347\n",
            "Iteration 427 - Train loss: 0.21817613714778103\n",
            "Iteration 428 - Train loss: 0.21785253267865803\n",
            "Iteration 429 - Train loss: 0.21757489761298562\n",
            "Iteration 430 - Train loss: 0.21796590698623033\n",
            "Iteration 431 - Train loss: 0.21804988728901004\n",
            "Iteration 432 - Train loss: 0.21787924080423113\n",
            "Iteration 433 - Train loss: 0.21765847516829023\n",
            "Iteration 434 - Train loss: 0.21737117070301268\n",
            "Iteration 435 - Train loss: 0.21747154183043488\n",
            "Iteration 436 - Train loss: 0.21753827828290198\n",
            "Iteration 437 - Train loss: 0.2174223082830588\n",
            "Iteration 438 - Train loss: 0.2176126445533036\n",
            "Iteration 439 - Train loss: 0.21731641665228338\n",
            "Iteration 440 - Train loss: 0.21741615893446248\n",
            "Iteration 441 - Train loss: 0.21730448640756259\n",
            "Iteration 442 - Train loss: 0.21688715219961235\n",
            "Iteration 443 - Train loss: 0.21719892443899846\n",
            "Iteration 444 - Train loss: 0.21721467291642618\n",
            "Iteration 445 - Train loss: 0.2172376269349054\n",
            "Iteration 446 - Train loss: 0.2168209932456336\n",
            "Iteration 447 - Train loss: 0.21694980339840955\n",
            "Iteration 448 - Train loss: 0.217104780302699\n",
            "Iteration 449 - Train loss: 0.21685263379902178\n",
            "Iteration 450 - Train loss: 0.21722483090021544\n",
            "Iteration 451 - Train loss: 0.2175831746974931\n",
            "Iteration 452 - Train loss: 0.21745867326451812\n",
            "Iteration 453 - Train loss: 0.21739639820239073\n",
            "Iteration 454 - Train loss: 0.21706378536856843\n",
            "Iteration 455 - Train loss: 0.21790552491137943\n",
            "Iteration 456 - Train loss: 0.21761002888077902\n",
            "Iteration 457 - Train loss: 0.21752357463142477\n",
            "Iteration 458 - Train loss: 0.2185553711163913\n",
            "Iteration 459 - Train loss: 0.21855724760187153\n",
            "Iteration 460 - Train loss: 0.21835756884766339\n",
            "Iteration 461 - Train loss: 0.21803563492021114\n",
            "Iteration 462 - Train loss: 0.21765299105726474\n",
            "Iteration 463 - Train loss: 0.2176713957622261\n",
            "Iteration 464 - Train loss: 0.21748781868794548\n",
            "Iteration 465 - Train loss: 0.21715838851266972\n",
            "Iteration 466 - Train loss: 0.21705761813866414\n",
            "Iteration 467 - Train loss: 0.21749131390884743\n",
            "Iteration 468 - Train loss: 0.21776031447837177\n",
            "Iteration 469 - Train loss: 0.21778137597249453\n",
            "Iteration 470 - Train loss: 0.21798650083191534\n",
            "Iteration 471 - Train loss: 0.21787404314656356\n",
            "Iteration 472 - Train loss: 0.2175278883028807\n",
            "Iteration 473 - Train loss: 0.21728562445322694\n",
            "Iteration 474 - Train loss: 0.21708682421319783\n",
            "Iteration 475 - Train loss: 0.2166624893189261\n",
            "Iteration 476 - Train loss: 0.21681140809507743\n",
            "Iteration 477 - Train loss: 0.2169464184478925\n",
            "Iteration 478 - Train loss: 0.21658410316831958\n",
            "Iteration 479 - Train loss: 0.21645912858764477\n",
            "Iteration 480 - Train loss: 0.216293972429897\n",
            "Iteration 481 - Train loss: 0.2161602713496825\n",
            "Iteration 482 - Train loss: 0.21585980607428554\n",
            "Iteration 483 - Train loss: 0.21577199052741625\n",
            "Iteration 484 - Train loss: 0.21622156455905045\n",
            "Iteration 485 - Train loss: 0.21616983539088794\n",
            "Iteration 486 - Train loss: 0.21616179553730558\n",
            "Iteration 487 - Train loss: 0.21598699434375432\n",
            "Iteration 488 - Train loss: 0.21598570341374115\n",
            "Iteration 489 - Train loss: 0.2159967047309004\n",
            "Iteration 490 - Train loss: 0.21624612311965652\n",
            "Iteration 491 - Train loss: 0.2162823249394351\n",
            "Iteration 492 - Train loss: 0.21598558885999386\n",
            "Iteration 493 - Train loss: 0.21606677795739554\n",
            "Iteration 494 - Train loss: 0.2159626165476556\n",
            "Iteration 495 - Train loss: 0.2157026963195566\n",
            "Iteration 496 - Train loss: 0.21553197876557767\n",
            "Iteration 497 - Train loss: 0.21575741323884165\n",
            "Iteration 498 - Train loss: 0.21556131337698264\n",
            "Iteration 499 - Train loss: 0.21604736701199012\n",
            "Iteration 500 - Train loss: 0.21587938826717437\n",
            "Iteration 501 - Train loss: 0.21619741734072656\n",
            "Iteration 502 - Train loss: 0.21615778660193916\n",
            "Iteration 503 - Train loss: 0.21622015591562682\n",
            "Iteration 504 - Train loss: 0.21669467384298702\n",
            "Iteration 505 - Train loss: 0.2164567953086283\n",
            "Iteration 506 - Train loss: 0.21612859658874836\n",
            "Iteration 507 - Train loss: 0.21666617468330104\n",
            "Iteration 508 - Train loss: 0.21649803728869874\n",
            "Iteration 509 - Train loss: 0.21618792478522114\n",
            "Iteration 510 - Train loss: 0.21598573325055778\n",
            "Iteration 511 - Train loss: 0.21585582620455152\n",
            "Iteration 512 - Train loss: 0.21584897055618057\n",
            "Iteration 513 - Train loss: 0.2158278644930927\n",
            "Iteration 514 - Train loss: 0.21593985950994643\n",
            "Iteration 515 - Train loss: 0.2158252774312803\n",
            "Iteration 516 - Train loss: 0.21646554633260293\n",
            "Iteration 517 - Train loss: 0.21649195393097068\n",
            "Iteration 518 - Train loss: 0.21646876536259377\n",
            "Iteration 519 - Train loss: 0.21642460851519152\n",
            "Iteration 520 - Train loss: 0.21682525541669187\n",
            "Iteration 521 - Train loss: 0.21646779193051316\n",
            "Iteration 522 - Train loss: 0.21614228292650248\n",
            "Iteration 523 - Train loss: 0.21579728750102437\n",
            "Iteration 524 - Train loss: 0.21589496362250554\n",
            "Iteration 525 - Train loss: 0.21563110581643524\n",
            "Iteration 526 - Train loss: 0.21559551933482857\n",
            "Iteration 527 - Train loss: 0.21545662068049698\n",
            "Iteration 528 - Train loss: 0.21533253027133248\n",
            "Iteration 529 - Train loss: 0.21519047906534264\n",
            "Iteration 530 - Train loss: 0.21527297187701994\n",
            "Iteration 531 - Train loss: 0.21508504583793228\n",
            "Iteration 532 - Train loss: 0.21497800107520157\n",
            "Iteration 533 - Train loss: 0.21491194259389312\n",
            "Iteration 534 - Train loss: 0.2154730738817334\n",
            "Iteration 535 - Train loss: 0.21627852912713713\n",
            "Iteration 536 - Train loss: 0.2166879945104497\n",
            "Iteration 537 - Train loss: 0.21634809867248245\n",
            "Iteration 538 - Train loss: 0.21600393456715025\n",
            "Iteration 539 - Train loss: 0.21582082269371164\n",
            "Iteration 540 - Train loss: 0.2163089486339164\n",
            "Iteration 541 - Train loss: 0.21659754716761948\n",
            "Iteration 542 - Train loss: 0.21701735248651727\n",
            "Iteration 543 - Train loss: 0.21683079093646662\n",
            "Iteration 544 - Train loss: 0.21684818765703207\n",
            "Iteration 545 - Train loss: 0.21712537529715978\n",
            "Iteration 546 - Train loss: 0.216777280513743\n",
            "Iteration 547 - Train loss: 0.21661618166182606\n",
            "Iteration 548 - Train loss: 0.2166959322992386\n",
            "Iteration 549 - Train loss: 0.2172513255062352\n",
            "Iteration 550 - Train loss: 0.2173993455229158\n",
            "Iteration 551 - Train loss: 0.21716505930862118\n",
            "Iteration 552 - Train loss: 0.2171968265750405\n",
            "Iteration 553 - Train loss: 0.2174027664351124\n",
            "Iteration 554 - Train loss: 0.21708421334657912\n",
            "Iteration 555 - Train loss: 0.21725427514563003\n",
            "Iteration 556 - Train loss: 0.21749987019889272\n",
            "Iteration 557 - Train loss: 0.21727456578640417\n",
            "Iteration 558 - Train loss: 0.21719374983841855\n",
            "Iteration 559 - Train loss: 0.2169812774378973\n",
            "Iteration 560 - Train loss: 0.2167263940059846\n",
            "Iteration 561 - Train loss: 0.21705227708626612\n",
            "Iteration 562 - Train loss: 0.2168505266921933\n",
            "Iteration 563 - Train loss: 0.21660723294533432\n",
            "Iteration 564 - Train loss: 0.21669333018131343\n",
            "Iteration 565 - Train loss: 0.21664538237933828\n",
            "Iteration 566 - Train loss: 0.2166077546962922\n",
            "Iteration 567 - Train loss: 0.21634480685186186\n",
            "Iteration 568 - Train loss: 0.21619902311948877\n",
            "Iteration 569 - Train loss: 0.21610577380438523\n",
            "Iteration 570 - Train loss: 0.21579214952918782\n",
            "Iteration 571 - Train loss: 0.2157102382893555\n",
            "Iteration 572 - Train loss: 0.21581154177453812\n",
            "Iteration 573 - Train loss: 0.21581873278349742\n",
            "Iteration 574 - Train loss: 0.21600456786494482\n",
            "Iteration 575 - Train loss: 0.21597818435696156\n",
            "Iteration 576 - Train loss: 0.2158038112376946\n",
            "Iteration 577 - Train loss: 0.21584050157357595\n",
            "Iteration 578 - Train loss: 0.21620857771196011\n",
            "Iteration 579 - Train loss: 0.2162351806588561\n",
            "Iteration 580 - Train loss: 0.21629629049225357\n",
            "Iteration 581 - Train loss: 0.21606589361493075\n",
            "Iteration 582 - Train loss: 0.2159373880319195\n",
            "Iteration 583 - Train loss: 0.2158208681129903\n",
            "Iteration 584 - Train loss: 0.2157338378226946\n",
            "Iteration 585 - Train loss: 0.21571299793023585\n",
            "Iteration 586 - Train loss: 0.2157942985578634\n",
            "Iteration 587 - Train loss: 0.21594469092018406\n",
            "Iteration 588 - Train loss: 0.21672071737148577\n",
            "Iteration 589 - Train loss: 0.2168537484330136\n",
            "Iteration 590 - Train loss: 0.21687654565104236\n",
            "Iteration 591 - Train loss: 0.21707992444946328\n",
            "Iteration 592 - Train loss: 0.2172953329392004\n",
            "Iteration 593 - Train loss: 0.2176604180990971\n",
            "Iteration 594 - Train loss: 0.21767792241296632\n",
            "Iteration 595 - Train loss: 0.21756004295585787\n",
            "Iteration 596 - Train loss: 0.21761577371539134\n",
            "Iteration 597 - Train loss: 0.21766847541939524\n",
            "Iteration 598 - Train loss: 0.21796930404566853\n",
            "Iteration 599 - Train loss: 0.21774952540419784\n",
            "Iteration 600 - Train loss: 0.21755369482096285\n",
            "Iteration 601 - Train loss: 0.2172872546995167\n",
            "Iteration 602 - Train loss: 0.21809414107247296\n",
            "Iteration 603 - Train loss: 0.2179111796840531\n",
            "Iteration 604 - Train loss: 0.2177196907400715\n",
            "Iteration 605 - Train loss: 0.21750936163512388\n",
            "Iteration 606 - Train loss: 0.21747018141278585\n",
            "Iteration 607 - Train loss: 0.21747476773419383\n",
            "Iteration 608 - Train loss: 0.2173438369262792\n",
            "Iteration 609 - Train loss: 0.21747144063466728\n",
            "Iteration 610 - Train loss: 0.2172216647609946\n",
            "Iteration 611 - Train loss: 0.21711741367266582\n",
            "Iteration 612 - Train loss: 0.21688627697728283\n",
            "Iteration 613 - Train loss: 0.216801288929657\n",
            "Iteration 614 - Train loss: 0.216740111721807\n",
            "Iteration 615 - Train loss: 0.2166453391813287\n",
            "Iteration 616 - Train loss: 0.21643311428904002\n",
            "Iteration 617 - Train loss: 0.2165105934096334\n",
            "Iteration 618 - Train loss: 0.2169002656234145\n",
            "Iteration 619 - Train loss: 0.21662405077428204\n",
            "Iteration 620 - Train loss: 0.21668938015767883\n",
            "Iteration 621 - Train loss: 0.21652752201368317\n",
            "Iteration 622 - Train loss: 0.21663600107702843\n",
            "Iteration 623 - Train loss: 0.2167073752488911\n",
            "Iteration 624 - Train loss: 0.21673693918199158\n",
            "Iteration 625 - Train loss: 0.21668356036692857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 3/3 [44:36<00:00, 892.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6, 6, 5, 5, 3, 3, 3, 6, 2, 2, 5, 6, 2, 3, 5, 2, 2, 2, 6, 1, 1, 5, 1, 2, 4, 6, 5, 6, 6, 3, 1, 3, 5, 4, 4, 1, 3, 2, 3, 4, 6, 3, 3, 6, 5, 1, 3, 6, 6, 4, 3, 2, 6, 2, 6, 6, 1, 4, 5, 3, 2, 3, 4, 6, 6, 5, 4, 4, 3, 3, 1, 2, 5, 1, 4, 3, 1, 4, 6, 2, 3, 5, 1, 4, 1, 4, 3, 2, 2, 6, 6, 2, 6, 4, 6, 3, 3, 2, 4, 1, 5, 5, 4, 6, 5, 1, 5, 1, 1, 4, 4, 6, 3, 4, 2, 6, 3, 2, 1, 5, 6, 5, 5, 2, 5, 3, 2, 5, 5, 2, 2, 5, 5, 6, 6, 5, 1, 3, 1, 4, 6, 4, 6, 3, 4, 3, 6, 2, 2, 2, 6, 5, 6, 6, 1, 1, 3, 1, 4, 2, 6, 2, 1, 3, 2, 6, 3, 2, 5, 6, 1, 2, 3, 5, 2, 4, 4, 6, 6, 3, 2, 1, 3, 3, 2, 5, 4, 3, 5, 5, 3, 1, 4, 2, 5, 5, 4, 4, 4, 6, 3, 5, 6, 2, 3, 2, 5, 6, 1, 6, 3, 4, 6, 5, 3, 6, 4, 6, 6, 2, 4, 1, 3, 1, 3, 1, 1, 6, 4, 6, 3, 2, 2, 5, 5, 5, 4, 3, 6, 1, 5, 1, 1, 4, 3, 4, 2, 3, 6, 3, 3, 4, 4, 3, 6, 5, 5, 4, 1, 3, 2, 3, 5, 1, 2, 6, 6, 4, 6, 2, 2, 3, 4, 6, 3, 6, 3, 5, 5, 3, 2, 3, 1, 6, 5, 6, 6, 5, 2, 2, 4, 3, 5, 2, 3, 1, 2, 6, 5, 4, 4, 2, 6, 4, 3, 2, 6, 6, 3, 3, 5, 6, 2, 2, 6, 6, 5, 3, 4, 1, 1, 1, 5, 6, 2, 5, 1, 2, 5, 4, 4, 5, 4, 2, 3, 4, 5, 2, 2, 2, 6, 2, 3, 5, 6, 4, 4, 4, 1, 6, 2, 6, 2, 1, 1, 1, 4, 5, 4, 1, 3, 6, 6, 5, 6, 4, 6, 3, 6, 6, 1, 5, 4, 2, 3, 4, 3, 5, 1, 2, 1, 1, 5, 2, 5, 6, 2, 3, 3, 6, 4, 2, 5, 4, 4, 5, 3, 2, 4, 3, 6, 3, 3, 2, 3, 5, 3, 4, 6, 2, 4, 3, 4, 5, 2, 2, 2, 5, 4, 3, 4, 1, 5, 1, 6, 5, 2, 6, 1, 2, 4, 2, 6, 1, 6, 1, 4, 1, 4, 5, 3, 1, 1, 2, 1, 6, 4, 5, 4, 6, 4, 2, 6, 2, 6, 3, 4, 1, 5, 2, 2, 6, 2, 2, 5, 5, 5, 3, 6, 4, 3, 2, 6, 1, 2, 4, 5, 4, 5, 1, 5, 1, 4, 2, 4, 5, 3, 6, 2, 4, 5, 1, 6, 4, 5, 2, 3, 6, 5, 3, 2, 2, 5, 4, 1, 2, 6, 2, 6, 4, 2, 6, 1, 1, 3, 5, 2, 4, 3, 6, 2, 5, 5, 3, 1, 1, 5, 6, 5, 3, 2, 2, 6, 2, 4, 6, 3, 6, 3, 4, 3, 6, 4, 2, 2, 2, 2, 3, 3, 2, 1, 4, 5, 3, 3, 6, 2, 4, 4, 3, 1, 4, 5, 4, 2, 1, 2, 5, 4, 2, 3, 5, 3, 6, 3, 1, 4, 3, 4, 6, 6, 6, 6, 4, 4, 6, 1, 1, 1, 1, 3, 4, 5, 5, 1, 6, 6, 6, 3, 1, 5, 1, 1, 4, 1, 1, 3, 6, 5, 1, 6, 2, 3, 3, 6, 2, 3, 2, 3, 1, 4, 5, 3, 1, 1, 4, 4, 3, 3, 2, 4, 2, 4, 2, 5, 1, 3, 2, 4, 2, 1, 4, 3, 2, 5, 1, 3, 6, 1, 2, 1, 5, 6, 4, 5, 4, 6, 1, 6, 3, 3, 3, 4, 6, 3, 6, 2, 3, 5, 2, 3, 5, 5, 1, 4, 5, 6, 6, 2, 3, 5, 2, 2, 4, 1, 5, 1, 4, 1, 3, 5, 5, 3, 6, 5, 5, 5, 6, 5, 3, 4, 6, 5, 6, 4, 5, 6, 4, 4, 1, 2, 1, 3, 4, 1, 4, 2, 4, 2, 3, 6, 4, 3, 2, 1, 1, 5, 4, 4, 4, 5, 6, 5, 5, 5, 2, 3, 5, 5, 3, 5, 3, 5, 5, 1, 5, 6, 6, 6, 2, 4, 6, 1, 3, 4, 5, 1, 3, 1, 5, 4, 1, 2, 5, 2, 6, 6, 6, 6, 6, 6, 6, 1, 6, 3, 4, 1, 5, 6, 3, 6, 3, 1, 2, 5, 6, 6, 2, 1, 6, 6, 6, 6, 2, 3, 2, 5, 2, 6, 2, 2, 2, 4, 5, 2, 1, 3, 3, 4, 6, 1, 1, 4, 1, 2, 1, 5, 2, 6, 1, 3, 3, 1, 3, 4, 2, 6, 1, 6, 4, 4, 1, 6, 4, 4, 4, 4, 5, 3, 1, 2, 1, 2, 3, 2, 6, 5, 3, 3, 3, 2, 5, 3, 3, 2, 2, 4, 5, 1, 1, 1, 3, 1, 3, 3, 5, 5, 5, 2, 5, 1, 5, 2, 3, 3, 3, 3, 5, 2, 3, 4, 5, 4, 6, 6, 3, 3, 1, 4, 6, 6, 6, 1, 6, 1, 5, 5, 3, 2, 5, 6, 6, 3, 1, 3, 4, 1, 6, 5, 2, 6, 6, 5, 1, 2, 6, 1, 4, 2, 2, 3, 6, 4, 6, 6, 2, 4, 6, 5, 5, 3, 4, 5, 6, 5, 3, 2, 3, 6, 4, 4, 1, 6, 4, 5, 1, 4, 6, 1, 1, 1, 4, 4, 1, 3, 2, 5, 1, 1, 5, 3, 6, 6, 5, 3, 6, 5, 3, 6, 1, 6, 2, 4, 6, 5, 4, 5, 1, 1, 1, 3, 6, 2, 3, 1, 4, 5, 6, 6, 3, 3, 4, 4, 6, 3, 1, 3, 2, 3, 4]\n",
            "[6, 6, 5, 5, 3, 3, 3, 6, 2, 2, 5, 6, 2, 3, 5, 2, 2, 2, 6, 4, 1, 5, 1, 2, 4, 6, 5, 6, 6, 3, 1, 3, 5, 2, 4, 1, 3, 2, 3, 4, 6, 3, 3, 6, 5, 1, 3, 6, 6, 4, 3, 2, 6, 2, 6, 6, 2, 4, 5, 3, 2, 3, 2, 6, 6, 5, 4, 4, 3, 3, 1, 2, 5, 1, 4, 3, 1, 4, 6, 2, 3, 5, 1, 4, 1, 4, 3, 2, 2, 6, 6, 2, 6, 4, 6, 3, 3, 2, 4, 1, 5, 5, 1, 6, 5, 1, 5, 1, 1, 4, 4, 6, 3, 4, 2, 6, 3, 2, 1, 5, 6, 5, 5, 2, 5, 3, 2, 5, 5, 2, 2, 5, 5, 6, 6, 5, 2, 3, 1, 4, 6, 4, 6, 3, 4, 3, 6, 2, 2, 2, 6, 5, 6, 6, 2, 2, 3, 2, 4, 2, 6, 2, 1, 3, 2, 6, 3, 2, 5, 6, 1, 2, 3, 5, 2, 4, 4, 6, 6, 3, 2, 4, 3, 3, 2, 5, 4, 3, 5, 5, 3, 1, 4, 2, 5, 5, 4, 4, 4, 6, 3, 5, 6, 2, 3, 2, 4, 6, 1, 6, 3, 4, 6, 5, 3, 6, 4, 6, 6, 2, 4, 1, 3, 1, 3, 1, 1, 6, 4, 6, 3, 2, 2, 5, 2, 5, 4, 3, 6, 1, 5, 1, 1, 4, 3, 4, 2, 3, 6, 3, 3, 4, 4, 3, 2, 5, 5, 4, 1, 3, 2, 3, 5, 1, 2, 6, 6, 4, 6, 2, 2, 3, 4, 6, 3, 6, 3, 5, 5, 3, 2, 3, 1, 6, 5, 6, 6, 5, 2, 2, 4, 3, 5, 2, 3, 1, 2, 6, 5, 4, 4, 2, 6, 4, 3, 2, 6, 6, 3, 3, 5, 6, 2, 2, 6, 6, 5, 3, 4, 1, 4, 1, 5, 6, 2, 5, 4, 2, 5, 4, 1, 5, 4, 2, 3, 4, 5, 2, 2, 2, 6, 2, 3, 5, 6, 4, 4, 2, 1, 6, 2, 6, 2, 1, 1, 1, 4, 5, 1, 1, 3, 6, 6, 5, 6, 4, 6, 3, 6, 6, 1, 5, 1, 2, 3, 4, 3, 5, 1, 2, 4, 1, 5, 2, 5, 6, 2, 3, 3, 6, 1, 2, 5, 4, 4, 5, 3, 2, 4, 3, 6, 3, 3, 2, 3, 5, 3, 4, 6, 2, 4, 3, 4, 5, 2, 2, 2, 5, 4, 3, 4, 1, 5, 1, 6, 5, 2, 6, 3, 2, 4, 2, 6, 1, 6, 1, 4, 1, 4, 5, 3, 1, 1, 2, 1, 6, 4, 5, 4, 6, 4, 2, 6, 2, 6, 3, 4, 1, 5, 2, 2, 6, 2, 2, 5, 5, 5, 3, 6, 4, 3, 2, 6, 4, 2, 1, 5, 4, 5, 4, 5, 1, 4, 2, 4, 5, 3, 6, 2, 4, 5, 1, 6, 4, 5, 2, 3, 6, 5, 3, 2, 2, 5, 2, 1, 2, 6, 2, 6, 4, 2, 6, 1, 1, 3, 5, 2, 6, 3, 6, 2, 5, 5, 3, 1, 1, 5, 6, 5, 3, 2, 2, 6, 2, 2, 6, 3, 6, 3, 4, 3, 6, 1, 2, 2, 2, 2, 3, 3, 2, 1, 4, 5, 3, 3, 6, 2, 4, 4, 3, 1, 4, 5, 4, 2, 3, 2, 5, 4, 2, 3, 5, 3, 6, 3, 4, 4, 3, 4, 6, 6, 6, 6, 4, 4, 6, 1, 1, 1, 1, 3, 4, 5, 5, 1, 6, 6, 6, 3, 1, 5, 1, 1, 4, 1, 1, 3, 6, 5, 1, 6, 2, 3, 3, 6, 2, 3, 2, 3, 1, 4, 5, 3, 1, 1, 4, 4, 3, 3, 2, 4, 2, 1, 2, 5, 1, 3, 2, 4, 2, 1, 4, 3, 2, 5, 1, 3, 6, 1, 2, 1, 5, 6, 4, 5, 4, 6, 4, 6, 3, 3, 3, 4, 6, 3, 6, 2, 3, 5, 2, 3, 5, 5, 1, 4, 5, 6, 6, 2, 3, 5, 2, 2, 4, 1, 5, 1, 1, 1, 3, 5, 5, 3, 6, 5, 5, 5, 6, 5, 3, 4, 6, 5, 6, 4, 5, 6, 4, 3, 1, 2, 1, 3, 4, 1, 4, 2, 4, 2, 3, 6, 4, 3, 2, 1, 1, 5, 4, 4, 1, 5, 6, 5, 5, 5, 2, 3, 5, 5, 3, 5, 3, 5, 5, 1, 5, 6, 6, 6, 2, 1, 6, 3, 3, 4, 5, 1, 3, 2, 5, 4, 4, 2, 5, 2, 6, 6, 6, 6, 6, 6, 6, 1, 6, 3, 4, 1, 5, 6, 3, 6, 3, 4, 2, 5, 6, 6, 2, 1, 6, 6, 6, 6, 2, 3, 2, 5, 2, 6, 2, 2, 2, 1, 5, 2, 1, 3, 3, 4, 6, 1, 4, 4, 1, 2, 1, 5, 2, 6, 2, 3, 3, 1, 3, 4, 2, 6, 1, 6, 4, 4, 1, 6, 4, 4, 4, 4, 5, 3, 2, 2, 1, 2, 3, 2, 6, 5, 3, 3, 3, 2, 5, 3, 3, 2, 2, 4, 5, 3, 1, 1, 3, 3, 3, 3, 5, 5, 5, 2, 5, 4, 5, 2, 3, 3, 3, 3, 5, 2, 3, 4, 5, 4, 6, 6, 3, 3, 1, 4, 6, 6, 6, 1, 6, 1, 5, 5, 3, 2, 5, 6, 6, 3, 1, 3, 2, 1, 6, 5, 4, 6, 6, 5, 4, 2, 6, 1, 4, 2, 2, 3, 6, 2, 6, 6, 2, 4, 6, 5, 5, 3, 1, 5, 6, 5, 3, 2, 3, 6, 4, 4, 1, 6, 4, 5, 1, 1, 6, 2, 1, 1, 4, 1, 1, 3, 2, 5, 1, 1, 5, 3, 6, 6, 5, 3, 6, 5, 3, 6, 1, 6, 2, 4, 6, 5, 4, 5, 1, 1, 1, 3, 6, 2, 3, 1, 4, 5, 6, 6, 3, 3, 4, 4, 6, 2, 1, 3, 2, 3, 4]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Using the validation dataset to calculate accuracy score\n",
        "sen_model.eval()\n",
        "\n",
        "logit_preds, true_labels, pred_labels, tokenized_texts = [],[],[],[]\n",
        "\n",
        "# Predict\n",
        "for i, batch in enumerate(val_dloader):\n",
        "\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    with torch.no_grad():\n",
        "        outs = sen_model(b_input_ids, attention_mask=b_input_mask)\n",
        "        b_logit_pred = outs[0]\n",
        "        pred_label = torch.softmax(b_logit_pred, dim=1)\n",
        "\n",
        "        b_logit_pred = b_logit_pred.detach().cpu().numpy()\n",
        "        pred_label = pred_label.to('cpu').numpy()\n",
        "        b_labels = b_labels.to('cpu').numpy()\n",
        "\n",
        "    tokenized_texts.append(b_input_ids)\n",
        "    logit_preds.append(b_logit_pred)\n",
        "    true_labels.append(b_labels)\n",
        "    pred_labels.append(pred_label)\n",
        "\n",
        "pred_labels = [np.argmax(item) for sublist in pred_labels for item in sublist]\n",
        "true_labels = [item for sublist in true_labels for item in sublist]"
      ],
      "metadata": {
        "id": "HlPEtzJhriP-"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flat_accuracy = accuracy_score(true_labels, pred_labels) * 100\n",
        "print(f'Flat accuracy: {flat_accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HNkKFrXSSGZ",
        "outputId": "daab466d-aea8-4ffc-af75-3452fa165670"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flat accuracy: 94.3\n"
          ]
        }
      ]
    }
  ]
}